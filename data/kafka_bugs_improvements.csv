key,id,created,updated,resolution_date,summary,description,status,status_category,issue_type,priority,assignee,reporter,project_key,project_name,story_points,epic_link,components,labels,fix_versions
KAFKA-19512,13623502,2025-07-15T18:37:00.000+0000,2025-07-15T19:09:07.000+0000,,add missing MVs in MetadataVersionTest,"We need to add the 4.2 MVs in the following tests.
testFromVersionString()testShortVersion()
testVersion()",Patch Available,In Progress,Improvement,Minor,Dmitry Werner,Jun Rao,KAFKA,Kafka,,,,,
KAFKA-19511,13623487,2025-07-15T15:33:17.000+0000,2025-07-15T17:14:51.000+0000,,Fix flaky test HandlingSourceTopicDeletionIntegrationTest.shouldThrowErrorAfterSourceTopicDeleted,"The test org.apache.kafka.streams.integration.HandlingSourceTopicDeletionIntegrationTest.shouldThrowErrorAfterSourceTopicDeleted(boolean, TestInfo)[2] often fails.

Observed in:
 # [https://develocity.apache.org/scans/tests?search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&search.values=trunk&tests.container=org.apache.kafka.streams.integration.HandlingSourceTopicDeletionIntegrationTest&tests.sortField=FLAKY&tests.test=shouldThrowErrorAfterSourceTopicDeleted(boolean%2C%20TestInfo)%5B2%5D]

 

This test has a high failure/flaky rate since July 9th.",Open,To Do,Improvement,Major,,Lucy Liu,KAFKA,Kafka,,,,,
KAFKA-19510,13623457,2025-07-15T11:50:27.000+0000,2025-07-15T12:57:40.000+0000,,Kafka Streams does not always release lock when adding or removing threads multiple times,"Kafka Streams seems to not always release the state directory lock for all tasks, when threads are added / removed. 

Effectively, the old thread (same name, different object) still seems to own the task.

To reproduce, checkout

[https://github.com/lucasbru/kafka/commit/2701dd39fe7df23a123939a1c9e140c849904b5f]

and run `AdjustStreamThreadCountTest > 
shouldAddAndRemoveThreadsMultipleTimes` ... multiple times.
 
The test was changed to use a stateful topology. 
 

I reproduced this on 3.9, 4.0, 4.1, trunk
 ",Open,To Do,Bug,Major,,Lucas Brutschy,KAFKA,Kafka,,,streams,,
KAFKA-19509,13623438,2025-07-15T08:43:06.000+0000,2025-07-15T09:15:41.000+0000,,Improve error message when release version is wrong,"When running kafka-storage.sh, if the release-version is wrongly set, we'll get the error messages including un-released metadata version:

 
{code:java}
> bin/kafka-storage.sh format --standalone -t kEzc4vk3TIKhCQKsh40klQ -c config/server.properties --release-version 4.0-IV4
Exception in thread ""main"" java.lang.IllegalArgumentException: Version 4.0-IV4 is not a valid version. The minimum version is 3.3-IV3 and the maximum version is 4.2-IV1
    at org.apache.kafka.server.common.MetadataVersion.lambda$fromVersionString$0(MetadataVersion.java:356)
    at java.base/java.util.Optional.orElseThrow(Optional.java:403)
    at org.apache.kafka.server.common.MetadataVersion.fromVersionString(MetadataVersion.java:354)
    at kafka.tools.StorageTool$.$anonfun$runFormatCommand$1(StorageTool.scala:133)
    at scala.Option.foreach(Option.scala:437)
    at kafka.tools.StorageTool$.runFormatCommand(StorageTool.scala:132)
    at kafka.tools.StorageTool$.execute(StorageTool.scala:86)
    at kafka.tools.StorageTool$.main(StorageTool.scala:46)
    at kafka.tools.StorageTool.main(StorageTool.scala)
 {code}

One idea is to improve this by relying on the internal config: `unstable.feature.versions.enable` to decide if we want to log the unstable feature versions. 

The other thought is we can mimic what Kafka-feature.sh did: 
{code:java}
 >  bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --release-version 4.0-IV4
[2025-07-15 16:31:53,280] WARN [AdminClient clientId=adminclient-1] 
Unknown metadata.version 4.1-IV4. Supported metadata.version are 3.3-IV3, 3.4-IV0, 3.5-IV0, 3.5-IV1, 3.5-IV2, 3.6-IV0, 3.6-IV1, 3.6-IV2, 3.7-IV0, 3.7-IV1, 3.7-IV2, 3.7-IV3, 3.7-IV4, 3.8-IV0, 3.9-IV0, 4.0-IV0, 4.0-IV1, 4.0-IV2, 4.0-IV3 {code}
It didn't output the supported versions to users.

 

 ",Open,To Do,Improvement,Major,,Luke Chen,KAFKA,Kafka,,,,,
KAFKA-19508,13623434,2025-07-15T07:58:00.000+0000,2025-07-15T15:20:51.000+0000,,Producer keeps reporting a NotLeaderOrFollowerException,"1.After the server restarts, the producer keeps reporting a NotLeaderOrFollowerException when producing messages.

!image-2025-07-15-15-55-01-596.png!

2.Check the zknode data; the leader is functioning normally.

!image-2025-07-15-15-59-02-855.png!
3.Strangely, at 04:34:01, the leader was 12, and at 04:34:16, the leader was 11, but the leaderEpoch did not change.

!image-2025-07-15-16-03-13-043.png!

!image-2025-07-15-16-01-59-877.png!
4.state-change logs

 ",Open,To Do,Bug,Blocker,,mooner,KAFKA,Kafka,,,"clients,core,producer ",,
KAFKA-19507,13623427,2025-07-15T07:30:09.000+0000,2025-07-15T07:58:11.000+0000,,Optimize Replica Assignment for Broker Load Balance in Uneven Rack Configurations,"h3. Issue Description

Kafka's current replica assignment strategy prioritizes _balancing replica counts across racks_ (availability zones in cloud environments) over _balancing replicas across individual brokers_. While this ensures rack diversity, it creates significant broker-level load imbalance when racks contain unequal numbers of brokers.
h3. Problem Illustration

Consider a 3-replica topic with 3 racks:
 * *Rack A*: Brokers 1, 4

 * *Rack B*: Brokers 2, 5

 * *Rack C*: Broker 3 (single broker)

Under the current strategy:
 * Brokers 1, 2, 4, 5 each receive 1/6 of all replicas

 * Broker 3 receives 1/3 of all replicas (twice the load of others)

This forces Broker 3 into a bottleneck (""bucket effect""), as it handles double the traffic and storage load.

 

To mitigate this, deployments today must maintain broker counts as _multiples of rack counts_ (e.g., 3, 6, 9 brokers for 3 racks). While this ensures balance, it:
 # *Restricts deployment flexibility*: Scaling clusters horizontally requires adding/removing nodes in rack-sized increments.

 # *Increases costs unnecessarily*: For example, a 4-broker cluster could suffice for a 3-rack setup, but users must deploy 6 brokers to maintain balance—increasing infrastructure costs by 50%.

h3. Proposed Solution

Modify the assignment strategy to:
 # *Prioritize broker-level balance* as the primary objective.

 # *Weight rack-level distribution* by broker count per rack (e.g., a rack with 2 brokers receives twice the replicas of a rack with 1 broker).

h4. Benefits
 * *Balanced load*: All brokers receive near-equal replicas regardless of rack imbalance.

 * *Deployment flexibility*: Clusters can scale to _any size_ as long as {{rack_count ≥ replica_factor}}.

 * *Cost efficiency*: Users deploy only necessary brokers.

h4. Example Scenario

_3 replicas, 4 racks with 5 brokers:_
 * *Rack A*: Brokers 1, 5 → Receives 2/5 of replicas (distributed evenly between Brokers 1 & 5)

 * *Racks B, C, D*: 1 broker each → Each receives 1/5 of replicas _Result_: Every broker handles exactly 1/5 of total replicas—eliminating bottlenecks.

h3. Request

We propose modifying the replica assignment algorithm to prioritize broker-level replica balance, while using rack-node-count-weighted distribution. This allows enterprises to deploy Kafka clusters with more flexible node counts, significantly improving cost efficiency while maintaining rack awareness.",Open,To Do,Improvement,Major,Jialun Peng,Jialun Peng,KAFKA,Kafka,,,,,
KAFKA-19506,13623422,2025-07-15T06:34:04.000+0000,2025-07-15T15:14:03.000+0000,,Implement dynamic compression type selection and fallback for client telemetry,"Kafka clients currently select the first compression type offered by the broker for telemetry push, often leading to errors if the required compression library (e.g., zstd, lz4, snappy) is not present in the client runtime. This causes telemetry push failures, while produce requests continue to work normally. Gzip is generally reliable as it is always present in the JDK, but other algorithms can trigger fatal exceptions, stopping further telemetry pushes.

This ticket proposes enhancing the Kafka client to dynamically select the best available compression type at runtime. If the initial compression attempt fails (e.g., due to a missing dependency), the client should automatically retry using the next broker-supported compression type. If none are available, it must gracefully fall back to sending uncompressed telemetry data, ensuring telemetry push always succeeds regardless of deployment environment or available libraries.",Open,To Do,Bug,Major,Kaushik Raina,Kaushik Raina,KAFKA,Kafka,,,,,4.2.0
KAFKA-19504,13623375,2025-07-14T17:12:28.000+0000,2025-07-15T17:45:04.000+0000,2025-07-15T00:41:22.000+0000,AdminClient creates and adds second metrics reporter ,"The `AdminClient` adds a telemetry reporter to the metrics reporters list in the constructor.  The problem is that the reporter was already added in the `createInternal` method.  In the `createInternal` method call, the `clientTelemetryReporter` is added to a `List<MetricReporters>` which is passed to the `Metrics` object, will get closed when `Metrics.close()` is called.  But adding a reporter to the reporters list in the constructor is not used by the `Metrics` object and hence doesn't get closed, causing a memory leak.  Note, this problem only exists if telemetry metrics is enabled for the `AdminClient` and it is disabled by default.",Resolved,Done,Bug,Blocker,Bill Bejeck,Bill Bejeck,KAFKA,Kafka,,,clients,,"4.0.1,4.1.0,4.2.0"
KAFKA-19501,13623318,2025-07-14T11:08:46.000+0000,2025-07-14T14:01:47.000+0000,,System tests should use 17-bullseye instead of 17-buster,"the version buster was removed from https://security.debian.org/debian-security, and hence the command ""apt update"" fails due to a 404 error (see below log)

{code:java}
 > [stage-1  2/59] RUN apt update && apt install -y sudo git netcat iptables rsync unzip wget curl jq coreutils openssh-server net-tools vim python3-pip python3-dev libffi-dev libssl-dev cmake pkg-config libfuse-dev iperf traceroute iproute2 iputils-ping && apt-get -y clean:                                                                                             
0.721                                                                                                                                                                                   
0.721 WARNING: apt does not have a stable CLI interface. Use with caution in scripts.                                                                                                   
0.721                                                                                                                                                                                   
0.901 Ign:1 http://security.debian.org/debian-security buster/updates InRelease
0.902 Err:2 http://security.debian.org/debian-security buster/updates Release
0.902   404  Not Found [IP: 151.101.194.132 80]
0.903 Ign:3 http://deb.debian.org/debian buster InRelease
0.906 Ign:4 http://deb.debian.org/debian buster-updates InRelease
0.909 Err:5 http://deb.debian.org/debian buster Release
0.909   404  Not Found [IP: 151.101.2.132 80]
0.911 Err:6 http://deb.debian.org/debian buster-updates Release
0.911   404  Not Found [IP: 151.101.2.132 80]

{code}

the simple approach is to use 17-bullseye instead of 17-buster ",Open,To Do,Bug,Major,Tsung-Han Ho,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19500,13623313,2025-07-14T10:17:07.000+0000,2025-07-15T14:06:11.000+0000,,kafka-consumer-groups.sh should fail quickly if the partition leader is unavailable,"
{code:java}
Error: Executing consumer group command failed due to java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
	at org.apache.kafka.tools.OffsetsUtils.getLogTimestampOffsets(OffsetsUtils.java:190)
	at org.apache.kafka.tools.OffsetsUtils.resetByDuration(OffsetsUtils.java:352)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.prepareOffsetsToReset(ConsumerGroupCommand.java:1015)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.resetOffsetsForInactiveGroup(ConsumerGroupCommand.java:704)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.lambda$resetOffsets$24(ConsumerGroupCommand.java:681)
	at java.base/java.util.HashMap.forEach(HashMap.java:1429)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.resetOffsets(ConsumerGroupCommand.java:675)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand.run(ConsumerGroupCommand.java:130)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand.main(ConsumerGroupCommand.java:110)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)
	at org.apache.kafka.tools.OffsetsUtils.getLogTimestampOffsets(OffsetsUtils.java:167)
	... 8 more

{code}

`Admin#listOffsets` needs to communicate to the partition leader to get latest information. Hence, the call hangs if the node hosting the leader is unavailable. It should fail quickly by using `describeTopics` to check the leaders for all input partitions",Open,To Do,Improvement,Minor,kangning.li,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19498,13623291,2025-07-14T06:13:09.000+0000,2025-07-14T15:16:23.000+0000,,Add include argument to ConsumerPerformance tool,"Add include argument to ConsumerPerformance tool to subscribe to multiple topics.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-1192%3A+Add+include+argument+to+ConsumerPerformance+tool",Open,To Do,Improvement,Major,Federico Valeri,Federico Valeri,KAFKA,Kafka,,,tools,kip-required,
KAFKA-19497,13623185,2025-07-11T17:51:33.000+0000,2025-07-15T05:13:41.000+0000,,Topic replay code does not handle creation and deletion properly if it occurs in the same batch,"There is a small logic bug in topic replay. If a topic is created and then removed before the TopicsDelta is applied, we end up with the deleted topic in {{createdTopics}} on the delta but not in deletedTopicIds. I think we are extremely unlikely to see this since MetadataLoader will apply the delta for each batch of records it receives. Since it’s impossible to see a TopicRecord and RemoveTopicRecord in the same batch, the only way this could surface is if MetadataLoader did some buffering.

{{}}",Open,To Do,Bug,Major,zhu zhe,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19496,13623154,2025-07-11T12:36:28.000+0000,2025-07-13T16:37:12.000+0000,2025-07-13T16:35:30.000+0000,Failing test: DescribeStreamsGroupTest.testDescribeMultipleStreamsGroupWithMembersAndVerboseOptions(),https://github.com/apache/kafka/actions/runs/16216903812/job/45788662908?pr=20153,Resolved,Done,Bug,Major,Alieh Saeedi,Apoorv Mittal,KAFKA,Kafka,,,"streams,unit tests",flaky-test,4.2.0
KAFKA-19495,13623061,2025-07-10T15:31:18.000+0000,2025-07-11T16:01:03.000+0000,2025-07-11T16:01:03.000+0000,DefaultJwtRetriever could not be found with Docker native image,"I built a native image for 4.1.0-rc0 (https://github.com/apache/kafka/actions/runs/16196532977/job/45724349624) and pushed it to DockerHub (https://hub.docker.com/layers/apache/kafka-native/4.1.0-rc0/images/sha256-dd69cac8a2aa1282e42b25f92d2a66f29c6a6bf375bc8759b9728b2d2c0561c7)

When trying to run the image, I get the following error:

{noformat}
docker run -p 9092:9092 apache/kafka-native:4.1.0-rc0
===> User
uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
===> Setting default values of environment variables if not already set.
CLUSTER_ID not set. Setting it to default value: ""5L6g3nShT-eMCtK--X86sw""
===> Configuring ...
===> Launching ...
Exception in thread ""main"" java.lang.ExceptionInInitializerError at org.apache.kafka.server.config.AbstractKafkaConfig.<clinit>(AbstractKafkaConfig.java:56) at java.base@21.0.2/java.lang.Class.ensureInitialized(DynamicHub.java:601) at kafka.tools.StorageTool$.$anonfun$execute$1(StorageTool.scala:79) at scala.Option.flatMap(Option.scala:283) at kafka.tools.StorageTool$.execute(StorageTool.scala:79) at kafka.tools.StorageTool$.main(StorageTool.scala:46) at kafka.docker.KafkaDockerWrapper$.main(KafkaDockerWrapper.scala:57) at kafka.docker.KafkaDockerWrapper.main(KafkaDockerWrapper.scala) at java.base@21.0.2/java.lang.invoke.LambdaForm$DMH/sa346b79c.invokeStaticInit(LambdaForm$DMH) Caused by: org.apache.kafka.common.config.ConfigException: Invalid value org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever for configuration sasl.oauthbearer.jwt.retriever.class: Class org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever could not be found. at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:778) at org.apache.kafka.common.config.ConfigDef$ConfigKey.<init>(ConfigDef.java:1271) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:155) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:198) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:237) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:399) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:412) at org.apache.kafka.common.config.internals.BrokerSecurityConfigs.<clinit>(BrokerSecurityConfigs.java:197) ... 9 more
{noformat}

Since DefaultJwtRetriever is dynamically loaded at runtime, I wonder if the docker/native/native-image-configs/reflect-config.json file needs to be updated.
",Resolved,Done,Bug,Blocker,Luke Chen,Mickael Maison,KAFKA,Kafka,,,docker,,4.1.0
KAFKA-19494,13623048,2025-07-10T13:29:44.000+0000,2025-07-11T08:12:15.000+0000,2025-07-11T08:12:15.000+0000,Undeprecate JoinGroup V0 & V1 in 3.x,"We added back support for JoinGroup v0 & v1 in Kafka 4.0.1 and 4.1.0 due to KAFKA-19444. Due to that, we should undeprecate these protocol api versions in 3.x.",Resolved,Done,Improvement,Major,,Ismael Juma,KAFKA,Kafka,,,,,"3.7.3,3.8.2,3.9.2"
KAFKA-19493,13623043,2025-07-10T13:13:39.000+0000,2025-07-12T05:03:23.000+0000,,Incorrect rate metric with larger window size,"The Kafka Rate metric gives incorrect rate when the window size is larger say hourly. 

I suspect the code for calculating rate is incorrect for metrics which are not emitting rate per second or millisecond. The code for time unit calculation seems wrong: [convert(windowSize(config, now), unit)|https://github.com/apache/kafka/blob/da4fbba2793528e283458e080a690ad141857b0b/clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java#L67] i.e. the convert code divides the elapsed time in window to the time unit.

 

The problem can be easily reporduced by looking at `rebalance-rate-per-hour` metric.",Open,To Do,Bug,Major,Lan Ding,Apoorv Mittal,KAFKA,Kafka,,,"clients,metrics",,
KAFKA-19489,13622947,2025-07-09T15:29:01.000+0000,2025-07-11T13:53:48.000+0000,,storage tool should check controller.quorum.voters is not set alongside a dynamic quorum flag when formatting,"The storage tool currently allows for setting both the static voters config ({{{}controller.quorum.voters{}}}) and attempting to format with one of {{--standalone, --initial-controllers}} on a controller, but instead it should throw an exception. This is because setting {{controller.quorum.voters}} itself is formatting the voter set.

Setting {{controller.quorum.voters}} while trying to format with a --standalone and {{-no-initial-controllers}} setup can result in 2 voter sets. For example, in a three node setup, the two nodes that formatted with --no-initial-controllers could form quorum with each other since they have the static voter set, and the {{-standalone}} node would ignore the config and read the voter set of itself from its log, forming its own quorum of 1.",Open,To Do,Bug,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19488,13622935,2025-07-09T14:50:01.000+0000,2025-07-10T02:26:26.000+0000,2025-07-10T02:26:18.000+0000,"Update the docs of ""if-not-exists""","""the action will only execute"" is incorrect, as the admin still sends the request. The ""if-not-exists"" flag is actually used to swallow the exception. ",Resolved,Done,Improvement,Minor,xuanzhang gong,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19487,13622927,2025-07-09T14:02:55.000+0000,2025-07-09T14:02:55.000+0000,,Improve consistency of command-line arguments,This issue tracks the development of [KIP-1147: Improve consistency of command-line arguments|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Improve+consistency+of+command-line+arguments].,Open,To Do,New Feature,Major,Andrew Schofield,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-19486,13622923,2025-07-09T13:44:51.000+0000,2025-07-10T02:44:29.000+0000,,Always use the latest version of kafka-topics.sh to create topics in system tests,"Using ""old"" kafka-topics.sh to create topics on ""old"" brokers is stable, but it also has some disadvantages.

1. E2E does not cover the case of using ""new"" kafka-topics.sh on ""old"" brokers
2. it requires a bunch of conditions for ""zk"", since some old kafka-topics.sh require using zk connection

In short, we should always use latest kafka-topics.sh to create topics on ""old"" brokers",Open,To Do,Improvement,Major,Chih-Yuan Chien,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19484,13622854,2025-07-08T19:46:51.000+0000,2025-07-08T19:47:06.000+0000,,Tiered Storage Quota Metrics can stop reporting,"It is possible for tiered storage throttle metrics (introduced as a part of [KIP-956|https://cwiki.apache.org/confluence/display/KAFKA/KIP-956+Tiered+Storage+Quotas]) to stop reporting if the relevant tiered storage operation (copy/fetch) goes idle for longer than the sensor expiry timeout of one hour.

 

RemoteLogManager maintains a static reference to the sensors used for metric reporting. This is a problem because the default sensor expiry time is one hour and there is nothing responsible for handling expired sensors. If the sensors expire, RemoteLogManager will continue producing metrics through it's static references to sensor objects that have already been cleaned up by the ExpireSensorTask.

 

This issue tends to affect fetch metrics a lot more than copy metrics because the copy sensors don't go idle unless the topics stop being produced to. In contrast, the use case of backfilling from earliest offset using tiered storage is a pretty common use case.

 

*Reproduction*
 * Generate some amount of tiered storage fetch traffic on a topic. Confirm the remote-fetch-throttle-time-avg/max metrics are being reported.
 * Remove the consumer workload that triggers the tiered storage fetch traffic. Wait for one hour (the sensor expiration period)
 * Generate some more tiered storage fetch traffic. The metric will no longer report.",Open,To Do,Bug,Minor,,George Wu,KAFKA,Kafka,,,Tiered-Storage,,
KAFKA-19483,13622839,2025-07-08T17:56:22.000+0000,2025-07-08T18:06:40.000+0000,,Fix flaky test RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists(),"Observed in :

[https://github.com/apache/kafka/actions/runs/16130278655/job/45516681844]

[https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 
{code:java}
2025-07-05T09:39:50.2299240Z org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists() failed, log available in /home/runner/work/kafka/kafka/storage/build/reports/testOutput/org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists().test.stdout 2025-07-05T09:39:50.2300479Z 2025-07-05T09:39:50.2300817Z Gradle Test Run :storage:test > Gradle Test Executor 50 > RemoteIndexCacheTest > testConcurrentCacheDeletedFileExists() FAILED 2025-07-05T09:39:50.2301484Z org.opentest4j.AssertionFailedError: expected: <0> but was: <1> 2025-07-05T09:39:50.2302021Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) 2025-07-05T09:39:50.2303093Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T09:39:50.2303807Z at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) 2025-07-05T09:39:50.2304339Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) 2025-07-05T09:39:50.2305021Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) 2025-07-05T09:39:50.2305539Z at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) 2025-07-05T09:39:50.2306163Z at app//org.apache.kafka.test.TestUtils.assertNoLeakedThreadsWithNameAndDaemonStatus(TestUtils.java:179) 2025-07-05T09:39:50.2306912Z at app//org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.cleanup(RemoteIndexCacheTest.java:143){code}",Open,To Do,Improvement,Major,,Sushant Mahajan,KAFKA,Kafka,,,,,
KAFKA-19482,13622838,2025-07-08T17:55:14.000+0000,2025-07-15T00:14:11.000+0000,2025-07-15T00:14:11.000+0000,Fix flaky test KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics,"KafkaStreamsTelemetryIntegrationTest > ""shouldPassMetrics(String, boolean, String).topologyType=complex, stateUpdaterEnabled=true, groupProtocol=streams""

 

Observed in: [https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 

 
{code:java}
2025-07-05T10:46:22.1980071Z org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(String, boolean, String)[7] failed, log available in /home/runner/work/kafka/kafka/streams/integration-tests/build/reports/testOutput/org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(String, boolean, String)[7].test.stdout 2025-07-05T10:46:22.1988523Z 2025-07-05T10:46:22.1990639Z Gradle Test Run :streams:integration-tests:test > Gradle Test Executor 121 > KafkaStreamsTelemetryIntegrationTest > shouldPassMetrics(String, boolean, String) > ""shouldPassMetrics(String, boolean, String).topologyType=complex, stateUpdaterEnabled=true, groupProtocol=streams"" FAILED 2025-07-05T10:46:22.1996767Z org.opentest4j.AssertionFailedError: expected: <92> but was: <96> 2025-07-05T10:46:22.1997817Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) 2025-07-05T10:46:22.1999309Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T10:46:22.2000511Z at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) 2025-07-05T10:46:22.2001638Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) 2025-07-05T10:46:22.2002596Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) 2025-07-05T10:46:22.2003688Z at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) 2025-07-05T10:46:22.2005204Z at app//org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(KafkaStreamsTelemetryIntegrationTest.java:298){code}
 ",Resolved,Done,Improvement,Major,Matthias J. Sax,Sushant Mahajan,KAFKA,Kafka,,,"streams,unit tests",flaky-test,4.2.0
KAFKA-19481,13622837,2025-07-08T17:52:57.000+0000,2025-07-08T18:07:33.000+0000,,Fix flaky test AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex(),"Observed in 

[https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 
{code:java}
2025-07-05T09:11:08.8004324Z Gradle Test Run :core:test > Gradle Test Executor 21 > SaslOAuthBearerSslEndToEndAuthorizationTest > testProduceConsumeViaSubscribe(String) > testProduceConsumeViaSubscribe(String).groupProtocol=classic STARTED 2025-07-05T09:11:09.6005665Z kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex() failed, log available in /home/runner/work/kafka/kafka/core/build/reports/testOutput/kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex().test.stdout 2025-07-05T09:11:09.6008071Z 2025-07-05T09:11:09.6008981Z Gradle Test Run :core:test > Gradle Test Executor 22 > AuthorizerIntegrationTest > testConsumerGroupHeartbeatWithRegex() FAILED 2025-07-05T09:11:09.6011714Z org.opentest4j.AssertionFailedError: Unexpected assignment ConsumerGroupHeartbeatResponseData(throttleTimeMs=0, errorCode=0, errorMessage=null, memberId='dBGv01cmTbOyJoyQDgpFxg', memberEpoch=1, heartbeatIntervalMs=5000, assignment=null) ==> expected: not <null> 2025-07-05T09:11:09.6014642Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:152) 2025-07-05T09:11:09.6016154Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T09:11:09.6017649Z at app//org.junit.jupiter.api.AssertNotNull.failNull(AssertNotNull.java:49) 2025-07-05T09:11:09.6018855Z at app//org.junit.jupiter.api.AssertNotNull.assertNotNull(AssertNotNull.java:35) 2025-07-05T09:11:09.6020035Z at app//org.junit.jupiter.api.Assertions.assertNotNull(Assertions.java:312) 2025-07-05T09:11:09.6021483Z at app//kafka.api.AuthorizerIntegrationTest.sendAndReceiveRegexHeartbeat(AuthorizerIntegrationTest.scala:4085) 2025-07-05T09:11:09.6051031Z at app//kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex(AuthorizerIntegrationTest.scala:3125){code}
 ",Open,To Do,Improvement,Major,,Sushant Mahajan,KAFKA,Kafka,,,,,
KAFKA-19480,13622801,2025-07-08T11:22:27.000+0000,2025-07-08T11:37:11.000+0000,,KRaft migration hangs when /migration has null value,"When using the [zookeeper-security-migration|https://kafka.apache.org/39/documentation.html#zk_authz_migration] tool without the '–enable.path.check' option, the script not only updates the ACLs for the existing znodes, but also creates any non-existing ones (with the ACL options specified) using null values based on the list defined in [ZkData.SecureRootPaths.|https://github.com/apache/kafka/blob/3.9/core/src/main/scala/kafka/zk/ZkData.scala#L1089-L1102] This is especially problematic for the /migration znode as the current logic only checks for the existence of the znode and later the migration process will hang when it tries to parse the null value over and over again. 

In summary, the migration cannot be completed if the zookeeper-security-migration script was run previously, and the only workaround is to manually remove the /migration znode in such cases. I propose a simple fix to circumvent the manual step by recreating the /migration znode if it contains a null value.",Patch Available,In Progress,Bug,Major,Gergely Harmadás,Gergely Harmadás,KAFKA,Kafka,,,"kraft,migration",,
KAFKA-19479,13622789,2025-07-08T09:23:07.000+0000,2025-07-15T09:02:54.000+0000,,"at_least_once mode in Kafka Streams silently drops messages when the producer fails with MESSAGE_TOO_LARGE, violating delivery guarantees","*Description*

It appears there is a scenario where Kafka Streams running with {{processing.guarantee=at_least_once}} does {*}not uphold its delivery guarantees{*}, resulting in *message loss.*

 

*Reproduction Details*

We run a simple Kafka Streams topology like the following:
{code:java}
props[StreamsConfig.APPLICATION_ID_CONFIG] = ""poc-at-least-once""
props[StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name
props[StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name
props[StreamsConfig.PROCESSING_GUARANTEE_CONFIG] = StreamsConfig.AT_LEAST_ONCE
// Large producer batch size to induce MESSAGE_TOO_LARGE
props[ProducerConfig.LINGER_MS_CONFIG] = ""300000""
props[ProducerConfig.BATCH_SIZE_CONFIG] = ""33554432""
/**         
* a custom ProductionExceptionHandler is registered to demonstrate that it is not triggered in this scenario. 
* in fact, neither the ProductionExceptionHandler nor the StreamsUncaughtExceptionHandler are invoked during this failure        
*/ props[StreamsConfig.PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG] = ""poc.MyProductionExceptionHandler""

val stream = streamsBuilder.stream<String, String>(""input.topic"")
stream.peek { key, value -> println(""$key:$value"") }
      .to(""output.topic"")*
 {code}
 

*What we observe:*
 * Records from {{input.topic}} are consumed and buffered at producer side

 * After some time (likely based on {{{}commit.interval.ms{}}}), the *consumer offset is committed*
 * Producer records *flush* is triggered

 * The sendind of records to kafka broker fails with {{{}MESSAGE_TOO_LARGE{}}}{*}{*}

 * As a result, the application {*}commits offsets without actually producing the records{*}, which leads to *silent message loss*

 

*Steps to Reproduce*
 # Generate ~50,000 records (sized similarly to the sample project) in {{input.topic to induce MESSAGE_TOO_LARGE}}
 # Start the topology with the configuration above
 # Wait for all messages to be consumed
 # Observe:

 * 
 ** Offsets are committed

 * 
 ** Output topic receives no messages

 * 
 ** Log shows repeated {{MESSAGE_TOO_LARGE}} error:

{code:java}
11:50:30.695 [kafka-producer-network-thread | kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] WARN  o.a.k.c.producer.internals.Sender - [Producer clientId=kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] Got error produce response in correlation id 255 on topic-partition output.topic-0, splitting and retrying (2147483647 attempts left). Error: MESSAGE_TOO_LARGE {code}
 

*Reproduced* with :
 * kafka-client-3.7.0, kafka-streams-3.7.0
 * kafka-client-4.-.0, kafka-streams-4.0.0
 * kafka-client-7.9.2-ccs, kafka-streams-7.9.2-ccs
 * kafka-client-8.0.0-ccs, kafka-streams-8.0.0-ccs

 

*Expected Behavior*

In {{at_least_once}} mode, Kafka Streams should *not commit offsets* unless records are {*}successfully produced{*}. 

 

*Attached*
 * configs for stream, producer, consumer
 * sample project used to replicate the issue",Open,To Do,Bug,Critical,,Mihai Lucian,KAFKA,Kafka,,,streams,,
KAFKA-19475,13622643,2025-07-06T17:57:15.000+0000,2025-07-06T18:03:37.000+0000,,updateQuotaMetricConfigs could iterate through all metrics under a write lock,see the comment: https://github.com/apache/kafka/pull/19807#discussion_r2185848328,Open,To Do,Improvement,Major,Chang Chi Hsu,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19474,13622586,2025-07-04T16:38:02.000+0000,2025-07-09T01:55:14.000+0000,2025-07-09T01:55:14.000+0000,Wrong placement of WARN log for truncation below HWM,"{{ReplicaFetcherThread#truncate}} has a useful [WARN log|https://github.com/apache/kafka/blob/da4fbba2793528e283458e080a690ad141857b0b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala#L171] when a follower is truncating its log segment below its HWM.

This is of particular help when debugging data loss scenarios like those described in the motivation for [KIP-966|https://cwiki.apache.org/confluence/display/KAFKA/KIP-966%3A+Eligible+Leader+Replicas].

Unfortunately, a refactoring in [PR 5608|https://github.com/apache/kafka/pull/5608/files#diff-693a5eaaaa4fe61a18af23d5b06ccc024261dda01558cbaacb7020e1911c6d55R264] moved the check to _after_ the truncation, which may modify the HWM. As a result, the log may not be emitted even though the log is truncated below the HWM",Resolved,Done,Bug,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,core,,4.2.0
KAFKA-19473,13622585,2025-07-04T16:34:48.000+0000,2025-07-06T13:24:07.000+0000,,The client does not throw RecordTooLargeException when the Record size exceeds max.partition.fetch.bytes,"When the Record size is greater than max.partition.fetch.bytes, the client does not fail with RecordTooLargeException, and then gets stuck consuming this message indefinitely.

 

!image-2025-07-05-00-54-23-657.png!




The fix below appears to resolve this issue, however, the potential side effects remain unknown.




 !screenshot-2.png! ",Open,To Do,Bug,Major,dyingjiecai,dyingjiecai,KAFKA,Kafka,,,core,,
KAFKA-19472,13622584,2025-07-04T16:19:17.000+0000,2025-07-09T02:22:19.000+0000,,A BufferOverflowException is thrown by RemoteLogManager when FetchApiVersion <= 2,"RemoteLogReader throws a BufferOverflowException when FetchApi version <= 2 and firstBatchSize exceeds max.partition.fetch.bytes

Exception Message
{code:java}
[2025-07-04 16:11:42,910] ERROR Error occurred while reading the remote data for remote-storage-perf-5-0 (kafka.log.remote.RemoteLogReader)
java.nio.BufferOverflowException
        at java.base/java.nio.ByteBuffer.put(ByteBuffer.java:1007)
        at java.base/java.nio.HeapByteBuffer.put(HeapByteBuffer.java:243)
        at org.apache.kafka.common.record.DefaultRecordBatch.writeTo(DefaultRecordBatch.java:236)
        at kafka.log.remote.RemoteLogManager.read(RemoteLogManager.java:1647)
        at kafka.log.remote.RemoteLogReader.lambda$call$0(RemoteLogReader.java:66)
        at com.yammer.metrics.core.Timer.time(Timer.java:91)
        at kafka.log.remote.RemoteLogReader.call(RemoteLogReader.java:66)
        at kafka.log.remote.RemoteLogReader.call(RemoteLogReader.java:36)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:842)



{code}
firstBatch.writeTo(buffer) will throw BufferOverflowException when firstBatch size is larger than  the size of buffer passed to the method.


The fix below appears to resolve this issue, however, the potential side effects remain unknown.


 !screenshot-1.png! ",Patch Available,In Progress,Bug,Major,dyingjiecai,dyingjiecai,KAFKA,Kafka,,,log,,
KAFKA-19470,13622553,2025-07-04T11:10:59.000+0000,2025-07-12T03:34:00.000+0000,,Avoid creating loggers repeatedly to affect the performance of request processing,"We see that the broker has a performance bottleneck when processing requests in kafka 2.8.2 version + jdk 11 environment. The CPU profiler is as follows:

!image-2025-07-04-19-05-51-900.png|width=1065,height=489!

After research, we found that this is a problem with jdk11 but without resolution [https://bugs.openjdk.org/browse/JDK-8266964]  StackWalker has a performance degradation for jdk11.

When creating every logger, log4j2 will invoke StackWalker, the following is the performance data observed with arthas. It takes about 60ms to create each logger.

!image-2025-07-04-19-25-57-774.png|width=725,height=367!

we should set the logger variable of each class to static to avoid wasting performance on creating loggers.

seen in KAFKA-15141, the corresponding classes IncrementalFetchContext/DelayedProduce/SessionlessFetchContext have been processed, but I think a better approach should be to optimize the trait Logging, to avoid similar performance issues later. We can introduce a static Map object to cache the duplicate loggers.

 ",Open,To Do,Improvement,Major,,terrytlu,KAFKA,Kafka,,,core,,
KAFKA-19469,13622523,2025-07-04T06:33:21.000+0000,2025-07-08T10:32:08.000+0000,,Dead-letter queues for share groups,This Jira tracks the development of KIP-1191: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1191%3A+Dead-letter+queues+for+share+groups,Open,To Do,New Feature,Major,,Lan Ding,KAFKA,Kafka,,,,queues-for-kafka,
KAFKA-19467,13622463,2025-07-03T15:14:42.000+0000,2025-07-03T20:52:56.000+0000,,Add a metric for controller thread idleness,KIP-1190: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1190%3A+Add+a+metric+for+controller+thread+idleness],Open,To Do,New Feature,Major,Mahsa Seifikar,Mahsa Seifikar,KAFKA,Kafka,,,,need-kip,
KAFKA-19466,13622386,2025-07-02T19:53:36.000+0000,2025-07-09T17:01:47.000+0000,2025-07-09T17:01:47.000+0000,LogConcurrencyTest should close the log when the test completes,"In 
testUncommittedDataNotConsumedFrequentSegmentRolls() and testUncommittedDataNotConsumed(), we call createLog(), but never close the log when the tests complete.",Resolved,Done,Improvement,Major,Jhen-Yung Hsu,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19465,13622375,2025-07-02T18:08:57.000+0000,2025-07-02T18:12:59.000+0000,,Enable Schema definition for InsertHeader SMT,"InsertHeader transformation doesn't support ByteArray values. This causes issues with using ByteArray convertor as `header.converter`, which is described in a [separate Jira issue|https://issues.apache.org/jira/browse/KAFKA-10428].

 ",Open,To Do,Improvement,Major,,Ed Berezitsky,KAFKA,Kafka,,,connect,,
KAFKA-19462,13622234,2025-07-01T12:14:42.000+0000,2025-07-08T17:55:09.000+0000,2025-07-03T02:46:49.000+0000,"""fetch.max.bytes"" config is not honored when remote + local fetch","Currently in local fetch case, we'll calculate the remaining bytes to be fetched for each partition via ""fetch.max.bytes"" and ""max.partition.fetch.bytes"" configs. For example:
 # Config:
max.partition.fetch.bytes = 1MB
fetch.max.bytes = 1.5MB
 # Topic foo has 2 partitions.
 # Consumer fetches data from topic foo
 # Fetches from foo-0 first, it got 1MB of data (max.partition.fetch.bytes), so remaining 0.5 MB of data available to be fetched
 # Fetches from foo-1 for max 0.5MB.
 # Total returned 1.5MB records

However, in remote + local fetch case, because we don't know how much data we can fetch before querying remote log metadata manager or other resource, we can't have a value to tell replicaManager beforehand. Currently, we treat it as 0 bytes read. And that's why the final returned data could exceed the ""fetch.max.bytes"" value.

For example:
 # Config:
max.partition.fetch.bytes = 1MB
fetch.max.bytes = 1.5MB
 # Topic foo has 2 partitions + topic boo has 1 partition with tiered storage enabled.
 # Consumer fetches data from topic foo and boo
 # Fetches from boo-0, because we don't know how much data we can get, return 0, and send to remote async read.
 # Fetches from foo-0, it got 1MB of data, so remaining 0.5 MB of data available to be fetched
 # Fetches from foo-1 for max 0.5MB.
 # remote async read for boo-0 returned 1MB data (max.partition.fetch.bytes).
 # Total returned 2.5MB records, which exceeds `fetch.max.bytes = 1.5MB`

 ",Resolved,Done,Bug,Major,Luke Chen,Luke Chen,KAFKA,Kafka,,,,,4.2.0
KAFKA-19460,13622218,2025-07-01T09:35:17.000+0000,2025-07-14T08:09:09.000+0000,2025-07-14T04:57:50.000+0000,fetch result might have size < fetch.min.bytes even if data is available in replica ,"In the doc of ""[fetch.min.bytes|https://kafka.apache.org/documentation/#consumerconfigs_fetch.min.bytes]"", it said:

??The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.??

It makes users believe the records returned will always greater fetch.min.bytes if there is sufficient data in replica. But even if the data is sufficient is available in the replica, there is still possible the returned records size < fetch.min.bytes.

 

For example:
 # Config 
fetch.max.bytes=1500
max.partition.fetch.bytes=1000
fetch.min.bytes=1100
fetch.max.wait.ms=500
 # topic foo has 2 partitions, and each partition contains 1 record with size 1000 bytes.
 # When a consumer fetches data from these 2 partitions, it starts from foo-0, and fetch 1000 bytes of data, and 500 bytes left before reaching fetch.max.bytes.
 # When fetching foo-1, since we only have 500 bytes available to be fetched, and the first batch size in foo-1 is 1000 bytes, which is greater than 500, so we don't fetch it.
 # In the end, the total returned size is 1000 bytes, which is less than fetch.min.bytes, without waiting until `fetch.max.wait.ms` expired. It's because we checked the total size in replicas are more than ""fetch.min.bytes"", so no wait for ""fetch.max.wait.ms"".

 

I think the logic is correct. It's just we need to update the doc to make it clear to users. We might also need to check `replica.fetch.min.bytes` config.",Resolved,Done,Improvement,Major,Xuze Yang,Luke Chen,KAFKA,Kafka,,,,,4.2.0
KAFKA-19458,13622186,2025-06-30T23:47:27.000+0000,2025-07-14T09:56:06.000+0000,,Successive AlterReplicaLogDirsRequest on a topic partition may leak log segments,"Successive {{AlterReplicaLogDirsRequest}} to change log directory of a given topic partition may cause log segment leak. Consider the following scenario:

1. A request tries to change the logdir for topic partition {{tp}} from {{d1}} to {{d2}}.
2. The handler invokes {{replicaManager#alterReplicaLogDirs}}
3. A future replica is created as a result of the above method invoking {{partition#maybeCreateFutureReplica}} and cleaning for {{tp}} is disabled as {{logManager#abortAndPauseCleaning}} is invoked.
4. Now, *before* the previous request is completed, let's assume another request to change the logdir from {{d2}} to {{d3}}
5. This time, {{replicaManager#alterReplicaLogDirs}}'s call to {{partition#futureReplicaDirChanged}} will return {{true}} and we remove the fetcher and unset the reference to {{futureLog}} in {{Partition}}.
6. We then re-create a future by invoking {{partition#maybeCreateFutureReplica}} with {{d3}} and pause log cleaning for {{tp}} *again*.
7. {{partition#maybeReplaceCurrentWithFutureReplica}} is invoked when the future has caught up and the callback in it swaps the future log for the local log and resumes cleaning by invoking {{LogManager#resumeCleaning}}.
8. The above decrements the count in {{LogCleaningState.logCleaningPaused}} from {{2}} to {{1}}. Cleanup for {{tp}} is therefore paused until a broker restart",Patch Available,In Progress,Bug,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,,,4.2.0
KAFKA-19452,13622167,2025-06-30T18:33:02.000+0000,2025-07-14T16:48:18.000+0000,2025-07-14T16:48:18.000+0000,Fix flaky test: LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments,"Saw multiple occurrences of the following failure. [https://github.com/apache/kafka/actions/runs/15937460305/job/44960490730?pr=19964] and [https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961.|https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961]

 

 
{code:java}
2025-06-27T23:43:12.2936738Z kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments() failed, log available in /home/runner/work/kafka/kafka/core/build/reports/testOutput/kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments().test.stdout
2025-06-27T23:43:12.2938722Z 
2025-06-27T23:43:12.2939140Z Gradle Test Run :core:test > Gradle Test Executor 17 > LogRecoveryTest > testHWCheckpointWithFailuresMultipleLogSegments() FAILED
2025-06-27T23:43:12.2939940Z     org.opentest4j.AssertionFailedError: Did not observe leader change for partition new-topic-0 after 15000 ms
2025-06-27T23:43:12.2940561Z         at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38)
2025-06-27T23:43:12.2945169Z         at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138)
2025-06-27T23:43:12.2946584Z         at app//kafka.utils.TestUtils$.awaitLeaderChange(TestUtils.scala:719)
2025-06-27T23:43:12.2947923Z         at app//kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:218) {code}
 

 ",Resolved,Done,Improvement,Major,Rajani Karuturi,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19451,13622165,2025-06-30T17:59:26.000+0000,2025-07-14T19:02:19.000+0000,2025-07-14T19:02:19.000+0000,Fix flaky test: RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval(),"Saw the following flaky test in [https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961].
{code:java}
2025-06-28T00:09:57.4324761Z Gradle Test Run :storage:test > Gradle Test Executor 47 > RemoteIndexCacheTest > testCacheEntryIsDeletedOnRemoval() STARTED 2025-06-28T00:09:57.4327688Z org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval() failed, log available in /home/runner/work/kafka/kafka/storage/build/reports/testOutput/org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval().test.stdout 2025-06-28T00:09:57.4329862Z  2025-06-28T00:09:57.4330480Z Gradle Test Run :storage:test > Gradle Test Executor 47 > RemoteIndexCacheTest > testCacheEntryIsDeletedOnRemoval() FAILED 2025-06-28T00:09:57.4332568Z     java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/kafka-RemoteIndexCacheTest6205787022310961862/remote-log-index-cache/2147584984_WMUkQgEpTJ6GksATEZn4iQ.txnindex.deleted 2025-06-28T00:09:57.4334299Z         at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:87) 2025-06-28T00:09:57.4336792Z         at java.base/java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:103) 2025-06-28T00:09:57.4337909Z         at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1855) 2025-06-28T00:09:57.4339038Z         at java.base/java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:129) 2025-06-28T00:09:57.4340181Z         at java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:527) 2025-06-28T00:09:57.4341199Z         at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513) 2025-06-28T00:09:57.4342209Z         at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) 2025-06-28T00:09:57.4343223Z         at java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150) 2025-06-28T00:09:57.4344159Z         at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 2025-06-28T00:09:57.4345155Z         at java.base/java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:652) 2025-06-28T00:09:57.4346710Z         at org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.getIndexFileFromRemoteCacheDir(RemoteIndexCacheTest.java:1290) 2025-06-28T00:09:57.4348431Z         at org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval(RemoteIndexCacheTest.java:339) 2025-06-28T00:09:57.4349345Z  2025-06-28T00:09:57.4349471Z         Caused by: 2025-06-28T00:09:57.4350789Z         java.nio.file.NoSuchFileException: /tmp/kafka-RemoteIndexCacheTest6205787022310961862/remote-log-index-cache/2147584984_WMUkQgEpTJ6GksATEZn4iQ.txnindex.deleted 2025-06-28T00:09:57.4352393Z             at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) 2025-06-28T00:09:57.4353392Z             at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) 2025-06-28T00:09:57.4354360Z             at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) 2025-06-28T00:09:57.4355793Z             at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) 2025-06-28T00:09:57.4356539Z             at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148) 2025-06-28T00:09:57.4357193Z             at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) 2025-06-28T00:09:57.4357721Z             at java.base/java.nio.file.Files.readAttributes(Files.java:1851) 2025-06-28T00:09:57.4358201Z             at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:220) 2025-06-28T00:09:57.4358697Z             at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:277) 2025-06-28T00:09:57.4359159Z             at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:374) 2025-06-28T00:09:57.4359670Z             at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:83) 2025-06-28T00:09:57.4360085Z             ... 11 more {code}
 ",Resolved,Done,Improvement,Major,Lan Ding,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19444,13622001,2025-06-27T14:09:16.000+0000,2025-07-10T13:49:22.000+0000,2025-07-09T09:24:58.000+0000,SASL GSSAPI not working with librdkafka and AK 4.x,"By running librdkafka with AK 4.0 we see that SASL GSSAPI isn't working.
The feature is missing because librdkafka is incorrectly checking for JoinGroup v0 only and not v0+.

When testing librdkafka versions with 4.0 we missed this case so JoinGroup v0 and v1 were removed.

A [fix|https://github.com/confluentinc/librdkafka/pull/5131] is already merged in librdkafka and will be released in v2.11.0.
Enabling back the deprecated JoinGroup RPC versions will help the users looking to continue using Kerberos authentication without upgrading.",Resolved,Done,Bug,Critical,Ismael Juma,Emanuele Sabellico,KAFKA,Kafka,,,,,"4.0.1,4.1.0"
KAFKA-19442,13621964,2025-06-27T09:35:19.000+0000,2025-07-02T08:38:21.000+0000,,"Kafka Streams state store used an outdated value when reducing records, resulting in incorrect output.","We observed a case where a reduce operation used an older version of the store value  instead of the latest when processing a new input.

Below i am describing the sequence of input and its outcome in changelog and output topic

Lets see currently there is a key,value pair  M0, in changelog & output Topic.
and u1,u2,u3 updates are received for the same key in the same order.

For u1,u2 all are fine

But For u3 update , Changelog topic has the *expected Outcome* but Output topic has *incorrect* data.

 

*{*}Sequence:{*}*
|Step|Reduction                           |ChangelogTopic|OutputTopic |
|------   |----------------------------|-------------------|-------------|
|Initial |M0                                   |M0                     |
|u1     |M1 = reduce(M0, u1)       |M1                     |M1          |
|u2     |M2 = reduce(M1, u2)       |M2                     |M2          |
|u3     |M3 = reduce(M2, u3)       |M3                     |❌ Actually: reduce(M1, u3) |

 

Environment:
 - Kafka-clients:3.5.1

 - kafka-streams:3.5.1",Open,To Do,Bug,Major,,juned,KAFKA,Kafka,,,streams,,
KAFKA-19441,13621946,2025-06-27T05:39:45.000+0000,2025-07-15T03:35:39.000+0000,,Encapsulate MetadataImage in GroupCoordinator,"The MetadataImage has a ton of stuff in it and it gets passed around all over the place in the new GroupCoordinator. This makes it difficult to understand what metadata the group coordinator actually relies on and makes it too easy to use metadata in ways it wasn't meant to be used. 

 

If we encapsulate the MetadataImage in an interface that clearly indicates what metadata the group coordinator actually uses, it is much easier at a glance to see what dependencies it has on the metadata. Also, if the source of metadata needs to evolve for whatever reason, we can do so easily by wrapping MetadataImage in an interface.",Open,To Do,Improvement,Major,Liz Bennett,Liz Bennett,KAFKA,Kafka,,,group-coordinator,,
KAFKA-19435,13621752,2025-06-25T10:01:30.000+0000,2025-07-14T14:13:12.000+0000,2025-07-14T14:13:12.000+0000,Optimize `kafka-consumer-groups.sh` to return the offset info of other partitions even when the leader of some partitions are missing,"When we use the `{{{}kafka-consumer-groups.sh`{}}} script to query the LAG of some consumer group, if the leader of the corresponding topic-partition is missing, it will directly throw an exception, which may cause the following problems:
 # There are 10 partitions in total, but if only 1 partition lacks a leader, users will be unable to view the LAG of the other 9 partitions.
 # Throwing an exception directly will subconsciously make users think that there is a major failure in the cluster.

 

Perhaps we can optimize this script to return as much information as possible instead of throwing an exception directly.

 

 ",Resolved,Done,Improvement,Minor,kangning.li,kangning.li,KAFKA,Kafka,,,admin,,4.2.0
KAFKA-19431,13621684,2025-06-24T14:24:14.000+0000,2025-07-11T10:04:39.000+0000,,Stronger assignment consistency with subscription for consumer groups ,"Currently, consumer group assignments are eventually consistent with subscriptions: when a member has unrevoked partitions, it is not allowed to reconcile with the latest target assignment. If a member with unrevoked partitions shrinks its subscription, it may observe assignments from the broker containing topics it is no longer subscribed to.

If we wanted to, we could tighten this up at the cost of extra CPU time. Note that it's not feasible to close the gap for regex subscriptions, since there will always be a window when the regex is not yet resolved and we cannot tell whether a topic is part of the subscription.

One way to do this would be to update {{CurrentAssignmentBuilder}} and
 * Add a {{MetadataImage}} and map of resolved regexes
 * Define the set of subscribed topic uuids as the union of the topic name subscription and resolved regex topic names, like how {{TargetAssignmentBuilder}} does it.
 ** When the regex is unresolved, we can’t know which topics are part of the subscription. We treat unresolved regexes as matching no topics, to be conservative. This way, the assignment is always consistent with the subscription.
 * Update the loop over topics in {{computeNextAssignment}} to treat the assigned partitions as an empty set when the topic is not part of the subscription.
 * Do not advance the member epoch past the target assignment epoch when exiting the {{UNREVOKED_PARTITIONS}} state.
 * Define an {{updateCurrentAssignment}} method that drops any unsubscribed topics from the member’s current assignment and transitions the member to {{UNREVOKED_PARTITIONS}} if any topics were dropped.
 * Use {{updateCurrentAssignment}} on the other {{UNREVOKED_PARTITIONS}} path.

Additionally, if we ever end up with asynchronous assignors (such as client-computed ones),
 * We add a new flag to {{maybeReconcile}} called {{{}hasSubscriptionChanged{}}}. When the flag is set, we run the {{CurrentAssignmentBuilder}} even when reconciled to the target assignment, since the target assignment can lag behind the group epoch.
 * Use {{updateCurrentAssignment}} on all {{CurrentAssignmentBuilder}} paths that do not use {{{}computeNextAssignment{}}}.",Open,To Do,Improvement,Major,Sean Quah,Sean Quah,KAFKA,Kafka,,,group-coordinator,,
KAFKA-19430,13621594,2025-06-23T19:58:19.000+0000,2025-07-09T20:57:15.000+0000,,Don't fail on RecordCorruptedException,"From [https://github.com/confluentinc/kafka-streams-examples/issues/524]

Currently, the existing `DeserializationExceptionHandler` is applied when de-serializing the record key/value byte[] inside Kafka Streams. This implies that a `RecordCorruptedException` is not handled.

We should explore to not let Kafka Streams crash, but maybe retry this error automatically (as `RecordCorruptedException extends RetriableException`), and find a way to pump the error into the existing exception handler.

If the error is transient, user can still use `REPLACE_THREAD` in the uncaught exception handler, but this is a rather heavy weight approach.",Open,To Do,Improvement,Major,Uladzislau Blok,Matthias J. Sax,KAFKA,Kafka,,,streams,,
KAFKA-19429,13621567,2025-06-23T13:31:48.000+0000,2025-07-01T19:48:17.000+0000,2025-06-24T12:52:13.000+0000,Deflake streams_smoke_test,"It looks like we are checking for properties that are not guaranteed under at_least_once, for example, exact counting (not allowing for overcounting).",Resolved,Done,Bug,Major,Lucas Brutschy,Lucas Brutschy,KAFKA,Kafka,,,"streams,system tests",,
KAFKA-19427,13621524,2025-06-23T07:35:07.000+0000,2025-07-15T12:54:39.000+0000,,"The __consumer_offsets topic applies the broker configuration message.max.bytes, which may cause the coordinator broker to allocate too much memory and cause OOM","h3. Kafka cluster configuration

1.Kafka version：4.0
2.The cluster specifications are: 3 brokers and 3 controllers
3.JVM startup parameters:
!image-2025-06-23-14-16-00-554.png!

4.JDK version：
!image-2025-06-23-14-17-34-767.png!
h3. Steps to reproduce the problem

1.In this new cluster, create a test topic: {*}test{*}，and this cluster will eventually have *only this one topic* tested by external users.

topic config : NewTopic newTopic = new NewTopic(""test"", 3, (short) 1);
2.Start the producer and send 1,000 messages
3.Start the consumer and use the earliest strategy for consumption. The groupIds are rivenTest1/rivenTest2/.../rivenTest8

4.During the process of starting the consumer, it was found that some consumer groups failed to start, and the coordinator brokers corresponding to these groups also had OOM exceptions

client error logs：
{code:java}
[main] INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
[main] INFO org.apache.kafka.common.security.authenticator.AbstractLogin - Successfully logged in.
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.9.1
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: f745dfdcee2b9851
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750661985923
[main] INFO org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Subscribed to topic(s): test
[main] INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Cluster ID: 3esGOWhETi-zo2uHq7NsFg
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Discovered group coordinator 18-97-25-88-k.mq.zoomdev.us:9889 (id: 2147483644 rack: null)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: need to re-join with the given member-id: consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Successfully joined group with generation Generation{generationId=17, memberId='consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4', protocol='roundrobin'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Finished assignment for group at generation 17: {consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4=Assignment(partitions=[test-0, test-1, test-2])}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: rebalance failed due to 'Unexpected error from SyncGroup: The server experienced an unexpected error when processing the request.' (KafkaException)
org.apache.kafka.common.KafkaException: Unexpected error from SyncGroup: The server experienced an unexpected error when processing the request.
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:893)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:812)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1311)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1286)
    at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:429)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:314)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:253)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.pollForFetches(ClassicKafkaConsumer.java:692)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.poll(ClassicKafkaConsumer.java:623)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.poll(ClassicKafkaConsumer.java:596)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
    at us.zoom.mq.examples.ConsumerTest.startConsumer(ConsumerTest.java:233)
    at us.zoom.mq.examples.ConsumerTest.main(ConsumerTest.java:149)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Member consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4 sending LeaveGroup request to coordinator 18-97-25-88-k.mq.zoomdev.us:9889 (id: 2147483644 rack: null) due to the consumer is being closed
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Resetting generation and member id due to: consumer pro-actively leaving the group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: consumer pro-actively leaving the group
[main] ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] LeaveGroup request with Generation{generationId=17, memberId='consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4', protocol='roundrobin'} failed with error: The server experienced an unexpected error when processing the request.
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
[main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
[main] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-rivenTest6-1 unregistered {code}
coordinator broker error logs：
!image-2025-06-23-15-04-26-598.png!

 

 
h3. Analysis:

This is a brand new Kafka4.0 cluster with only one topic created;
The JDK version is 17;
Why does the broker encounter OOM so quickly when it is just sending and consuming data? Is there a memory leak somewhere?
1 First, use the arthas tool to analyze the memory usage

!image-2025-06-23-15-04-15-708.png!

We can see that most of the heap memory is {*}occupied by the old generation{*}, and it is likely that the program will directly experience OOM of the heap memory when it needs to {color:#ff0000}*apply for a large object. It should be noted that the maximum memory we allocate to the Kafka process is actually 3G, and there is also a lot of space left in the heap memory. Why does it directly trigger the Java heap space type OOM in this case?*{color}

{color:#172b4d}2.Dump memory snapshots and use tools to analyze what is currently occupying a large amount of memory in the program
!image-2025-06-23-15-33-06-851.png!

!image-2025-06-23-15-33-26-209.png!
After analyzing the memory usage, I found that it was basically all the *coordinators* objects in the *CoordinatorRuntime* class that occupied the memory and did not release it; coordinators is a ConcurrentHashMap structure, the key is the TopicPartition type, and the value is the CoordinatorContext type.
!image-2025-06-23-15-11-13-026.png!

Why does a broker machine simply start a consumer, the topic has only three partitions, and the consumer group uses no more than 10 partitions in total, and the *coordinators* object in the broker process occupies such a large amount of memory and does not release it?
Is there a problem with the broker configuration or the JDK17 version or the jvm startup parameters, or is there a memory leak in the kafka 4.0 version code?{color}{color:#172b4d}Please help analyze and answer, looking forward to your reply.
Thank you very much!{color}",Open,To Do,Bug,Critical,,RivenSun,KAFKA,Kafka,,,"consumer,group-coordinator",,
KAFKA-19425,13621466,2025-06-21T11:34:41.000+0000,2025-07-03T13:12:47.000+0000,,local segment on disk never deleted forever when remote storage initial failed,"remote storage initial failed is silence so that the disk keep increasing forever.
[stop the server when fail to initialize to avoid local segment never got deleted. by jiafu1115 · Pull Request #20007 · apache/kafka|https://github.com/apache/kafka/pull/20007]",Open,To Do,Bug,Critical,,fujian,KAFKA,Kafka,3600.0,,Tiered-Storage,,
KAFKA-19400,13620670,2025-06-11T14:00:42.000+0000,2025-07-14T14:38:11.000+0000,,Update AddRaftVoterRPC to support controller auto-joining,"When AddRaftVoterRPCs are sent as part of auto-joining, the active controller should send a response after the new voter set has been appended to only its own log. This allows the auto-joining replica to fetch the new voter set.",Open,To Do,Improvement,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19397,13620540,2025-06-10T11:56:44.000+0000,2025-07-07T11:56:19.000+0000,2025-07-07T11:56:19.000+0000,TransactionManager.handleCompletedBatch throws NPE,"Sometimes, current trunk throws the following NPE:

 
{code:java}
[2025-05-29 04:06:05,855] ERROR [kafka-producer-network-thread | i-07bbab180f6062ba3-StreamThread-3-producer] [Producer clientId=i-07bbab180f6062ba3-StreamThread-3-producer] Uncaught error in request completion: (org.apache.kafka.clients.NetworkClient)
java.lang.NullPointerException: Cannot read field ""topicPartition"" because ""batch"" is null
at org.apache.kafka.clients.producer.internals.TransactionManager.handleCompletedBatch(TransactionManager.java:748)
at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:736)
at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:710)
at org.apache.kafka.clients.producer.internals.Sender.lambda$handleProduceResponse$2(Sender.java:613)
at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
at org.apache.kafka.clients.producer.internals.Sender.lambda$handleProduceResponse$3(Sender.java:597)
at java.base/java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:597)
at org.apache.kafka.clients.producer.internals.Sender.lambda$sendProduceRequest$9(Sender.java:895)
at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:154)
at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:669)
at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:661)
at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:340)
at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:242)
at java.base/java.lang.Thread.run(Thread.java:840)
 
 
{code}
 

This was discovered in a long-running test, so we do not have a directly reproducible test case. However, DEBUG logs are included below, which show the sequence of METADATA and PRODUCE requests / responses that seem to cause this.

 

Likely cause is the change here: [https://github.com/apache/kafka/pull/15968]",Resolved,Done,Bug,Blocker,Omnia Ibrahim,Lucas Brutschy,KAFKA,Kafka,,,"clients,producer ",,4.1.0
KAFKA-19395,13620494,2025-06-10T04:54:06.000+0000,2025-07-11T08:22:37.000+0000,,The version and license information in the `NOTICE-binary` file for `JUnit` are inconsistent,"[1]: [https://github.com/apache/kafka/blob/3a0a1705a1a7caa9b4b14158b05325138c904451/NOTICE-binary#L163]

[2]:

https://github.com/apache/kafka/blob/3a0a1705a1a7caa9b4b14158b05325138c904451/NOTICE-binary#L251",Open,To Do,Improvement,Minor,Nick Guo,Nick Guo,KAFKA,Kafka,,,,,
KAFKA-19390,13620394,2025-06-09T05:47:05.000+0000,2025-07-15T17:07:04.000+0000,2025-07-08T16:16:09.000+0000,AbstractIndex#resize() does not release old mmap on Linux,"Our kafka broker crashed with the following error:
{code:java}
[2025-03-29 09:37:03,218] ERROR Error while appending records to <topic>-<partition> in dir /kafka-logs/data ...
java.io.IOException: Map failed
at java.base/sun.nio.ch.FileChannelImpl.mapInternal(FileChannelImpl.java:1127)
at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1032)
at org.apache.kafka.storage.internals.log.AbstractIndex.createMappedBuffer(AbstractIndex.java:467)
at org.apache.kafka.storage.internals.log.AbstractIndex.createAndAssignMmap(AbstractIndex.java:105)
at org.apache.kafka.storage.internals.log.AbstractIndex.<init>(AbstractIndex.java:83)
at org.apache.kafka.storage.internals.log.TimeIndex.<init>(TimeIndex.java:65)
at org.apache.kafka.storage.internals.log.LazyIndex.loadIndex(LazyIndex.java:242)
at org.apache.kafka.storage.internals.log.LazyIndex.get(LazyIndex.java:179)
at org.apache.kafka.storage.internals.log.LogSegment.timeIndex(LogSegment.java:146)
at org.apache.kafka.storage.internals.log.LogSegment.readMaxTimestampAndOffsetSoFar(LogSegment.java:201)
at org.apache.kafka.storage.internals.log.LogSegment.maxTimestampSoFar(LogSegment.java:211)
at org.apache.kafka.storage.internals.log.LogSegment.append(LogSegment.java:262)
at kafka.log.LocalLog.append(LocalLog.scala:417)
...
Caused by: java.lang.OutOfMemoryError: Map failed
at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
at java.base/sun.nio.ch.FileChannelImpl.mapInternal(FileChannelImpl.java:1124)
... 33 more{code}
We found that kafka process hit the vm.max_map_count limit (which was set to 262144) and most of the mapped entries correspond to deleted index files.
{code:java}
> sudo cat /proc/${KAFKA_PID}/maps | grep deleted
7d8c5cc00000-7d8c5d600000 rw-s 00000000 08:11 202854769 /kafka-logs/data/topic1-22/00000000332910579773.timeindex.deleted (deleted)
7d8c5d800000-7d8c5e200000 rw-s 00000000 08:11 202854768 /kafka-logs/data/topic1-22/00000000332910579773.index.deleted (deleted)
7d8c67400000-7d8c67e00000 rw-s 00000000 08:11 202562514 /kafka-logs/data/topic2-116/00000000165968090794.timeindex.deleted (deleted)
7d8c68000000-7d8c68a00000 rw-s 00000000 08:11 202562513 /kafka-logs/data/topic2-116/00000000165968090794.index.deleted (deleted)
7d8c6d400000-7d8c6de00000 rw-s 00000000 08:11 202596518 /kafka-logs/data/topic2-356/00000000168702579081.timeindex.deleted (deleted)
7d8c6e000000-7d8c6ea00000 rw-s 00000000 08:11 202596517 /kafka-logs/data/topic2-356/00000000168702579081.index.deleted (deleted)
7d8c71c00000-7d8c72600000 rw-s 00000000 08:11 202798981 /kafka-logs/data/topic3-433/00000000116740630582.timeindex.deleted (deleted)
7d8c72800000-7d8c73200000 rw-s 00000000 08:11 202798980 /kafka-logs/data/topic3-433/00000000116740630582.index.deleted (deleted)
7d8c77c00000-7d8c78600000 rw-s 00000000 08:11 202754947 /kafka-logs/data/topic3-74/00000000118067749684.timeindex.deleted (deleted)
7d8c78800000-7d8c79200000 rw-s 00000000 08:11 202754946 /kafka-logs/data/topic3-74/00000000118067749684.index.deleted (deleted)
7d8c79400000-7d8c79e00000 rw-s 00000000 08:11 202813710 /kafka-logs/data/topic2-82/00000000162756700035.timeindex.deleted (deleted)
7d8c7a000000-7d8c7aa00000 rw-s 00000000 08:11 202813709 /kafka-logs/data/topic2-82/00000000162756700035.index.deleted (deleted)
7d8c7ac00000-7d8c7b600000 rw-s 00000000 08:11 202596526 /kafka-logs/data/topic2-355/00000000169939763750.timeindex.deleted (deleted)
7d8c7b800000-7d8c7c200000 rw-s 00000000 08:11 202596525 /kafka-logs/data/topic2-355/00000000169939763750.index.deleted (deleted)
7d8c7c400000-7d8c7ce00000 rw-s 00000000 08:11 202562498 /kafka-logs/data/topic2-295/00000000168913981903.timeindex.deleted (deleted)
7d8c7d000000-7d8c7da00000 rw-s 00000000 08:11 202562497 /kafka-logs/data/topic2-295/00000000168913981903.index.deleted (deleted)
7d8c80c00000-7d8c81600000 rw-s 00000000 08:11 202754939 /kafka-logs/data/topic3-13/00000000115588098896.timeindex.deleted (deleted)
7d8c81800000-7d8c82200000 rw-s 00000000 08:11 202754938 /kafka-logs/data/topic3-13/00000000115588098896.index.deleted (deleted)
7d8c83c00000-7d8c84600000 rw-s 00000000 08:11 202798989 /kafka-logs/data/topic3-314/00000000118254254601.timeindex.deleted (deleted)
7d8c84800000-7d8c85200000 rw-s 00000000 08:11 202798988 /kafka-logs/data/topic3-314/00000000118254254601.index.deleted (deleted)
...{code}
In AbstractIndex.resize(), the old memory mapping is explicitly unmapped on windows or z/OS using safeForceUnmap(), but on Linux the unmapping step is skipped.
The same issue was originally reported in KAFKA-7442, but the corresponding pull request was never merged.
We propose that resize() should call safeForceUnmap() on all platforms to prevent stale mappings from lingering.

ref: [https://speakerdeck.com/lycorptech_jp/20250609b]
h2. Test

We tested our patch on the following system environment and found no measurable performance regression.

*Environment*
 * Kafka: 3.8.1
 * Java: OpenJDK 17
 * OS: Rocky Linux 9
 * Broker: 10 brokers for original, 10 brokers for patched
 ** CPU: Intel Xeon Silver 4310 (2 sockets)
 ** Memory: 250 GB
 ** Disk: HDD

*Workloads*
 * Peak throughput per broker:
 ** 6K req/s
 ** 85.2 MB/s

h3. Idle percentage of request handler and network processor threads

original:

!orig_request_handler.png|width=1000,height=222!

!orig_network_processor.png|width=999,height=163!

patched:

!patched_request_handler.png|width=997,height=228!

!patched_network_processor.png|width=1004,height=160!
h3. Resource load

original:

!orig_cpu.png|width=1000,height=227!

!orig_disk_io.png|width=997,height=168!

patched:

!patched_cpu.png|width=999,height=227!

!patched_disk_io.png|width=1002,height=160!",Resolved,Done,Bug,Major,Masahiro Mori,Masahiro Mori,KAFKA,Kafka,,,core,Linux,4.2.0
KAFKA-19384,13620318,2025-06-06T19:41:43.000+0000,2025-07-07T01:51:24.000+0000,,The passing of BrokerRegistrationRequestTest is a false positive,"All requests used in testing are ""failed"" due to isMigratingZkBroker, and therefore the cases we want to test are not actually covered.",In Progress,In Progress,Improvement,Minor,HongYi Chen,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19371,13620056,2025-06-04T13:27:41.000+0000,2025-07-02T05:53:14.000+0000,2025-07-02T05:53:14.000+0000,"When a broker restarts, it should not attempt to create the __remote_log_metadata topic if it already exists.","*[Precondition]*
Kafka cluster already enabled the remote storage feature based on inner topic's implementation. The core inner topic ""__remote_log_metadata"" already created.
 
*[Steps]*

1. Restart one broker of the Kafka cluster.

2. Check the log and the code logic for the ""__remote_log_metadata""s creating when broker restarting
 
*[Expect result]*
The broker shouldn't attempt to call API to create the topic due to that it already existed.
 
*[Actual result]*
The results are different which depend on the start process' duration for broker:
*Case 1: Happy Path when restarting take a short time*
[2025-06-03 22:35:11,648] INFO Topic __remote_log_metadata {color:#00875a}exists{color}. TopicId: 4CT2TTC-R6u7fNo_njYlDA, numPartitions: 50,

*Case 2: Unhappy path 1 when restarting take some time*
[2025-06-03 23:59:40,505] INFO Topic __remote_log_metadata{color:#de350b} does not exist{color}. Error: Timed out waiting for a node assignment. Call: listNodes
[2025-06-04 00:00:36,938] INFO Topic [__remote_log_metadata] {color:#de350b}already exists{color}
*Case 3: Unhappy path 2 when restarting take a long time.*
[2025-06-03 21:57:21,151] INFO Topic __remote_log_metadata {color:#de350b}does not exist{color}. Error: {color:#de350b}Timed out waiting{color} for a node assignment. Call: {color:#de350b}listNodes {color}at
[2025-06-03 21:58:21,153] ERROR Encountered error while creating __remote_log_metadata topic. java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.{color:#de350b}TimeoutException{color}: Timed out waiting for a node assignment. Call: {color:#de350b}createTopics {color}at
 
From the log and current code. we can know that {color:#de350b}case 2 and case 3 both give the prompt ""the topic does not exist"" and try to call topic creating API. In actually. it is useless and contradict the fact that the topic already existed. Especially. the case 2's log prompt the topic existed and not existed at the same time.{color}
 
*[Root Cause analyst]*
After reviewing the related code (TopicBasedRemoteLogMetadataManager#doesTopicExist). It is one {color:#de350b}wrong implement{color} to judge one topic existed or not.
So let me create this [PR |#19899 · apache/kafka]to fix this minor bug. Thanks
 
FYI:
Why we got the timeout exception?
It is normal case based on the fact:
When restarting broker. The connection to query/create topic in ""TopicBasedRemoteLogMetadataManager#initializeResources""will fail until the broker's self  get ready.
[2025-06-03 23:21:20,752] WARN [AdminClient clientId=adminclient-1] Connection to node -1 ([10.20.1.125:9559)|https://10-20-1-125/] could not be established. Node may not be available.
[2025-06-03 23:21:21,282] INFO [BrokerServer id=2] Transition from STARTING to STARTED (kafka.server.BrokerServer)",Resolved,Done,Bug,Major,,fujian,KAFKA,Kafka,259200.0,,Tiered-Storage,,4.2.0
KAFKA-19357,13619796,2025-06-01T07:50:22.000+0000,2025-07-15T04:17:09.000+0000,,AsyncConsumer#close hangs during closing because the commitAsync request never completes due to a missing coordinator,"KAFKA-16103 ensures the AsyncConsumer wait the requests sent by commitAsync during the close. However, the `CoordinatorRequestManager` does not send any request to find coordinator during close, and hence the requests sent by `CommitRequestManager are never completed during the close since there is no coordinator",Open,To Do,Bug,Major,Yu Chia Ma,Chia-Ping Tsai,KAFKA,Kafka,,,"clients,consumer",,4.2.0
KAFKA-19354,13619705,2025-05-30T17:59:08.000+0000,2025-07-09T15:16:00.000+0000,,KRaft observer unable to recover after re-bootstrapping to follower,"[Original dev mail thread|https://lists.apache.org/thread/ws3390khsxhdg2b8cnv2mzv8slz5xq7q]

If an observer's FETCH request to the quorum leader experiences a failure/timeout, it is possible that when it re-bootstraps, it will connect to a follower node (random selection). Subsequently, the observer node will continually send FETCH requests to that follower, and in receive a response with a ""partitionError"" errorCode=6 (NOT_LEADER_OR_FOLLOWER), which does not trigger a re-bootstrap.
Thus, the observer will be stuck sending FETCH requests to the follower instead of the leader, halting metadata replication and causing it to fall out of sync.

To recover from this state, re-bootstrapping would need to occur by restarting the affected observer or follower, until it connects to the correct leader.

*Steps to reproduce:*
1. Spin up Kafka cluster with 3 or 5 controllers. (ideally 5 to increase likelihood of bootstrapping to a follower instead of the leader)
2. Enable a network delay on a particular observer broker (e.g. `tc qdisc add dev eth0 root netem delay 2500ms`). I picked 2500ms since default timeout is 2s for `controller.quorum.fetch.timeout.ms`/`controller.quorum.request.timeout.ms`. After a few seconds, disable the network delay (e.g. `tc qdisc del dev eth0 root netem`).
3. The observer node will re-bootstrap, potentially to a follower instead of the leader. If so, the observer will continuously send fetch requests to the follower node, receive `NOT_LEADER_OR_FOLLOWER` in response, and will no longer replicate metadata.

*Debug logs demonstrating this scenario:*
- https://gist.github.com/justin-chen/1f3eee79d9a5066a467818a0b1bc006f
- kraftcontroller-3 (leader), kraftcontroller-4 (follower), kafka-0 (observer)
",Patch Available,In Progress,Bug,Major,Alyssa Huang,Justin Chen,KAFKA,Kafka,,,kraft,,
KAFKA-19306,13618461,2025-05-18T17:54:52.000+0000,2025-07-11T18:00:44.000+0000,,Migrate LogCompactionTester to tool module,as title,Open,To Do,Improvement,Major,Yunchi Pang,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19305,13618460,2025-05-18T16:27:35.000+0000,2025-07-14T18:53:16.000+0000,,Ensure all image classes are immutable,"The collections in ClientQuotaImage, TopicImage, and ScramImage are not wrapped to ensure immutability. In contrast, the collections in other image classes are.",Open,To Do,Improvement,Minor,Chang Chi Hsu,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19279,13618079,2025-05-14T09:03:13.000+0000,2025-07-14T05:55:03.000+0000,,DefaultMessageFormatter is inaccessible as it's package-private class,"As per [upgrade-notes|
https://github.com/apache/kafka/blob/ec70c4436210b7578777739091e365b03c69946f/docs/upgrade.html#L259],  it has been told as below
{code:java}
kafka.tools.DefaultMessageFormatter{code}
class was removed. 
 
Please use the 
{code:java}
org.apache.kafka.tools.consumer.DefaultMessageFormatter{code}
class instead.
 
 
Now 
{code:java}
org.apache.kafka.tools.consumer.DefaultMessageFormatter{code}
is package-private class which means, we can't directly use it as below
 
{code:java}
private val defaultFormatter: DefaultMessageFormatter = new DefaultMessageFormatter {code}
 
It throws error as 
{code:java}
Symbol DefaultMessageFormatter is inaccessible from this place {code}
Due ot this, we can't use other member variable `printHeaders`. 
 
This is happened when we upgraded kafka-tools from 3.4.1 to 3.9.0 version. 
 
Need help in this regarding. ",Patch Available,In Progress,Bug,Major,,Aniruddha Navare,KAFKA,Kafka,,,tools,,
KAFKA-19259,13617707,2025-05-09T20:25:34.000+0000,2025-07-10T00:10:14.000+0000,,Async consumer fetch intermittent delays on console consumer,"We noticed that fetching with the kafka-console-consumer.sh tool using the new consumer shows some intermittent delays, that are not seen when running the same with the classic consumer. Note that I disabled auto-commit to isolate the delay, and from a first look seems to come from the fetchBuffer.awaitNonEmpty logic, that alternatively takes almost the full poll timeout (runs ""fast"", then ""slow"", and continues to alternate)

[https://github.com/apache/kafka/blob/0b81d6c7802c1be55dc823ce51729f2c6a6071a7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1808]  

The difference in behaviour between the 2 consumers can be seen with this setup:
 * topic with 6 partitions (I tried with 1 partition first and didn't see the delay, then with 3 and 6 I could see it) 
 * data populated in topic with producer sending generated uuids to the topic in while loop 
 * run console consumer (asycn) no commit:

bin/kafka-console-consumer.sh --topic t1 --bootstrap-server localhost:9092 --consumer-property group.protocol=consumer --group cg1 --consumer-property enable.auto.commit=false
Here we can notice the pattern that looks like batches, and custom logs on the awaitNonEmpty show it take the full poll timeout on alternate poll iterations.

 * run same but for classic consumer (consumer-property group.protocol=classic) -> not such pattern of intermittent delays

Produce continuously (I used this) 
while sleep 1; do echo $(uuidgen); done | bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic t1

This needs more investigation to fully understand if it's indeed something in the fetch path or something else) ",Open,To Do,Bug,Major,Arpit Goyal,Lianet Magrans,KAFKA,Kafka,,,"clients,consumer",consumer-threading-refactor,4.2.0
KAFKA-19254,13617477,2025-05-07T20:15:16.000+0000,2025-07-14T20:28:30.000+0000,2025-07-14T20:28:30.000+0000,Add generic feature level metric,KIP link: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1180%3A+Add+a+generic+feature+level+metric,Resolved,Done,New Feature,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19248,13617319,2025-05-06T17:59:22.000+0000,2025-07-09T12:49:07.000+0000,2025-07-09T12:48:55.000+0000,Plugins Test for Multiversioning in Kafka Connect,Add tests for plugin level isolation for Kafka connect (KIP-891).,Resolved,Done,Improvement,Major,,Snehashis Pal,KAFKA,Kafka,,,connect,,4.1.0
KAFKA-19221,13616835,2025-04-30T16:05:31.000+0000,2025-07-05T19:07:07.000+0000,2025-07-05T19:07:07.000+0000,IOException on log segment close shouldn't be ignored,"Related to the findings in KAFKA-19200.

An IOException in {{TimeIndex#close}} is currently silently ignored. This may potentially lead to a situation where {{TimeIndex#trimToValidSize}} fails and the next broker startup will not attempt to recover the log segment as it considers the shutdown as ""clean"", eventually causing the symptoms described in the aforementioned JIRA.

",Resolved,Done,Bug,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,,,"4.0.1,4.1.0"
KAFKA-19213,13616707,2025-04-29T15:03:08.000+0000,2025-07-15T00:19:52.000+0000,,Kafka java client ignores default properties,"Using Kafka Java client, when creating a kafka consumer/producer using a properties object with defaults the defaults are ignored.

Not sure if this is an intentional design choice or not but it was a surprise to me.

My expectation was if a KafkaConsumer or KafkaProducer accepts a Properties object it should not ignore the defaults in the Properties object.

 

The culprit seems to be in the propsToMap in the Utils class.

It doesn't use the propertyNames method to get all the keys known to the Properties object.
{code:java}
public static Map<String, Object> propsToMap(Properties properties) {
    Map<String, Object> map = new HashMap<>(properties.size());
    for (Map.Entry<Object, Object> entry : properties.entrySet()) {
        if (entry.getKey() instanceof String) {
            String k = (String) entry.getKey();
            map.put(k, properties.get(k));
        } else {
            throw new ConfigException(entry.getKey().toString(), entry.getValue(), ""Key must be a string."");
        }
    }
    return map;
}
{code}
I was using version 7.6.0-ccs of the kafka client, on the trunk branch the implementation seems different but is subject to the same issue:
{code:java}
   /**
     * Convert a properties to map. All keys in properties must be string type. Otherwise, a ConfigException is thrown.
     * @param properties to be converted
     * @return a map including all elements in properties
     */
    public static Map<String, Object> propsToMap(Properties properties) {
        return castToStringObjectMap(properties);
    }   

 /**
     * Cast a map with arbitrary type keys to be keyed on String.
     * @param inputMap A map with unknown type keys
     * @return A map with the same contents as the input map, but with String keys
     * @throws ConfigException if any key is not a String
     */
    public static Map<String, Object> castToStringObjectMap(Map<?, ?> inputMap) {
        Map<String, Object> map = new HashMap<>(inputMap.size());
        for (Map.Entry<?, ?> entry : inputMap.entrySet()) {
            if (entry.getKey() instanceof String) {
                String k = (String) entry.getKey();
                map.put(k, entry.getValue());
            } else {
                throw new ConfigException(String.valueOf(entry.getKey()), entry.getValue(), ""Key must be a string."");
            }
        }
        return map;
    }{code}
 

A possible suggestion for how to fix the behavior:

 
{code:java}
public static Map<String, Object> propsToMap(Properties properties) {
    final Enumeration<?> enumeration = properties.propertyNames();

    Map<String, Object> props = new HashMap<>();
    while (enumeration.hasMoreElements()) {
        Object key = enumeration.nextElement();
        if (key instanceof String) {
            final String keyString = (String) key;
            props.put(keyString,properties.getProperty(keyString));
        } else {
            throw new ConfigException(String.valueOf(key),properties.get(key), ""Key must be a string."");
        }
    }
    return props;
}
 {code}
Provided a basic test case to demonstrate the issue. See file attachment.

 

 

 ",Open,To Do,Bug,Minor,Evanston Zhou,Pas Filip,KAFKA,Kafka,,,clients,,
KAFKA-19191,13616177,2025-04-23T19:59:30.000+0000,2025-07-09T12:45:18.000+0000,2025-04-24T15:02:19.000+0000,Don't load bootstrap.checkpoint files if the cluster is already bootstrapped,"Clusters bootstrapped with a metadata version that is already deprecated throw the following exception:
{code:java}
java.lang.IllegalArgumentException: No MetadataVersion with metadata version 
        ...
	at org.apache.kafka.metadata.bootstrap.BootstrapMetadata.recordToMetadataVersion(BootstrapMetadata.java:109)
	at org.apache.kafka.metadata.bootstrap.BootstrapMetadata.fromRecords(BootstrapMetadata.java:90)
	at org.apache.kafka.metadata.bootstrap.BootstrapDirectory.readFromBinaryFile(BootstrapDirectory.java:91)
	at org.apache.kafka.metadata.bootstrap.BootstrapDirectory.read(BootstrapDirectory.java:72)
	at kafka.server.KafkaRaftServer$.initializeLogDirs(KafkaRaftServer.scala:206)
	at kafka.server.KafkaRaftServer.<init>(KafkaRaftServer.scala:57)
	at kafka.Kafka$.buildServer(Kafka.scala:68)
	at kafka.Kafka$.main(Kafka.scala:78)
	at kafka.Kafka.main(Kafka.scala) {code}",Resolved,Done,Bug,Blocker,Colin McCabe,José Armando García Sancio,KAFKA,Kafka,,,controller,,4.0.1
KAFKA-19152,13615401,2025-04-15T18:27:54.000+0000,2025-07-02T13:58:01.000+0000,2025-07-01T07:15:08.000+0000,Add top-level documentation for OAuth flows,"From [~omkreddy] on the KIP-1139 discussion thread:

{quote}> 5. We currently lack user-facing documentation for OAuth. As part of the
> implementation, it would be helpful to include:
>     - Example client configurations
>     - A full end-to-end usage guide for the JWT bearer grant flow in Kafka
{quote}",Resolved,Done,Improvement,Major,Kirk True,Kirk True,KAFKA,Kafka,,,"clients,documentation","OAuth2,kip-1139",4.1.0
KAFKA-19130,13615035,2025-04-11T22:46:59.000+0000,2025-07-01T13:33:20.000+0000,2025-07-01T13:33:20.000+0000,Do not add fenced brokers to BrokerRegistrationTracker on startup,"When the controller starts up (or becomes active after being inactive), we add all of the
registered brokers to BrokerRegistrationTracker so that they will not be accidentally fenced the
next time we are looking for a broker to fence. We do this because the state in
BrokerRegistrationTracker is ""soft state"" (it doesn't appear in the metadata log), and the newly
active controller starts off with no soft state. (Its soft state will be populated by the brokers
sending heartbeat requests to it over time.)

In the case of fenced brokers, we are not worried about accidentally fencing the broker due to it
being missing from BrokerRegistrationTracker for a while (it's already fenced). Therefore, it
should be reasonable to just not add fenced brokers to the tracker initially.

One case where this change will have a positive impact is for people running single-node demonstration
clusters in combined KRaft mode. In that case, when the single-node cluster is taken down and
restarted, it currently will have to wait about 9 seconds for the broker to come up and
re-register. With this change, the broker should be able to re-register immediately.

One possible negative impact is that if there is a controller failover, it will open a small window
where a broker with the same ID as a fenced broker could re-register. However, our detection of
duplicate broker IDs is best-effort (and duplicate broker IDs are an administrative mistake), so
this downside seems acceptable.",Resolved,Done,Bug,Minor,Colin McCabe,Colin McCabe,KAFKA,Kafka,,,,,4.1.0
KAFKA-19106,13614546,2025-04-08T10:10:05.000+0000,2025-07-15T03:51:58.000+0000,,Improve Connect SourceTask commit(),,Open,To Do,Improvement,Minor,Sudesh Wasnik,Sudesh Wasnik,KAFKA,Kafka,,,connect,,
KAFKA-19070,13613834,2025-04-01T14:41:38.000+0000,2025-07-15T03:36:10.000+0000,,Adding task number to the user provided client id (via consumer.override.client.id) to ensure each consumer has a unique client ID to avoid metric registration conflicts.,"If user provides client id via ""{*}consumer.override.client.id{*}"", we simply take this value and override the default client id (which is :: {*}""connector-consumer-"" + taskId{*}). This create all the consumers with the same client id which does not lead to failures but cause issues in metric registration and ultimately emitting wrong metrics values.",Open,To Do,Improvement,Minor,,Pritam Kumar,KAFKA,Kafka,,,connect,improvement,4.2.0
KAFKA-19047,13613231,2025-03-27T02:33:36.000+0000,2025-07-02T20:40:53.000+0000,2025-06-30T16:29:11.000+0000,Broker registrations are slow if previously fenced or shutdown,"BrokerLifecycleManager prevents registration of a broker w/ an id it has seen before with a different incarnation id if the broker session expires. On clean shutdown and restart of a broker this can cause an unnecessary delay in re-registration while the quorum controller waits for the session to expire.

```
[BrokerLifecycleManager id=1] Unable to register broker 1 because the controller returned error DUPLICATE_BROKER_REGISTRATION
```",Resolved,Done,Bug,Major,Alyssa Huang,Alyssa Huang,KAFKA,Kafka,,,controller,,4.1.0
KAFKA-19039,13613040,2025-03-25T10:02:00.000+0000,2025-07-15T03:51:59.000+0000,,The refresh_collaborators.py script messes .asf.yaml,"When running refresh_collaborators.py it's messing the ""protected_branches"" field in .asf.yaml:


{code:java}
@@ -48,4 +47,4 @@ github:
   # Disable legacy branch protections. We have manual rulesets which protect trunk
   # and our release branches. See INFRA-26603
-  protected_branches: ~
+  protected_branches:
{code}",Patch Available,In Progress,Bug,Major,Siyang He,Mickael Maison,KAFKA,Kafka,,,,,
KAFKA-19014,13612455,2025-03-19T23:24:00.000+0000,2025-07-09T12:46:23.000+0000,2025-04-21T17:28:18.000+0000,Potential race condition in remote-log-reader and remote-log-index-cleaner thread,"A race condition between threads below results in MappedByteBuffer to reference to a deleted file and attempts to read the file are potentially resulting in JVM to crash.

 

Chain of events:

*Thread - 1 remote-log-reader*

1/ Fetches the offsetIndex from the indexCache which internally maps the physical offset index file as MappedByteBuffer.

OffsetIndex offsetIndex = indexCache.getIndexEntry(segmentMetadata).offsetIndex(); ([here|https://github.com/apache/kafka/blob/cf7029c0264fd7f7b15c2e98acc874ec8c3403f2/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L1772])

*Thread - 2 index cache thread*

Entry is marked for cleanup i.e physical offset index file is renamed.

*Thread - 3 remote-log-index-cleaner*

Physical offset index file is deleted.

*Thread - 1 remote-log-reader*

Attempts run binary search on the MappedByteBuffer that is mapped to a non-existent file.

long upperBoundOffset = offsetIndex.fetchUpperBoundOffset(startOffsetPosition, fetchSize).map(position -> position.offset).orElse(segmentMetadata.endOffset() + 1); ([here|https://github.com/apache/kafka/blob/3.8/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L1619])

 

Results in JVM fatal error (SIGSEV) with stack trace:

 
{code:java}
Stack: [0x000072ee9112d000,0x000072ee9122d000],  sp=0x000072ee9122b360,  free space=1016k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 6483 c2 java.nio.DirectByteBuffer.getInt(I)I java.base@17.0.14 (28 bytes) @ 0x000072f23d2f12f1 [0x000072f23d2f12a0+0x0000000000000051]
j  org.apache.kafka.storage.internals.log.OffsetIndex.relativeOffset(Ljava/nio/ByteBuffer;I)I+5
j  org.apache.kafka.storage.internals.log.OffsetIndex.parseEntry(Ljava/nio/ByteBuffer;I)Lorg/apache/kafka/storage/internals/log/OffsetPosition;+11
j  org.apache.kafka.storage.internals.log.OffsetIndex.parseEntry(Ljava/nio/ByteBuffer;I)Lorg/apache/kafka/storage/internals/log/IndexEntry;+3
j  org.apache.kafka.storage.internals.log.AbstractIndex.binarySearch(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;Lorg/apache/kafka/storage/internals/log/AbstractIndex$SearchResultType;II)I+30
j  org.apache.kafka.storage.internals.log.AbstractIndex.indexSlotRangeFor(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;Lorg/apache/kafka/storage/internals/log/AbstractIndex$SearchResultType;)I+126
j  org.apache.kafka.storage.internals.log.AbstractIndex.smallestUpperBoundSlotFor(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;)I+8
 {code}
 

 

As per MappedByteBuffer documentation ([here|https://devdocs.io/openjdk~17/java.base/java/nio/mappedbytebuffer]):

All or part of a mapped byte buffer may become inaccessible at any time, for example if the mapped file is truncated. An attempt to access an inaccessible region of a mapped byte buffer will not change the buffer's content and will cause an unspecified exception to be thrown either at the time of the access or at some later time. It is therefore strongly recommended that appropriate precautions be taken to avoid the manipulation of a mapped file by this program, or by a concurrently running program, except to read or write the file's content.",Resolved,Done,Bug,Major,Kamal Chandraprakash,Hasil Sharma,KAFKA,Kafka,,,,tiered-storage,
KAFKA-19012,13612360,2025-03-19T12:53:13.000+0000,2025-07-10T19:47:01.000+0000,,Messages ending up on the wrong topic,"We're experiencing messages very occasionally ending up on a different topic than what they were published to. That is, we publish a message to topicA and consumers of topicB see it and fail to parse it because the message contents are meant for topicA. This has happened for various topics. 

We've begun adding a header with the intended topic (which we get just by reading the topic from the record that we're about to pass to the OSS client) right before we call producer.send, this header shows the correct topic (which also matches up with the message contents itself). Similarly we're able to use this header and compare it to the actual topic to prevent consuming these misrouted messages, but this is still concerning.

Some details:
 - This happens rarely: it happened approximately once per 10 trillion messages for a few months, though there was a period of a week or so where it happened more frequently (once per 1 trillion messages or so)
 - It often happens in a small burst, eg 2 or 3 messages very close in time (but from different hosts) will be misrouted
 - It often but not always coincides with some sort of event in the cluster (a broker restarting or being replaced, network issues causing errors, etc). Also these cluster events happen quite often with no misrouted messages
 - We run many clusters, it has happened for several of them
 - There is no pattern between intended and actual topic, other than the intended topic tends to be higher volume ones (but I'd attribute that to there being more messages published -> more occurrences affecting it rather than it being more likely per-message)
 - It only occurs with clients that are using a non-zero linger
 - Once it happened with two sequential messages, both were intended for topicA but both ended up on topicB, published by the same host (presumably within the same linger batch)
 - Most of our clients are 3.2.3 and it has only affected those, most of our brokers are 3.2.3 but it has also happened with a cluster that's running 3.8.1 (but I suspect a client rather than broker problem because of it never happening with clients that use 0 linger)",In Progress,In Progress,Bug,Blocker,Kirk True,Donny Nadolny,KAFKA,Kafka,,,"clients,producer ",,
KAFKA-18982,13611758,2025-03-13T18:23:18.000+0000,2025-07-01T03:52:03.000+0000,,Allow ClusterTests to ignore specific thread leaks,"When testing plug-in components to Kafka, such as a custom metric reporter, 
it is very useful to reuse the ClusterTest extension to write integration tests that spin Kafka instances under junit.

However the plugins may create additional Threads that should be tolerated rather than being shown as a failure due to a Thread leak . 

For example, testing the Prometheus Java libraries and hooking the registry to report also JVM metrics, I encountered a thread being created due to a GC event.

{color:#1d1c1d}java.lang.Thread.<init>(Thread.java:715) at java.lang.Thread.<init>(Thread.java:500) at io.prometheus.metrics.core.util.Scheduler$DaemonThreadFactory.newThread(Scheduler.java:19) at
... 
io.prometheus.metrics.instrumentation.jvm.JvmMemoryPoolAllocationMetrics$AllocationCountingNotificationListener.handleNotification(JvmMemoryPoolAllocationMetrics.java:97) at sun.management.NotificationEmitterSupport.sendNotification(NotificationEmitterSupport.java:155) at com.sun.management.internal.GarbageCollectorExtImpl.createGCNotification(GarbageCollectorExtImpl.java:115){color}

 

I would like to annotate my custom ClusterTest as not to fail because of a thread leak in such cases.

 ",Patch Available,In Progress,Improvement,Minor,Edoardo Comar,Edoardo Comar,KAFKA,Kafka,,,unit tests,,
KAFKA-18977,13611666,2025-03-13T06:12:34.000+0000,2025-07-07T17:55:47.000+0000,2025-07-07T17:55:47.000+0000,Fix flaky streams_smoke_test,"
{code:java}
RemoteCommandError({'ssh_config': {'host': 'worker21', 'hostname': '10.140.61.216', 'user': 'ubuntu', 'port': 22, 'password': None, 'identityfile': '/home/semaphore/kafka-overlay/semaphore-muckrake.pem', 'connecttimeout': None}, 'hostname': 'worker21', 'ssh_hostname': '10.140.61.216', 'user': 'ubuntu', 'externally_routable_ip': '10.140.61.216', '_logger': <Logger kafkatest.tests.streams.streams_smoke_test.StreamsSmokeTest.test_streams.processing_guarantee=at_least_once.crash=True.metadata_quorum=COMBINED_KRAFT-905 (DEBUG)>, 'os': 'linux', '_ssh_client': <paramiko.client.SSHClient object at 0x7f92b7365040>, '_sftp_client': <paramiko.sftp_client.SFTPClient object at 0x7f92b758aa60>, '_custom_ssh_exception_checks': None}, 'grep SUCCESS /mnt/streams/streams.stdout', 1, b'')
Traceback (most recent call last):
  File ""/home/semaphore/kafka-overlay/kafka/venv/lib/python3.8/site-packages/ducktape/tests/runner_client.py"", line 351, in _do_run
    data = self.run_test()
  File ""/home/semaphore/kafka-overlay/kafka/venv/lib/python3.8/site-packages/ducktape/tests/runner_client.py"", line 411, in run_test
    return self.test_context.function(self.test)
  File ""/home/semaphore/kafka-overlay/kafka/venv/lib/python3.8/site-packages/ducktape/mark/_mark.py"", line 438, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/home/semaphore/kafka-overlay/kafka/tests/kafkatest/tests/streams/streams_smoke_test.py"", line 110, in test_streams
    self.driver.node.account.ssh(""grep SUCCESS %s"" % self.driver.STDOUT_FILE, allow_fail=False)
  File ""/home/semaphore/kafka-overlay/kafka/venv/lib/python3.8/site-packages/ducktape/cluster/remoteaccount.py"", line 35, in wrapper
    return method(self, *args, **kwargs)
  File ""/home/semaphore/kafka-overlay/kafka/venv/lib/python3.8/site-packages/ducktape/cluster/remoteaccount.py"", line 310, in ssh
    raise RemoteCommandError(self, cmd, exit_status, stderr.read())
ducktape.cluster.remoteaccount.RemoteCommandError: ubuntu@worker21: Command 'grep SUCCESS /mnt/streams/streams.stdout' returned non-zero exit status 1.

{code}
",Resolved,Done,Improvement,Major,Chia-Ping Tsai,Chia-Ping Tsai,KAFKA,Kafka,,,"streams,system tests",,
KAFKA-18943,13611104,2025-03-07T23:27:45.000+0000,2025-07-03T04:01:20.000+0000,2025-03-13T23:58:53.000+0000,Kafka Streams incorrectly commits TX during task revokation,"We found a very rare edge case in Kafka Streams commit logic, that may lead to duplicates for EOSv2 under certain circumstances.

Background:

When tasks are revoked cleanly, we need to commit the progress the revoked tasks made before we can release them. For EOSv2, if we commit, a thread always needs to commit all tasks it owns. Thus, during revocation, if a single revoked tasks did make progress, we need to include all non-revoked task in the commit.

To avoid unnecessary commits (which are expensive), we have an optimization in place, that is supposed to skip the commit, if no revoked task did make progress (ie, for revoked tasks w/o progress, we can release them safely w/o the need to commit, and if all tasks can be revoked w/o a commit, we don't commit at all).

This optimization has a bug, and may commit an open TX, even if no revoked task did make progress, and because the commit is done accidentally, we do not add offsets to the TX. Thus, we may commit result data of non-revoked tasks, w/o including the non-revoked tasks advanced offsets.

If this happens, and an fatal error happens right afterwards (ie before a consecutive commit for the same non-revoked tasks was done, which would mask the error by commit the correct offsets with the consecutive TX), we would seek to an incorrect (too old) offset after recovery, and re-process the same input data a second time, producing duplicate results.

These issue results in duplicates if the following conditions are all met at the same time:
 * a thread has more than one task assigned
 * when tasks are revoked, at least one task is not revoked
 * all revoked tasks did not make any progress (or to rephrase: no revoked task did make progress)
 * at least one non-revoked task did make progress
 * after the incorrect commit, and before the next successful commit, a fatal error happen, triggering a rebalance, task re-assignment and a ""fetch offsets"" happen after the rebalance

The bug was introduced via https://issues.apache.org/jira/browse/KAFKA-14294 in 3.4.0 release.

Looking into the details, it actually seems that there is some other issue, that KAFKA-14294 did not address: KAFKA-14294 attempts to commit tasks after output was produced by a punctuation, even if offsets did not advance. However, it does apply this logic only for regular commits, but for the task-revoked-case, we don't commit. Thus, if a revoked task did not advance offsets, but did produce output data, no commit happens (while it seems we should commit the open TX for this case, too; this might even apply to the ALOS case, too).

Thus, there is two possible fixes.
 # Try to identify all corner case and patch it up
 # Drop the optimization and just commit all tasks on `TaskManager#handleRevocation()`",Resolved,Done,Bug,Blocker,Matthias J. Sax,Matthias J. Sax,KAFKA,Kafka,,,streams,,"3.9.1,4.0.0"
KAFKA-18874,13609910,2025-02-26T16:29:11.000+0000,2025-07-15T03:51:55.000+0000,,KRaft controller does not retry registration if the first attempt times out,"There is a [retry mechanism|https://github.com/apache/kafka/blob/3.9.0/core/src/main/scala/kafka/server/ControllerRegistrationManager.scala#L274] with exponential backoff built-in in KRaft controller registration. The timeout of the first attempt is 5 s for KRaft controllers ([code|https://github.com/apache/kafka/blob/3.9.0/core/src/main/scala/kafka/server/ControllerServer.scala#L448]) which is not configurable.

If for some reason the controller's first registration request times out, the attempt should be retried but in practice this does not happen and the controller is not able to join the quorum. We see the following in the faulty controller's log:
{noformat}
2025-02-21 13:31:46,606 INFO [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=3, incarnationId=mEzjHheAQ_eDWejAFquGiw, zkMigrationReady=true, listeners=[Listener(name='CONTROLPLANE-9090', host='kraft-rollback-kafka-controller-pool-3.kraft-rollback-kafka-kafka-brokers.csm-op-test-kraft-rollback-631e64ac.svc', port=9090, securityProtocol=1)], features=[Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=21)]) (kafka.server.ControllerRegistrationManager) [controller-3-registration-manager-event-handler]
...
2025-02-21 13:31:51,627 ERROR [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] RegistrationResponseHandler: channel manager timed out before sending the request. (kafka.server.ControllerRegistrationManager) [controller-3-to-controller-registration-channel-manager]
2025-02-21 13:31:51,726 INFO [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] maybeSendControllerRegistration: waiting for the previous RPC to complete. (kafka.server.ControllerRegistrationManager) [controller-3-registration-manager-event-handler]
{noformat}
After this we can not see any controller retry in the log.",Open,To Do,Bug,Minor,,Daniel Fonai,KAFKA,Kafka,,,controller,,
KAFKA-18524,13604923,2025-01-14T20:51:20.000+0000,2025-07-09T12:46:58.000+0000,2025-02-18T16:29:21.000+0000,Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache,org.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511),Resolved,Done,Bug,Major,Chia-Ping Tsai,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-18379,13603450,2024-12-30T19:20:53.000+0000,2025-07-10T03:33:02.000+0000,,Enforce resigned cannot transition to any other state in same epoch,,Open,To Do,Improvement,Major,TengYao Chi,Alyssa Huang,KAFKA,Kafka,,,,,
KAFKA-18297,13602519,2024-12-18T06:46:45.000+0000,2025-07-09T12:47:09.000+0000,2025-02-13T23:34:10.000+0000,Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups,,Resolved,Done,Bug,Major,Yu-Lin Chen,Chia-Ping Tsai,KAFKA,Kafka,,,"clients,consumer","flaky-test,integration-test,kip-848-client-support",
KAFKA-18201,13601607,2024-12-10T20:40:14.000+0000,2025-07-14T03:54:13.000+0000,,testGroupMetadataMessageFormatter fails for new consumer protocol,"This fails when the new protocol/consumer are used, with java.lang.NullPointerException: Cannot invoke ""com.fasterxml.jackson.databind.JsonNode.get(String)"" because ""keyNode"" is null

Not sure if it's a bug that needs fixing or if the test needs to be updated to successfully run with group.protocol=CONSUMER",In Progress,In Progress,Bug,Major,PoAn Yang,Lianet Magrans,KAFKA,Kafka,,,,,
KAFKA-18157,13600890,2024-12-04T14:50:27.000+0000,2025-07-08T07:13:27.000+0000,,Consider UnsupportedVersionException child class to represent the case of unsupported fields,"[https://github.com/apache/kafka/pull/17989#discussion_r1869333289] 

 

Adding more pointers, I expect that when the Unsupported exceptions is due to fields validation it comes from building the request on [https://github.com/apache/kafka/blob/f60382bf21601b1c6708d094fadb13de59a77278/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java#L582] , vs when it happens because the API is not supported, it should be detected when checking the usable version on [https://github.com/apache/kafka/blob/81447c7c9543d56e2fce76b861d3918d4cac6a2a/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java#L155] ",Open,To Do,Improvement,Minor,TengYao Chi,Lianet Magrans,KAFKA,Kafka,,,clients,,
KAFKA-18149,13600765,2024-12-03T18:16:11.000+0000,2025-07-02T20:54:46.000+0000,,Docs improvement for 3.9,"Docs should
 * point to correct location for sample controller configs (e.g. config/kraft/controller.properties)
 * be specific that --command-config for add-controller needs to point to the to-be-added controller's config
 * be specific that `controller.quorum.bootstrap.servers` in the to-be-added controllers config needs to contain a controller in the quorum
 * alternatively, release should just contain a second copy of controller configs that will work for the reconfig example",In Progress,In Progress,Improvement,Minor,PoAn Yang,Alyssa Huang,KAFKA,Kafka,,,,,
KAFKA-18120,13600375,2024-11-29T09:20:28.000+0000,2025-07-09T12:49:39.000+0000,2025-07-09T12:49:27.000+0000,KIP-891: Support for multiple versions of connect plugins.,"Jira to track implementation of KIP-891 [KIP-891: Running multiple versions of Connector plugins - Apache Kafka - Apache Software Foundation|https://cwiki.apache.org/confluence/display/KAFKA/KIP-891%3A+Running+multiple+versions+of+Connector+plugins]

 ",Resolved,Done,New Feature,Major,Snehashis Pal,Snehashis Pal,KAFKA,Kafka,,,connect,,4.1.0
KAFKA-18105,13600222,2024-11-27T19:02:24.000+0000,2025-07-13T09:34:01.000+0000,,Fix flaky PlaintextAdminIntegrationTest#testElectPreferredLeaders,org.opentest4j.AssertionFailedError: Timed out waiting for leader to become Some(0). Last metadata lookup returned leader = Some(1) ==> expected: <true> but was: <false>,Open,To Do,Bug,Major,Chang Chi Hsu,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-17715,13594561,2024-10-07T23:13:14.000+0000,2025-07-09T09:08:01.000+0000,,Remove `force_use_zk_connection` from e2e,"Since no E2E tests are using the argument, we should remove it.",Open,To Do,Bug,Major,Ming-Yen Chung,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-17601,13593099,2024-09-24T14:05:30.000+0000,2025-07-10T00:34:32.000+0000,,Inter-broker connections do not expose their clientSoftwareName and clientSoftwareVersion tags,"[KIP-511|[https://cwiki.apache.org/confluence/display/KAFKA/KIP-511%3A+Collect+and+Expose+Client%27s+Name+and+Version+in+the+Brokers]] made it possible to see what library versions are used by the Kafka clients.

When Kafka brokers are connecting to other brokers this information is not properly populated, we see the ""unknown"" value instead for both `ClientSoftwareName` and `ClientSoftwareVersion`.",In Progress,In Progress,Bug,Minor,PoAn Yang,Tamas Kornai,KAFKA,Kafka,,,,,
KAFKA-17444,13590512,2024-08-30T02:33:44.000+0000,2025-07-13T03:52:12.000+0000,,High memory allocation rate for FetchSession.update ,"!image-2024-08-30-10-34-33-011.png|width=757,height=241!

when single node 7w fetch qps, 1000+ consumer fetch connection the touch compare logic will consume a lot memory allocate 8%.",Open,To Do,Improvement,Major,,jinlong wang,KAFKA,Kafka,,,core,,
KAFKA-17019,13583476,2024-06-21T18:54:17.000+0000,2025-07-15T13:14:29.000+0000,,Producer TimeoutException should include root cause,"With KAFKA-16965 we added a ""root cause"" to some `TimeoutException` thrown by the producer. However, it's only a partial solution to address a specific issue.

We should consider to add the ""root cause"" for _all_ `TimeoutException` cases and unify/cleanup the code to get an holistic solution to the problem.",Open,To Do,Improvement,Major,sanghyeok An,Matthias J. Sax,KAFKA,Kafka,,,"clients,producer ",,
KAFKA-17014,13583405,2024-06-21T05:52:52.000+0000,2025-07-13T03:52:19.000+0000,,ScramFormatter should not use String for password.,"Since String is immutable, there are no easy ways to erase a String password after use.  It is a security concern so we should not use String for passwords.  See also  https://stackoverflow.com/questions/8881291/why-is-char-preferred-over-string-for-passwords",Open,To Do,Improvement,Major,Mingdao Yang,Tsz-wo Sze,KAFKA,Kafka,,,security,,
KAFKA-16947,13582462,2024-06-13T00:15:17.000+0000,2025-07-07T15:57:23.000+0000,,Kafka Tiered Storage V2,,Open,To Do,Improvement,Major,Satish Duggana,Satish Duggana,KAFKA,Kafka,,,,KIP-405,
KAFKA-16913,13581904,2024-06-07T08:15:03.000+0000,2025-07-03T04:53:12.000+0000,,Support external schemas in JSONConverter, KIP-1054: Support external schemas in JSONConverter : https://cwiki.apache.org/confluence/display/KAFKA/KIP-1054%3A+Support+external+schemas+in+JSONConverter,Patch Available,In Progress,Improvement,Minor,Priyanka K U,Priyanka K U,KAFKA,Kafka,,,connect,,
KAFKA-16799,13579885,2024-05-20T23:20:26.000+0000,2025-07-01T00:27:43.000+0000,,NetworkClientDelegate is not backing off if the node is not found,"When performing stress testing, I found that AsycnKafkaConsumer's network client delegate isn't backing off if the node is not ready, causing a large number of: 
{code:java}
 358 [2024-05-20 22:59:02,591] DEBUG [Consumer clientId=consumer.7136899e-0c20-4ccb-8ba3-497e9e683594-0, groupId=consumer-groups-test-5] Node is not ready, handle the request in the next event loop: node=b4-pkc-devcmkz697.us-west-2.aws.devel.cpd     ev.cloud:9092 (id: 2147483643 rack: null), request=UnsentRequest{requestBuilder=ConsumerGroupHeartbeatRequestData(groupId='consumer-groups-test-5', memberId='', memberEpoch=0, instanceId=null, rackId=null, rebalanceTimeoutMs=100000, subscri     bedTopicNames=[_kengine-565-test-topic8081], serverAssignor=null, topicPartitions=[]), handler=org.apache.kafka.clients.consumer.internals.NetworkClientDelegate$FutureCompletionHandler@139a8761, node=Optional[b4-pkc-devcmkz697.us-west-2.aws     .devel.cpdev.cloud:9092 (id: 2147483643 rack: null)], timer=org.apache.kafka.common.utils.Timer@649fffad} (org.apache.kafka.clients.consumer.internals.NetworkClientDelegate:169) {code}
show up in the log.

What should have happened is: 1. node is not ready 2. exponential back off 3. retry",Open,To Do,Bug,Major,Kirk True,Philip Nee,KAFKA,Kafka,,,"clients,consumer",consumer-threading-refactor,4.2.0
KAFKA-16768,13579313,2024-05-15T01:50:53.000+0000,2025-07-15T14:06:09.000+0000,,SocketServer leaks accepted SocketChannel instances due to race condition,"The SocketServer has threads for Acceptors and Processors. These threads communicate via Processor#accept/Processor#configureNewConnections and the `newConnections` queue.

During shutdown, the Acceptor and Processors are each stopped by setting shouldRun to false, and then shutdown proceeds asynchronously in all instances together. This leads to a race condition where an Acceptor accepts a SocketChannel and queues it to a Processor, but that Processor instance has already started shutting down and has already drained the newConnections queue.

KAFKA-16765 is an analogous bug in NioEchoServer, which uses a completely different implementation but has the same flaw.

An example execution order that includes this leak:
1. Acceptor#accept() is called, and a new SocketChannel is accepted.
2. Acceptor#assignNewConnection() begins
3. Acceptor#close() is called, which sets shouldRun to false in the Acceptor and attached Processor instances
4. Processor#run() checks the shouldRun variable, and exits the loop
5. Processor#closeAll() executes, and drains the `newConnections` variable
6. Processor#run() returns and the Processor thread terminates
7. Acceptor#assignNewConnection() calls Processor#accept(), which adds the SocketChannel to `newConnections`
8. Acceptor#assignNewConnection() returns
9. Acceptor#run() checks the shouldRun variable and exits the loop, and the Acceptor thread terminates.
10. Acceptor#close() joins all of the terminated threads, and returns

At the end of this sequence, there are still open SocketChannel instances in newConnections, which are then considered leaked.",Open,To Do,Bug,Major,Chang-Yu Huang,Greg Harris,KAFKA,Kafka,,,core,newbie,
KAFKA-16143,13564776,2024-01-15T21:32:05.000+0000,2025-07-10T00:11:55.000+0000,2024-12-13T12:21:29.000+0000,New JMX metrics for AsyncKafkaConsumer,This task is to consider what _new_ metrics we need from the KIP-848 protocol that aren't already exposed by the current set of metrics. This will require a KIP to introduce the new metrics.,Resolved,Done,Improvement,Major,PoAn Yang,Kirk True,KAFKA,Kafka,,,"clients,consumer,metrics","consumer-threading-refactor,kip-848-client-support,metrics,needs-kip",4.0.0
KAFKA-16061,13563131,2023-12-29T01:57:33.000+0000,2025-07-08T18:00:45.000+0000,2025-07-08T17:59:01.000+0000,KRaft JBOD follow-ups and improvements,,Resolved,Done,Improvement,Major,Igor Soarez,Colin McCabe,KAFKA,Kafka,,,,,3.7.0
KAFKA-15630,13554591,2023-10-18T14:14:45.000+0000,2025-07-03T00:12:23.000+0000,,Improve documentation of offset.lag.max,"It would be good to expand on the role of this configuration on offset translation and mention that it can be set to a smaller value, or even 0, to help in scenarios when records may not flow constantly.

The documentation string is here: [https://github.com/apache/kafka/blob/06739d5aa026e7db62ff0bd7da57e079cca35f07/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConfig.java#L104] ",Open,To Do,Improvement,Major,Xuze Yang,Mickael Maison,KAFKA,Kafka,,,"docs,mirrormaker",newbie,
KAFKA-15615,13554316,2023-10-16T22:29:34.000+0000,2025-07-10T03:48:38.000+0000,,Improve handling of fetching during metadata updates,"[During a review of the new fetcher|https://github.com/apache/kafka/pull/14406#discussion_r1333393941], [~junrao] found what appears to be an opportunity for optimization.

When a fetch response receives an error about partition leadership, fencing, etc. a metadata refresh is triggered. However, it takes time for that refresh to occur, and in the interim, it appears that the consumer will blindly attempt to fetch data for the partition again, in kind of a ""definition of insanity"" type of way. Ideally, the consumer would have a way to temporarily ignore those partitions, in a way somewhat like the ""pausing"" approach so that they are skipped until the metadata refresh response is fully processed.

This affects both the existing KafkaConsumer and the new PrototypeAsyncConsumer.",Patch Available,In Progress,Improvement,Major,appchemist,Kirk True,KAFKA,Kafka,,,"clients,consumer","consumer-threading-refactor,fetcher",
KAFKA-15331,13546849,2023-08-10T15:47:41.000+0000,2025-07-14T16:04:05.000+0000,,Handle remote log enabled topic deletion when leader is not available,"When a topic gets deleted, then there can be a case where all the replicas can be out of ISR. This case is not handled, See: [https://github.com/apache/kafka/pull/13947#discussion_r1289331347] for more details.",In Progress,In Progress,Bug,Major,ally heev,Kamal Chandraprakash,KAFKA,Kafka,,,,,
KAFKA-15141,13542206,2023-07-01T20:03:48.000+0000,2025-07-04T12:06:16.000+0000,2023-09-20T07:47:36.000+0000,High CPU usage with log4j2,"Kafka brokers make use of the [Logging trait|https://github.com/apache/kafka/blob/1f4cbc5d53259031123b6e9e6bb9a5bbe1e084e8/core/src/main/scala/kafka/utils/Logging.scala#L41] which instantiates a Logger object for every instantiation of the class using the trait by default.

When using log4j2 as the logging implementation, the instantiation of a Logger object requires a stack traversal [[1]|https://github.com/apache/logging-log4j2/blob/2.x/log4j-api/src/main/java/org/apache/logging/log4j/spi/AbstractLoggerAdapter.java#L121] and [[2]|https://github.com/apache/logging-log4j2/blob/83bba1bc322e80e7e95edbebc2383f2724dbe0de/log4j-slf4j-impl/src/main/java/org/apache/logging/slf4j/Log4jLoggerFactory.java#L54].

While LOG4J2-2940 ensures stack is not traversed unless required, the default {{{}ContextSelector{}}}, [ClassLoaderContextSelector|https://logging.apache.org/log4j/2.x/log4j-core/apidocs/org/apache/logging/log4j/core/selector/ClassLoaderContextSelector.html] causes a stack traversal.

These stack traversals are frequent and quite CPU intensive and profiling suggests they consume ~5% CPU time (of the CPU stacks we have profiled on a sample of clusters). While log4j2 users can potentially avoid this by changing the context selector in their configuration, it is easy to overlook and the default configurations results in high CPU usage inadvertently.

An easy fix would be to instantiate the loggers statically for some commonly instantiated classes in Kafka which make use of the Logging trait.",Resolved,Done,Improvement,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,core,,"3.6.0,3.7.0"
KAFKA-14915,13532858,2023-04-17T10:05:19.000+0000,2025-07-15T06:07:21.000+0000,2025-07-14T14:14:38.000+0000,Option to consume multiple partitions that have their data in remote storage for the target offsets.,Context: https://github.com/apache/kafka/pull/13535#discussion_r1171250580,Resolved,Done,Improvement,Major,Luke Chen,Satish Duggana,KAFKA,Kafka,,,,tiered-storage,4.2.0
KAFKA-14830,13529501,2023-03-21T19:34:44.000+0000,2025-07-09T14:17:12.000+0000,,Illegal state error in transactional producer,"We have seen the following illegal state error in the producer:
{code:java}
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for topic-0:120027 ms has passed since batch creation
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for topic-1:120026 ms has passed since batch creation
[Producer clientId=client-id2, transactionalId=transactional-id] Aborting incomplete transaction
[Producer clientId=client-id2, transactionalId=transactional-id] Invoking InitProducerId with current producer ID and epoch ProducerIdAndEpoch(producerId=191799, epoch=0) in order to bump the epoch
[Producer clientId=client-id2, transactionalId=transactional-id] ProducerId set to 191799 with epoch 1
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.NetworkException: Disconnected from node 4
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: The request timed out.
[Producer clientId=client-id2, transactionalId=transactional-id] Uncaught error in request completion:
java.lang.IllegalStateException: TransactionalId transactional-id: Invalid transition attempted from state READY to state ABORTABLE_ERROR
        at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1089)
        at org.apache.kafka.clients.producer.internals.TransactionManager.transitionToAbortableError(TransactionManager.java:508)
        at org.apache.kafka.clients.producer.internals.TransactionManager.maybeTransitionToErrorState(TransactionManager.java:734)
        at org.apache.kafka.clients.producer.internals.TransactionManager.handleFailedBatch(TransactionManager.java:739)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:753)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:743)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:695)
        at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:634)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$null$1(Sender.java:575)
        at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$handleProduceResponse$2(Sender.java:562)
        at java.base/java.lang.Iterable.forEach(Iterable.java:75)
        at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:562)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$sendProduceRequest$5(Sender.java:836)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:583)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:575)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
        at java.base/java.lang.Thread.run(Thread.java:829)
 {code}
The producer hits timeouts which cause it to abort an active transaction. After aborting, the producer bumps its epoch, which transitions it back to the `READY` state. Following this, there are two errors for inflight requests, which cause an illegal state transition to `ABORTABLE_ERROR`. But how could the transaction ABORT complete if there were still inflight requests? ",In Progress,In Progress,Bug,Critical,Kirk True,Jason Gustafson,KAFKA,Kafka,,,"clients,producer ",transactions,4.2.0
KAFKA-14560,13516342,2023-01-03T15:32:49.000+0000,2025-07-10T13:49:22.000+0000,2025-02-24T04:29:59.000+0000,Remove old client protocol API versions in Kafka 4.0 (KIP-896),"Please see KIP for details:

https://cwiki.apache.org/confluence/display/KAFKA/KIP-896%3A+Remove+old+client+protocol+API+versions+in+Kafka+4.0",Resolved,Done,Improvement,Blocker,Ismael Juma,Ismael Juma,KAFKA,Kafka,,,,,4.0.0
KAFKA-14410,13504776,2022-11-21T08:45:02.000+0000,2025-07-09T12:47:20.000+0000,2024-12-11T10:01:19.000+0000,Allow connect runtime to run multiple versions of a connector. ,Connect Runtime should support running multiple versions of the same connector. Please refer to [KIP-891|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235834793] for more information on the problem and the proposed solution. ,Resolved,Done,Improvement,Major,Snehashis Pal,Snehashis Pal,KAFKA,Kafka,,,connect,,
KAFKA-14405,13503713,2022-11-19T01:24:27.000+0000,2025-07-14T01:27:44.000+0000,,Log a warning when users attempt to set a config controlled by Streams,"Related to https://issues.apache.org/jira/browse/KAFKA-14404

It's too easy for users to try overriding one of the client configs that Streams hardcodes, and since we just silently ignore it there's no good way for them to tell their config is not being used. Sometimes this may be harmless but in cases like the Producer's partitioner, there could be important application logic that's never being invoked.

When processing user configs in StreamsConfig, we should check for all these configs and log a warning when any of them have been set",Open,To Do,Bug,Major,,A. Sophie Blee-Goldman,KAFKA,Kafka,,,streams,newbie,
KAFKA-13965,13448889,2022-06-08T01:17:21.000+0000,2025-07-10T03:32:42.000+0000,,Document broker-side socket-server-metrics,"There are a bunch of broker JMX metrics in the ""socket-server-metrics"" space that are not documented on kafka.apache.org/documentation

 
 * {_}MBean{_}: kafka.server:{{{}type=socket-server-metrics,listener=<listenerName>,networkProcessor=<processorIndex>{}}}
 ** From KIP-188: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-188+-+Add+new+metrics+to+support+health+checks]
 *  kafka.server:type=socket-server-metrics,name=connection-accept-rate,listener=\{listenerName}
 ** From KIP-612: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-612%3A+Ability+to+Limit+Connection+Creation+Rate+on+Brokers]

It would be helpful to get all the socket-server-metrics documented

 ",Open,To Do,Improvement,Major,Ksolves India Limited,James Cheng,KAFKA,Kafka,,,documentation,"newbie,newbie++",
KAFKA-13555,13418323,2021-12-18T00:33:45.000+0000,2025-07-13T14:44:45.000+0000,,Consider number if input topic partitions for task assignment,"StreamsAssignor tries to distribute tasks evenly across all instances/threads of a Kafka Streams application. It knows about instances/thread (to give more capacity to instances with more thread), and it distinguishes between stateless and stateful tasks. We also try to not move state around but to use a sticky assignment if possible. However, the assignment does not take the number of input topic partitions into account.

For example, an upstream tasks could compute two joins, and thus has 3 input partitions, while a downstream task compute a follow up aggregation with a single input partitions (from the repartition topic). It could happen that one thread gets the 3 input partition tasks assigned, while the other thread get the single input partition tasks assigned resulting to an uneven partition assignment across both threads.",Open,To Do,Improvement,Major,,Matthias J. Sax,KAFKA,Kafka,,,streams,,
KAFKA-13251,13398176,2021-08-30T09:31:42.000+0000,2025-07-07T00:54:22.000+0000,,ISR shrink to an error broker,"Disk error occurred in broker(=42),and then Shrinking ISR to itself.
 so why Shrinking ISR to an error broker?

i.e.  why not ""Shrinking ISR from 55,42 to 55"" but ""Shrinking ISR from 55,42 to 42"".

note:

other partition(110) shrink correctly.

 

kafka logs:
 broker42:

[2021-08-26 20:20:55,640] ERROR [ReplicaManager broker=42] Error processing fetch with max size 1048576 from consumer on partition topic_xx-123: (fetchOffset=11061228956, logStartOffset=-1, maxBytes=1048576, currentLeaderEpoch=Optional.empty) (kafka.server.ReplicaManager)
 org.apache.kafka.common.errors.CorruptRecordException: Found record size 0 smaller than minimum record overhead (14) in file /data4/kafka-logs/topic_xx-123/00000000011060934646.log.
 [2021-08-26 20:20:55,640] ERROR Error while appending records to topic_xx-123 in dir /data4/kafka-logs (kafka.server.LogDirFailureChannel)
 [2021-08-26 20:20:55,645] ERROR Error while deleting segments for topic_xx-123 in dir /data4/kafka-logs (kafka.server.LogDirFailureChannel)
 java.nio.file.FileSystemException: /data4/kafka-logs/topic_xx-123/00000000011040402299.log -> /data4/kafka-logs/topic_xx-123/00000000011040402299.log.deleted: Read-only file system
 Suppressed: java.nio.file.FileSystemException: /data4/kafka-logs/topic_xx-123/00000000011040402299.log -> /data4/kafka-logs/topic_xx-123/00000000011040402299.log.deleted: Read-only file system
 [2021-08-26 20:20:55,644] ERROR Error while appending records to topic_xx-123 in dir /data4/kafka-logs (kafka.server.LogDirFailureChannel)
 [2021-08-26 20:20:55,652] INFO [Partition topic_xx-123 broker=42] Shrinking ISR from 55,42 to 42. Leader: (highWatermark: 11061228956, endOffset: 11061228965). Out of sync replicas: (brokerId: 55, endOffset: 11061228956). (kafka.cluster.Partition)
  

broker55:
 [2021-08-26 20:20:32,456] WARN [ReplicaFetcher replicaId=55, leaderId=42, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=55, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=830774713, epoch=1562806014), rackId=) (kafka.server.ReplicaFetcherThread)

[2021-08-26 20:20:43,503] INFO [Partition topic_xxx-110 broker=55] Shrinking ISR from 55,42 to 55. Leader: (highWatermark: 11061384367, endOffset: 11061388788). Out of sync replicas: (brokerId: 42, endOffset: 11061384367). (kafka.cluster.Partition)

 

disk error on broker42 is:
 Aug 26 20:20:55 kernel: sd 0:2:5:0: [sdf] tag#33 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK",Open,To Do,Bug,Major,,chaos,KAFKA,Kafka,,,core,,
KAFKA-12380,13361284,2021-02-26T22:41:58.000+0000,2025-07-15T05:43:32.000+0000,2022-04-29T05:37:28.000+0000,Executor in Connect's Worker is not shut down when the worker is,"The `Worker` class has an [`executor` field|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L100] that the public constructor initializes with a new cached thread pool ([https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127].]).

When the worker is stopped, it does not shutdown this executor. This is normally okay in the Connect runtime and MirrorMaker 2 runtimes, because the worker is stopped only when the JVM is stopped (via the shutdown hook in the herders).

However, we instantiate and stop the herder many times in our integration tests, and this means we're not necessarily shutting down the herder's executor. Normally this won't hurt, as long as all of the runnables that the executor threads run actually do terminate. But it's possible those threads *might* not terminate in all tests. TBH, I don't know that such cases actually exist.

 ",Resolved,Done,Bug,Minor,Rajani Karuturi,Randall Hauch,KAFKA,Kafka,,,connect,newbie,3.3.0
KAFKA-12281,13356487,2021-02-03T12:01:19.000+0000,2025-07-15T05:54:35.000+0000,,Deprecate org.apache.kafka.streams.errors.BrokerNotFoundException,"It's been 3 years since 234ec8a gets rid of usage of BrokerNotFoundException. Hence, it is time to deprecate BrokerNotFoundException.
",In Progress,In Progress,Improvement,Minor,Rajani Karuturi,Chia-Ping Tsai,KAFKA,Kafka,,,streams,"beginner,needs-kip,newbie",
KAFKA-10844,13345496,2020-12-11T11:20:21.000+0000,2025-07-09T15:36:12.000+0000,,groupBy without shuffling,"The idea is to give a way to keep the current partitioning while doing a groupBy.

Our use-case is the following:
We process device data (stream is partitioned by device-id), each device produces several metrics. We want to aggregate by metric, so currently we do a
{code:java}
 selectKey( ... => (device, metric)).groupByKey.windowedBy(...).aggregate(...)  {code}
This shuffles the data around, but it's not necessary, each (device, metric) group could stay in the original partition.

This is not only an optimization question. We are experiencing invalid aggregations when reprocessing history. In these reprocessing, we frequently see some tasks moving faster on some partitions. This causes problems with event-time: Lets' say data for device d1 is in partition p1 and stream-time t1, and device d2 / partition p2 / time t2.
Now, if I re-key by (device, metric), records from both devices could have the same hash-key and land in the same partition. And if t2 is far ahead of t1, then all time-windows for t1 get expired at once.

Maybe I miss some way of doing this with the existing API, please let me know. Currently, I manually repartition and specify a custom partitioner, but it's tedious.

If I were to rewrite the aggregations manually with Transformer API, I would use (device, key) for my state store key, without changing the record key.

 

_(poke_ [~vvcephei] _following our discussion on users ml)_

KIP-759: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-759%3A+Unneeded+repartition+canceling] ",Patch Available,In Progress,Improvement,Major,João Pedro Fonseca,Mathieu DESPRIEE,KAFKA,Kafka,,,streams,kip,
KAFKA-10428,13324348,2020-08-24T23:31:52.000+0000,2025-07-02T17:20:49.000+0000,,Mirror Maker connect applies base64 encoding to string headers,"MirrorSourceTask takes the header value as bytes from the ConsumerRecord, which does not have a header schema, and adds it to the SourceRecord headers using ""addBytes"". This uses Schema.BYTES as the schema for the header, and somehow, base64 encoding gets applied when the record gets committed.

This means that my original header value ""with_headers"" (created with a python producer, and stored as a 12 character byte array) becomes the string value ""d2l0aF9oZWFkZXJz"", a 16 character byte array, which is the base64 encoded version of the original. If I try to preempt this using ""d2l0aF9oZWFkZXJz"" to start with, and base64 encoding the headers everywhere, it just gets double encoded to ""ZDJsMGFGOW9aV0ZrWlhKeg=="" after passing through the MirrorSourceTask.

I think the base64 encoding may be coming from Values#append (https://github.com/apache/kafka/blob/trunk/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java#L674), but I'm not sure how. That is invoked by SimpleConnectorHeader#fromConnectHeader via Values#convertToString.

SimpleHeaderConverter#toConnectHeader produces the correct schema in this case, and solves the problem for me, but it seems to guess at the schema, so I'm not sure if it is the right solution. Since schemas seem to be required for SourceRecord headers, but not available from ConsumerRecord headers, I'm not sure what other option we have. I will open a PR with this solution",Open,To Do,Bug,Major,,Jennifer Thompson,KAFKA,Kafka,,,mirrormaker,,
KAFKA-10409,13323050,2020-08-17T12:42:20.000+0000,2025-07-11T16:18:24.000+0000,,Refactor Kafka Streams RocksDb iterators ,"From [https://github.com/apache/kafka/pull/9137#discussion_r470345513] :

[~ableegoldman] : 

> Kind of unrelated, but WDYT about renaming {{RocksDBDualCFIterator}} to {{RocksDBDualCFAllIterator}} or something on the side? I feel like these iterators could be cleaned up a bit in general to be more understandable – for example, it's weird that we do the {{iterator#seek}}-ing in the actual {{all()}} method but for range queries we do the seeking inside the iterator constructor.

and [https://github.com/apache/kafka/pull/9137#discussion_r470361726] :

> Personally I found the {{RocksDBDualCFIterator}} logic a bit difficult to follow even before the reverse iteration, so it would be nice to have some tests specifically covering reverse iterators over multi-column-family timestamped stores",Patch Available,In Progress,Improvement,Minor,João Pedro Fonseca,Jorge Esteban Quilcate Otoya,KAFKA,Kafka,,,streams,newbie,
KAFKA-9965,13303232,2020-05-06T23:44:49.000+0000,2025-07-15T13:47:04.000+0000,2024-12-27T14:28:12.000+0000,Uneven distribution with RoundRobinPartitioner in AK 2.4+,"{{RoundRobinPartitioner}} states that it will provide equal distribution of records across partitions. However with the enhancements made in KIP-480, it may not. In some cases, when a new batch is started, the partitioner may be called a second time for the same record:

[https://github.com/apache/kafka/blob/2.4/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L909]

[https://github.com/apache/kafka/blob/2.4/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L934]

Each time the partitioner is called, it increments a counter in {{RoundRobinPartitioner}}, so this can result in unequal distribution.

Easiest fix might be to decrement the counter in {{RoundRobinPartitioner#onNewBatch}}.

 ",Resolved,Done,Bug,Major,,Michael Bingham,KAFKA,Kafka,,,producer ,,
KAFKA-7442,13187518,2018-09-26T07:06:29.000+0000,2025-07-08T16:18:59.000+0000,2025-07-08T16:17:33.000+0000,forceUnmap mmap on linux when index resize,"when resize OffsetIndex or TimeIndex,We should force unmap mmap for linux platform. Rather than waiting mixedgc or  fullgc to unmap MappedByteBuffer objects


##before full gc
{code}
{""request"":{""mbean"":""java.nio:name=mapped,type=BufferPool"",""type"":""read""},""value"":{""TotalCapacity"":2434496968,""MemoryUsed"":2434496968,""Count"":5392,""Name"":""mapped"",""ObjectName"":{""objectName"":""java.nio:name=mapped,type=BufferPool""}},""timestamp"":1537945759,""status"":200}


S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
0.00 100.00  28.88   4.93  97.64  94.72     24    0.176     0    0.000    0.176
0.00 100.00  31.37   4.93  97.64  94.72     24    0.176     0    0.000    0.176
{code}

{code}
jmap -histo:live kafka_pid
{code}
 
###after full gc
{code}
S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
0.00   0.00  23.22   5.03  97.92  94.93     24    0.176     1    0.617    0.793
0.00   0.00  25.70   5.03  97.92  94.93     24    0.176     1    0.617    0.793
0.00   0.00  27.86   5.03  97.92  94.93     24    0.176     1    0.617    0.793

{""request"":{""mbean"":""java.nio:name=mapped,type=BufferPool"",""type"":""read""},""value"":{""TotalCapacity"":1868266036,""MemoryUsed"":1868266036,""Count"":5338,""Name"":""mapped"",""ObjectName"":{""objectName"":""java.nio:name=mapped,type=BufferPool""}},""timestamp"":1537945860,""status"":200}
{code}



{code}
def resize(newSize: Int) {
    inLock(lock) {
      val raf = new RandomAccessFile(_file, ""rw"")
      val roundedNewSize = roundDownToExactMultiple(newSize, entrySize)
      val position = mmap.position

      /* Windows won't let us modify the file length while the file is mmapped :-( */
      if(Os.isWindows)
        forceUnmap(mmap)
      try {
        raf.setLength(roundedNewSize)
        mmap = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, roundedNewSize)
        _maxEntries = mmap.limit / entrySize
        mmap.position(position)
      } finally {
        CoreUtils.swallow(raf.close())
      }
    }
  }
{code}



{code}
[2018-09-21 13:12:24,078] INFO Rolled new log segment for 'topic-265' in 2 ms. (kafka.log.Log)
[2018-09-21 13:13:16,436] FATAL [ReplicaFetcherThread-12-15], Disk error while replicating data for topic-264 (kafka.server.ReplicaFetcherThread)
kafka.common.KafkaStorageException: I/O exception in append to log 'topic-264'
        at kafka.log.Log.append(Log.scala:349)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:130)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:153)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:141)
        at scala.Option.foreach(Option.scala:257)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:141)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:138)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply$mcV$sp(AbstractFetcherThread.scala:138)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:138)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:138)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:234)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:136)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:103)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:63)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:940)
        at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:61)
        at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:52)
        at kafka.log.LogSegment.<init>(LogSegment.scala:67)
        at kafka.log.Log.roll(Log.scala:778)
        at kafka.log.Log.maybeRoll(Log.scala:744)
        at kafka.log.Log.append(Log.scala:405)
        ... 16 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:937)
        ... 22 more

{code}",Resolved,Done,Bug,Major,huxihx,scott.zhai,KAFKA,Kafka,,,log,,4.2.0

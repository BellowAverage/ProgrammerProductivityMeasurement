key,id,created,updated,resolution_date,summary,description,status,status_category,issue_type,priority,assignee,reporter,project_key,project_name,story_points,epic_link,components,labels,fix_versions
KAFKA-19512,13623502,2025-07-15T18:37:00.000+0000,2025-07-15T19:09:07.000+0000,,add missing MVs in MetadataVersionTest,"We need to add the 4.2 MVs in the following tests.
testFromVersionString()testShortVersion()
testVersion()",Patch Available,In Progress,Improvement,Minor,Dmitry Werner,Jun Rao,KAFKA,Kafka,,,,,
KAFKA-19511,13623487,2025-07-15T15:33:17.000+0000,2025-07-15T17:14:51.000+0000,,Fix flaky test HandlingSourceTopicDeletionIntegrationTest.shouldThrowErrorAfterSourceTopicDeleted,"The test org.apache.kafka.streams.integration.HandlingSourceTopicDeletionIntegrationTest.shouldThrowErrorAfterSourceTopicDeleted(boolean, TestInfo)[2] often fails.

Observed in:
 # [https://develocity.apache.org/scans/tests?search.names=Git%20branch&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.timeZoneId=America%2FLos_Angeles&search.values=trunk&tests.container=org.apache.kafka.streams.integration.HandlingSourceTopicDeletionIntegrationTest&tests.sortField=FLAKY&tests.test=shouldThrowErrorAfterSourceTopicDeleted(boolean%2C%20TestInfo)%5B2%5D]

 

This test has a high failure/flaky rate since July 9th.",Open,To Do,Improvement,Major,,Lucy Liu,KAFKA,Kafka,,,,,
KAFKA-19510,13623457,2025-07-15T11:50:27.000+0000,2025-07-15T12:57:40.000+0000,,Kafka Streams does not always release lock when adding or removing threads multiple times,"Kafka Streams seems to not always release the state directory lock for all tasks, when threads are added / removed. 

Effectively, the old thread (same name, different object) still seems to own the task.

To reproduce, checkout

[https://github.com/lucasbru/kafka/commit/2701dd39fe7df23a123939a1c9e140c849904b5f]

and run `AdjustStreamThreadCountTest > 
shouldAddAndRemoveThreadsMultipleTimes` ... multiple times.
 
The test was changed to use a stateful topology. 
 

I reproduced this on 3.9, 4.0, 4.1, trunk
 ",Open,To Do,Bug,Major,,Lucas Brutschy,KAFKA,Kafka,,,streams,,
KAFKA-19509,13623438,2025-07-15T08:43:06.000+0000,2025-07-15T09:15:41.000+0000,,Improve error message when release version is wrong,"When running kafka-storage.sh, if the release-version is wrongly set, we'll get the error messages including un-released metadata version:

 
{code:java}
> bin/kafka-storage.sh format --standalone -t kEzc4vk3TIKhCQKsh40klQ -c config/server.properties --release-version 4.0-IV4
Exception in thread ""main"" java.lang.IllegalArgumentException: Version 4.0-IV4 is not a valid version. The minimum version is 3.3-IV3 and the maximum version is 4.2-IV1
    at org.apache.kafka.server.common.MetadataVersion.lambda$fromVersionString$0(MetadataVersion.java:356)
    at java.base/java.util.Optional.orElseThrow(Optional.java:403)
    at org.apache.kafka.server.common.MetadataVersion.fromVersionString(MetadataVersion.java:354)
    at kafka.tools.StorageTool$.$anonfun$runFormatCommand$1(StorageTool.scala:133)
    at scala.Option.foreach(Option.scala:437)
    at kafka.tools.StorageTool$.runFormatCommand(StorageTool.scala:132)
    at kafka.tools.StorageTool$.execute(StorageTool.scala:86)
    at kafka.tools.StorageTool$.main(StorageTool.scala:46)
    at kafka.tools.StorageTool.main(StorageTool.scala)
 {code}

One idea is to improve this by relying on the internal config: `unstable.feature.versions.enable` to decide if we want to log the unstable feature versions. 

The other thought is we can mimic what Kafka-feature.sh did: 
{code:java}
 >  bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --release-version 4.0-IV4
[2025-07-15 16:31:53,280] WARN [AdminClient clientId=adminclient-1] 
Unknown metadata.version 4.1-IV4. Supported metadata.version are 3.3-IV3, 3.4-IV0, 3.5-IV0, 3.5-IV1, 3.5-IV2, 3.6-IV0, 3.6-IV1, 3.6-IV2, 3.7-IV0, 3.7-IV1, 3.7-IV2, 3.7-IV3, 3.7-IV4, 3.8-IV0, 3.9-IV0, 4.0-IV0, 4.0-IV1, 4.0-IV2, 4.0-IV3 {code}
It didn't output the supported versions to users.

 

 ",Open,To Do,Improvement,Major,,Luke Chen,KAFKA,Kafka,,,,,
KAFKA-19508,13623434,2025-07-15T07:58:00.000+0000,2025-07-15T15:20:51.000+0000,,Producer keeps reporting a NotLeaderOrFollowerException,"1.After the server restarts, the producer keeps reporting a NotLeaderOrFollowerException when producing messages.

!image-2025-07-15-15-55-01-596.png!

2.Check the zknode data; the leader is functioning normally.

!image-2025-07-15-15-59-02-855.png!
3.Strangely, at 04:34:01, the leader was 12, and at 04:34:16, the leader was 11, but the leaderEpoch did not change.

!image-2025-07-15-16-03-13-043.png!

!image-2025-07-15-16-01-59-877.png!
4.state-change logs

 ",Open,To Do,Bug,Blocker,,mooner,KAFKA,Kafka,,,"clients,core,producer ",,
KAFKA-19507,13623427,2025-07-15T07:30:09.000+0000,2025-07-15T07:58:11.000+0000,,Optimize Replica Assignment for Broker Load Balance in Uneven Rack Configurations,"h3. Issue Description

Kafka's current replica assignment strategy prioritizes _balancing replica counts across racks_ (availability zones in cloud environments) over _balancing replicas across individual brokers_. While this ensures rack diversity, it creates significant broker-level load imbalance when racks contain unequal numbers of brokers.
h3. Problem Illustration

Consider a 3-replica topic with 3 racks:
 * *Rack A*: Brokers 1, 4

 * *Rack B*: Brokers 2, 5

 * *Rack C*: Broker 3 (single broker)

Under the current strategy:
 * Brokers 1, 2, 4, 5 each receive 1/6 of all replicas

 * Broker 3 receives 1/3 of all replicas (twice the load of others)

This forces Broker 3 into a bottleneck (""bucket effect""), as it handles double the traffic and storage load.

 

To mitigate this, deployments today must maintain broker counts as _multiples of rack counts_ (e.g., 3, 6, 9 brokers for 3 racks). While this ensures balance, it:
 # *Restricts deployment flexibility*: Scaling clusters horizontally requires adding/removing nodes in rack-sized increments.

 # *Increases costs unnecessarily*: For example, a 4-broker cluster could suffice for a 3-rack setup, but users must deploy 6 brokers to maintain balance—increasing infrastructure costs by 50%.

h3. Proposed Solution

Modify the assignment strategy to:
 # *Prioritize broker-level balance* as the primary objective.

 # *Weight rack-level distribution* by broker count per rack (e.g., a rack with 2 brokers receives twice the replicas of a rack with 1 broker).

h4. Benefits
 * *Balanced load*: All brokers receive near-equal replicas regardless of rack imbalance.

 * *Deployment flexibility*: Clusters can scale to _any size_ as long as {{rack_count ≥ replica_factor}}.

 * *Cost efficiency*: Users deploy only necessary brokers.

h4. Example Scenario

_3 replicas, 4 racks with 5 brokers:_
 * *Rack A*: Brokers 1, 5 → Receives 2/5 of replicas (distributed evenly between Brokers 1 & 5)

 * *Racks B, C, D*: 1 broker each → Each receives 1/5 of replicas _Result_: Every broker handles exactly 1/5 of total replicas—eliminating bottlenecks.

h3. Request

We propose modifying the replica assignment algorithm to prioritize broker-level replica balance, while using rack-node-count-weighted distribution. This allows enterprises to deploy Kafka clusters with more flexible node counts, significantly improving cost efficiency while maintaining rack awareness.",Open,To Do,Improvement,Major,Jialun Peng,Jialun Peng,KAFKA,Kafka,,,,,
KAFKA-19506,13623422,2025-07-15T06:34:04.000+0000,2025-07-15T15:14:03.000+0000,,Implement dynamic compression type selection and fallback for client telemetry,"Kafka clients currently select the first compression type offered by the broker for telemetry push, often leading to errors if the required compression library (e.g., zstd, lz4, snappy) is not present in the client runtime. This causes telemetry push failures, while produce requests continue to work normally. Gzip is generally reliable as it is always present in the JDK, but other algorithms can trigger fatal exceptions, stopping further telemetry pushes.

This ticket proposes enhancing the Kafka client to dynamically select the best available compression type at runtime. If the initial compression attempt fails (e.g., due to a missing dependency), the client should automatically retry using the next broker-supported compression type. If none are available, it must gracefully fall back to sending uncompressed telemetry data, ensuring telemetry push always succeeds regardless of deployment environment or available libraries.",Open,To Do,Bug,Major,Kaushik Raina,Kaushik Raina,KAFKA,Kafka,,,,,4.2.0
KAFKA-19505,13623387,2025-07-14T18:57:18.000+0000,2025-07-14T22:37:20.000+0000,,ReplicaManagerTest#testMultipleRemoteFetchesInOneFetchRequest fails due to random topic-id in mocked log,"{{ReplicaManagerTest#testMultipleRemoteFetchesInOneFetchRequest}} fails in {{trunk}} at {{replicaManager.applyDelta(leaderDelta1, leaderMetadataImage1)}} as we expect a consistent topic-id but the mock setup in {{ReplicaManagerTest#setupMockLog}} results in a random topic-id being returned on the second call to {{log.topicId()}}",Patch Available,In Progress,Test,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,,,4.2.0
KAFKA-19504,13623375,2025-07-14T17:12:28.000+0000,2025-07-15T17:45:04.000+0000,2025-07-15T00:41:22.000+0000,AdminClient creates and adds second metrics reporter ,"The `AdminClient` adds a telemetry reporter to the metrics reporters list in the constructor.  The problem is that the reporter was already added in the `createInternal` method.  In the `createInternal` method call, the `clientTelemetryReporter` is added to a `List<MetricReporters>` which is passed to the `Metrics` object, will get closed when `Metrics.close()` is called.  But adding a reporter to the reporters list in the constructor is not used by the `Metrics` object and hence doesn't get closed, causing a memory leak.  Note, this problem only exists if telemetry metrics is enabled for the `AdminClient` and it is disabled by default.",Resolved,Done,Bug,Blocker,Bill Bejeck,Bill Bejeck,KAFKA,Kafka,,,clients,,"4.0.1,4.1.0,4.2.0"
KAFKA-19503,13623358,2025-07-14T15:06:09.000+0000,2025-07-14T15:15:48.000+0000,,Deprecate MX4j support,"Deprecate MX4j support.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-1193%3A+Deprecate+MX4j+support",Open,To Do,Task,Major,Federico Valeri,Federico Valeri,KAFKA,Kafka,,,core,kip-required,
KAFKA-19502,13623329,2025-07-14T12:06:29.000+0000,2025-07-14T12:08:13.000+0000,,"During write state call processing, a batch can be split into offsets which is not accounted for",,Open,To Do,Sub-task,Major,Apoorv Mittal,Abhinav Dixit,KAFKA,Kafka,,,,,
KAFKA-19501,13623318,2025-07-14T11:08:46.000+0000,2025-07-14T14:01:47.000+0000,,System tests should use 17-bullseye instead of 17-buster,"the version buster was removed from https://security.debian.org/debian-security, and hence the command ""apt update"" fails due to a 404 error (see below log)

{code:java}
 > [stage-1  2/59] RUN apt update && apt install -y sudo git netcat iptables rsync unzip wget curl jq coreutils openssh-server net-tools vim python3-pip python3-dev libffi-dev libssl-dev cmake pkg-config libfuse-dev iperf traceroute iproute2 iputils-ping && apt-get -y clean:                                                                                             
0.721                                                                                                                                                                                   
0.721 WARNING: apt does not have a stable CLI interface. Use with caution in scripts.                                                                                                   
0.721                                                                                                                                                                                   
0.901 Ign:1 http://security.debian.org/debian-security buster/updates InRelease
0.902 Err:2 http://security.debian.org/debian-security buster/updates Release
0.902   404  Not Found [IP: 151.101.194.132 80]
0.903 Ign:3 http://deb.debian.org/debian buster InRelease
0.906 Ign:4 http://deb.debian.org/debian buster-updates InRelease
0.909 Err:5 http://deb.debian.org/debian buster Release
0.909   404  Not Found [IP: 151.101.2.132 80]
0.911 Err:6 http://deb.debian.org/debian buster-updates Release
0.911   404  Not Found [IP: 151.101.2.132 80]

{code}

the simple approach is to use 17-bullseye instead of 17-buster ",Open,To Do,Bug,Major,Tsung-Han Ho,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19500,13623313,2025-07-14T10:17:07.000+0000,2025-07-15T14:06:11.000+0000,,kafka-consumer-groups.sh should fail quickly if the partition leader is unavailable,"
{code:java}
Error: Executing consumer group command failed due to java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
	at org.apache.kafka.tools.OffsetsUtils.getLogTimestampOffsets(OffsetsUtils.java:190)
	at org.apache.kafka.tools.OffsetsUtils.resetByDuration(OffsetsUtils.java:352)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.prepareOffsetsToReset(ConsumerGroupCommand.java:1015)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.resetOffsetsForInactiveGroup(ConsumerGroupCommand.java:704)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.lambda$resetOffsets$24(ConsumerGroupCommand.java:681)
	at java.base/java.util.HashMap.forEach(HashMap.java:1429)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand$ConsumerGroupService.resetOffsets(ConsumerGroupCommand.java:675)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand.run(ConsumerGroupCommand.java:130)
	at org.apache.kafka.tools.consumer.group.ConsumerGroupCommand.main(ConsumerGroupCommand.java:110)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets(api=METADATA), deadlineMs=1752487911886, tries=492805, nextAllowedTryMs=1752487912888) timed out at 1752487911888 after 492805 attempt(s)
	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)
	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:155)
	at org.apache.kafka.tools.OffsetsUtils.getLogTimestampOffsets(OffsetsUtils.java:167)
	... 8 more

{code}

`Admin#listOffsets` needs to communicate to the partition leader to get latest information. Hence, the call hangs if the node hosting the leader is unavailable. It should fail quickly by using `describeTopics` to check the leaders for all input partitions",Open,To Do,Improvement,Minor,kangning.li,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19499,13623305,2025-07-14T08:50:23.000+0000,2025-07-14T08:50:23.000+0000,,Correct the logger name in PersisterStateManager,"Currently, the logs in PersisterStateManager are logged using a logger defined in the inner class PersisterStateManagerHandler using LoggerFactory.getLogger(getClass()). This does not allow us to change the log level dynamically in kafka brokers. The solution for this is to use LoggerFactory.getLogger(getClass().getCanonicalName()) instead. ",Open,To Do,Sub-task,Minor,Chirag Wadhwa,Chirag Wadhwa,KAFKA,Kafka,,,,,
KAFKA-19498,13623291,2025-07-14T06:13:09.000+0000,2025-07-14T15:16:23.000+0000,,Add include argument to ConsumerPerformance tool,"Add include argument to ConsumerPerformance tool to subscribe to multiple topics.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-1192%3A+Add+include+argument+to+ConsumerPerformance+tool",Open,To Do,Improvement,Major,Federico Valeri,Federico Valeri,KAFKA,Kafka,,,tools,kip-required,
KAFKA-19497,13623185,2025-07-11T17:51:33.000+0000,2025-07-15T05:13:41.000+0000,,Topic replay code does not handle creation and deletion properly if it occurs in the same batch,"There is a small logic bug in topic replay. If a topic is created and then removed before the TopicsDelta is applied, we end up with the deleted topic in {{createdTopics}} on the delta but not in deletedTopicIds. I think we are extremely unlikely to see this since MetadataLoader will apply the delta for each batch of records it receives. Since it’s impossible to see a TopicRecord and RemoveTopicRecord in the same batch, the only way this could surface is if MetadataLoader did some buffering.

{{}}",Open,To Do,Bug,Major,zhu zhe,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19496,13623154,2025-07-11T12:36:28.000+0000,2025-07-13T16:37:12.000+0000,2025-07-13T16:35:30.000+0000,Failing test: DescribeStreamsGroupTest.testDescribeMultipleStreamsGroupWithMembersAndVerboseOptions(),https://github.com/apache/kafka/actions/runs/16216903812/job/45788662908?pr=20153,Resolved,Done,Bug,Major,Alieh Saeedi,Apoorv Mittal,KAFKA,Kafka,,,"streams,unit tests",flaky-test,4.2.0
KAFKA-19495,13623061,2025-07-10T15:31:18.000+0000,2025-07-11T16:01:03.000+0000,2025-07-11T16:01:03.000+0000,DefaultJwtRetriever could not be found with Docker native image,"I built a native image for 4.1.0-rc0 (https://github.com/apache/kafka/actions/runs/16196532977/job/45724349624) and pushed it to DockerHub (https://hub.docker.com/layers/apache/kafka-native/4.1.0-rc0/images/sha256-dd69cac8a2aa1282e42b25f92d2a66f29c6a6bf375bc8759b9728b2d2c0561c7)

When trying to run the image, I get the following error:

{noformat}
docker run -p 9092:9092 apache/kafka-native:4.1.0-rc0
===> User
uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
===> Setting default values of environment variables if not already set.
CLUSTER_ID not set. Setting it to default value: ""5L6g3nShT-eMCtK--X86sw""
===> Configuring ...
===> Launching ...
Exception in thread ""main"" java.lang.ExceptionInInitializerError at org.apache.kafka.server.config.AbstractKafkaConfig.<clinit>(AbstractKafkaConfig.java:56) at java.base@21.0.2/java.lang.Class.ensureInitialized(DynamicHub.java:601) at kafka.tools.StorageTool$.$anonfun$execute$1(StorageTool.scala:79) at scala.Option.flatMap(Option.scala:283) at kafka.tools.StorageTool$.execute(StorageTool.scala:79) at kafka.tools.StorageTool$.main(StorageTool.scala:46) at kafka.docker.KafkaDockerWrapper$.main(KafkaDockerWrapper.scala:57) at kafka.docker.KafkaDockerWrapper.main(KafkaDockerWrapper.scala) at java.base@21.0.2/java.lang.invoke.LambdaForm$DMH/sa346b79c.invokeStaticInit(LambdaForm$DMH) Caused by: org.apache.kafka.common.config.ConfigException: Invalid value org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever for configuration sasl.oauthbearer.jwt.retriever.class: Class org.apache.kafka.common.security.oauthbearer.DefaultJwtRetriever could not be found. at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:778) at org.apache.kafka.common.config.ConfigDef$ConfigKey.<init>(ConfigDef.java:1271) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:155) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:198) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:237) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:399) at org.apache.kafka.common.config.ConfigDef.define(ConfigDef.java:412) at org.apache.kafka.common.config.internals.BrokerSecurityConfigs.<clinit>(BrokerSecurityConfigs.java:197) ... 9 more
{noformat}

Since DefaultJwtRetriever is dynamically loaded at runtime, I wonder if the docker/native/native-image-configs/reflect-config.json file needs to be updated.
",Resolved,Done,Bug,Blocker,Luke Chen,Mickael Maison,KAFKA,Kafka,,,docker,,4.1.0
KAFKA-19494,13623048,2025-07-10T13:29:44.000+0000,2025-07-11T08:12:15.000+0000,2025-07-11T08:12:15.000+0000,Undeprecate JoinGroup V0 & V1 in 3.x,"We added back support for JoinGroup v0 & v1 in Kafka 4.0.1 and 4.1.0 due to KAFKA-19444. Due to that, we should undeprecate these protocol api versions in 3.x.",Resolved,Done,Improvement,Major,,Ismael Juma,KAFKA,Kafka,,,,,"3.7.3,3.8.2,3.9.2"
KAFKA-19493,13623043,2025-07-10T13:13:39.000+0000,2025-07-12T05:03:23.000+0000,,Incorrect rate metric with larger window size,"The Kafka Rate metric gives incorrect rate when the window size is larger say hourly. 

I suspect the code for calculating rate is incorrect for metrics which are not emitting rate per second or millisecond. The code for time unit calculation seems wrong: [convert(windowSize(config, now), unit)|https://github.com/apache/kafka/blob/da4fbba2793528e283458e080a690ad141857b0b/clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java#L67] i.e. the convert code divides the elapsed time in window to the time unit.

 

The problem can be easily reporduced by looking at `rebalance-rate-per-hour` metric.",Open,To Do,Bug,Major,Lan Ding,Apoorv Mittal,KAFKA,Kafka,,,"clients,metrics",,
KAFKA-19492,13623021,2025-07-10T10:54:47.000+0000,2025-07-10T10:58:24.000+0000,,ReplicaManager.deleteRecordsOnLocalLog should debug log the exception message when OffsetOutOfRangeException occurs,"When using the admin client to delete records on a Kafka partition, the deletion may fail for a few different reasons. One of those reasons is OFFSET_OUT_OF_RANGE. 

While the code that throws OffsetOutOfRangeException already has very good exception messages, those messages are lost because deleteRecordsOnLocalLog discards them. 

This makes it hard to tell what the issue was when an admin client receives this error.

It would be convenient if deleteRecordsOnLocalLog would debug log these messages.",Open,To Do,Task,Minor,,Stig Rohde Døssing,KAFKA,Kafka,,,core,,
KAFKA-19491,13622974,2025-07-10T00:09:16.000+0000,2025-07-10T00:12:33.000+0000,,Add documentation for missing AsyncKafkaConsumer metrics,The metrics added as part of [KIP-1068|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1068%3A+New+metrics+for+the+new+KafkaConsumer] need to be added to the top-level Kafka docs.,Open,To Do,Task,Major,Kirk True,Kirk True,KAFKA,Kafka,,,"clients,consumer,docs,documentation",,
KAFKA-19490,13622954,2025-07-09T16:05:06.000+0000,2025-07-10T01:35:46.000+0000,,Remove usages of distutils in docker scripts,"The distutils module was removed from the Python standard library in Python 3.12. We still use this module in docker/common.py and docker/docker_build_test.py.

We should switch to the shutils module which is part of the standard library. It provides similar methods (copy_file(), copy_tree()) than distutils.",Open,To Do,Task,Major,PoAn Yang,Mickael Maison,KAFKA,Kafka,,,docker,,
KAFKA-19489,13622947,2025-07-09T15:29:01.000+0000,2025-07-11T13:53:48.000+0000,,storage tool should check controller.quorum.voters is not set alongside a dynamic quorum flag when formatting,"The storage tool currently allows for setting both the static voters config ({{{}controller.quorum.voters{}}}) and attempting to format with one of {{--standalone, --initial-controllers}} on a controller, but instead it should throw an exception. This is because setting {{controller.quorum.voters}} itself is formatting the voter set.

Setting {{controller.quorum.voters}} while trying to format with a --standalone and {{-no-initial-controllers}} setup can result in 2 voter sets. For example, in a three node setup, the two nodes that formatted with --no-initial-controllers could form quorum with each other since they have the static voter set, and the {{-standalone}} node would ignore the config and read the voter set of itself from its log, forming its own quorum of 1.",Open,To Do,Bug,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19488,13622935,2025-07-09T14:50:01.000+0000,2025-07-10T02:26:26.000+0000,2025-07-10T02:26:18.000+0000,"Update the docs of ""if-not-exists""","""the action will only execute"" is incorrect, as the admin still sends the request. The ""if-not-exists"" flag is actually used to swallow the exception. ",Resolved,Done,Improvement,Minor,xuanzhang gong,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19487,13622927,2025-07-09T14:02:55.000+0000,2025-07-09T14:02:55.000+0000,,Improve consistency of command-line arguments,This issue tracks the development of [KIP-1147: Improve consistency of command-line arguments|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Improve+consistency+of+command-line+arguments].,Open,To Do,New Feature,Major,Andrew Schofield,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-19486,13622923,2025-07-09T13:44:51.000+0000,2025-07-10T02:44:29.000+0000,,Always use the latest version of kafka-topics.sh to create topics in system tests,"Using ""old"" kafka-topics.sh to create topics on ""old"" brokers is stable, but it also has some disadvantages.

1. E2E does not cover the case of using ""new"" kafka-topics.sh on ""old"" brokers
2. it requires a bunch of conditions for ""zk"", since some old kafka-topics.sh require using zk connection

In short, we should always use latest kafka-topics.sh to create topics on ""old"" brokers",Open,To Do,Improvement,Major,Chih-Yuan Chien,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19485,13622892,2025-07-09T07:47:25.000+0000,2025-07-10T09:29:56.000+0000,2025-07-10T09:29:56.000+0000,Acknowledgements should not be sent on initial epoch of ShareFetch,"There are some race scenarios in multi-broker environments where acknowledgements are sent on an initial epoch in a ShareSession.

This could arise if
 # Consumer subscribed to a partition whose leader was node-0.
 # Broker restart happens and node-0 is elected leader again. Broker starts a new ShareSession.
 # Background thread sends a fetch request with non-zero epoch.
 # Broker responds with SHARE_SESSION_NOT_FOUND.
 # Client updates session epoch to 0 once it receives this error. 
 # Application thread processing the previous fetch, completes and sends acks to piggyback on next fetch.
 # Consumer is no longer *subscribed* to this partition (from heartbeat response) due to rebalance/change in assignment.
 # Next fetch will send the piggyback acknowledgements on the fetch for previously subscribed partitions resulting in error from broker (""Acknowledgments sent on initial epoch"").

*Fix :*  Add a check before sending out acknowledgments if we are on an initial epoch and add necessary tests.",Resolved,Done,Sub-task,Major,Shivsundar R,Shivsundar R,KAFKA,Kafka,,,,,4.2.0
KAFKA-19484,13622854,2025-07-08T19:46:51.000+0000,2025-07-08T19:47:06.000+0000,,Tiered Storage Quota Metrics can stop reporting,"It is possible for tiered storage throttle metrics (introduced as a part of [KIP-956|https://cwiki.apache.org/confluence/display/KAFKA/KIP-956+Tiered+Storage+Quotas]) to stop reporting if the relevant tiered storage operation (copy/fetch) goes idle for longer than the sensor expiry timeout of one hour.

 

RemoteLogManager maintains a static reference to the sensors used for metric reporting. This is a problem because the default sensor expiry time is one hour and there is nothing responsible for handling expired sensors. If the sensors expire, RemoteLogManager will continue producing metrics through it's static references to sensor objects that have already been cleaned up by the ExpireSensorTask.

 

This issue tends to affect fetch metrics a lot more than copy metrics because the copy sensors don't go idle unless the topics stop being produced to. In contrast, the use case of backfilling from earliest offset using tiered storage is a pretty common use case.

 

*Reproduction*
 * Generate some amount of tiered storage fetch traffic on a topic. Confirm the remote-fetch-throttle-time-avg/max metrics are being reported.
 * Remove the consumer workload that triggers the tiered storage fetch traffic. Wait for one hour (the sensor expiration period)
 * Generate some more tiered storage fetch traffic. The metric will no longer report.",Open,To Do,Bug,Minor,,George Wu,KAFKA,Kafka,,,Tiered-Storage,,
KAFKA-19483,13622839,2025-07-08T17:56:22.000+0000,2025-07-08T18:06:40.000+0000,,Fix flaky test RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists(),"Observed in :

[https://github.com/apache/kafka/actions/runs/16130278655/job/45516681844]

[https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 
{code:java}
2025-07-05T09:39:50.2299240Z org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists() failed, log available in /home/runner/work/kafka/kafka/storage/build/reports/testOutput/org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testConcurrentCacheDeletedFileExists().test.stdout 2025-07-05T09:39:50.2300479Z 2025-07-05T09:39:50.2300817Z Gradle Test Run :storage:test > Gradle Test Executor 50 > RemoteIndexCacheTest > testConcurrentCacheDeletedFileExists() FAILED 2025-07-05T09:39:50.2301484Z org.opentest4j.AssertionFailedError: expected: <0> but was: <1> 2025-07-05T09:39:50.2302021Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) 2025-07-05T09:39:50.2303093Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T09:39:50.2303807Z at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) 2025-07-05T09:39:50.2304339Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) 2025-07-05T09:39:50.2305021Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) 2025-07-05T09:39:50.2305539Z at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) 2025-07-05T09:39:50.2306163Z at app//org.apache.kafka.test.TestUtils.assertNoLeakedThreadsWithNameAndDaemonStatus(TestUtils.java:179) 2025-07-05T09:39:50.2306912Z at app//org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.cleanup(RemoteIndexCacheTest.java:143){code}",Open,To Do,Improvement,Major,,Sushant Mahajan,KAFKA,Kafka,,,,,
KAFKA-19482,13622838,2025-07-08T17:55:14.000+0000,2025-07-15T00:14:11.000+0000,2025-07-15T00:14:11.000+0000,Fix flaky test KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics,"KafkaStreamsTelemetryIntegrationTest > ""shouldPassMetrics(String, boolean, String).topologyType=complex, stateUpdaterEnabled=true, groupProtocol=streams""

 

Observed in: [https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 

 
{code:java}
2025-07-05T10:46:22.1980071Z org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(String, boolean, String)[7] failed, log available in /home/runner/work/kafka/kafka/streams/integration-tests/build/reports/testOutput/org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(String, boolean, String)[7].test.stdout 2025-07-05T10:46:22.1988523Z 2025-07-05T10:46:22.1990639Z Gradle Test Run :streams:integration-tests:test > Gradle Test Executor 121 > KafkaStreamsTelemetryIntegrationTest > shouldPassMetrics(String, boolean, String) > ""shouldPassMetrics(String, boolean, String).topologyType=complex, stateUpdaterEnabled=true, groupProtocol=streams"" FAILED 2025-07-05T10:46:22.1996767Z org.opentest4j.AssertionFailedError: expected: <92> but was: <96> 2025-07-05T10:46:22.1997817Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151) 2025-07-05T10:46:22.1999309Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T10:46:22.2000511Z at app//org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:197) 2025-07-05T10:46:22.2001638Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:150) 2025-07-05T10:46:22.2002596Z at app//org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:145) 2025-07-05T10:46:22.2003688Z at app//org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:531) 2025-07-05T10:46:22.2005204Z at app//org.apache.kafka.streams.integration.KafkaStreamsTelemetryIntegrationTest.shouldPassMetrics(KafkaStreamsTelemetryIntegrationTest.java:298){code}
 ",Resolved,Done,Improvement,Major,Matthias J. Sax,Sushant Mahajan,KAFKA,Kafka,,,"streams,unit tests",flaky-test,4.2.0
KAFKA-19481,13622837,2025-07-08T17:52:57.000+0000,2025-07-08T18:07:33.000+0000,,Fix flaky test AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex(),"Observed in 

[https://github.com/apache/kafka/actions/runs/16086455065/job/45398324767?pr=20104]

 
{code:java}
2025-07-05T09:11:08.8004324Z Gradle Test Run :core:test > Gradle Test Executor 21 > SaslOAuthBearerSslEndToEndAuthorizationTest > testProduceConsumeViaSubscribe(String) > testProduceConsumeViaSubscribe(String).groupProtocol=classic STARTED 2025-07-05T09:11:09.6005665Z kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex() failed, log available in /home/runner/work/kafka/kafka/core/build/reports/testOutput/kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex().test.stdout 2025-07-05T09:11:09.6008071Z 2025-07-05T09:11:09.6008981Z Gradle Test Run :core:test > Gradle Test Executor 22 > AuthorizerIntegrationTest > testConsumerGroupHeartbeatWithRegex() FAILED 2025-07-05T09:11:09.6011714Z org.opentest4j.AssertionFailedError: Unexpected assignment ConsumerGroupHeartbeatResponseData(throttleTimeMs=0, errorCode=0, errorMessage=null, memberId='dBGv01cmTbOyJoyQDgpFxg', memberEpoch=1, heartbeatIntervalMs=5000, assignment=null) ==> expected: not <null> 2025-07-05T09:11:09.6014642Z at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:152) 2025-07-05T09:11:09.6016154Z at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132) 2025-07-05T09:11:09.6017649Z at app//org.junit.jupiter.api.AssertNotNull.failNull(AssertNotNull.java:49) 2025-07-05T09:11:09.6018855Z at app//org.junit.jupiter.api.AssertNotNull.assertNotNull(AssertNotNull.java:35) 2025-07-05T09:11:09.6020035Z at app//org.junit.jupiter.api.Assertions.assertNotNull(Assertions.java:312) 2025-07-05T09:11:09.6021483Z at app//kafka.api.AuthorizerIntegrationTest.sendAndReceiveRegexHeartbeat(AuthorizerIntegrationTest.scala:4085) 2025-07-05T09:11:09.6051031Z at app//kafka.api.AuthorizerIntegrationTest.testConsumerGroupHeartbeatWithRegex(AuthorizerIntegrationTest.scala:3125){code}
 ",Open,To Do,Improvement,Major,,Sushant Mahajan,KAFKA,Kafka,,,,,
KAFKA-19480,13622801,2025-07-08T11:22:27.000+0000,2025-07-08T11:37:11.000+0000,,KRaft migration hangs when /migration has null value,"When using the [zookeeper-security-migration|https://kafka.apache.org/39/documentation.html#zk_authz_migration] tool without the '–enable.path.check' option, the script not only updates the ACLs for the existing znodes, but also creates any non-existing ones (with the ACL options specified) using null values based on the list defined in [ZkData.SecureRootPaths.|https://github.com/apache/kafka/blob/3.9/core/src/main/scala/kafka/zk/ZkData.scala#L1089-L1102] This is especially problematic for the /migration znode as the current logic only checks for the existence of the znode and later the migration process will hang when it tries to parse the null value over and over again. 

In summary, the migration cannot be completed if the zookeeper-security-migration script was run previously, and the only workaround is to manually remove the /migration znode in such cases. I propose a simple fix to circumvent the manual step by recreating the /migration znode if it contains a null value.",Patch Available,In Progress,Bug,Major,Gergely Harmadás,Gergely Harmadás,KAFKA,Kafka,,,"kraft,migration",,
KAFKA-19479,13622789,2025-07-08T09:23:07.000+0000,2025-07-15T09:02:54.000+0000,,"at_least_once mode in Kafka Streams silently drops messages when the producer fails with MESSAGE_TOO_LARGE, violating delivery guarantees","*Description*

It appears there is a scenario where Kafka Streams running with {{processing.guarantee=at_least_once}} does {*}not uphold its delivery guarantees{*}, resulting in *message loss.*

 

*Reproduction Details*

We run a simple Kafka Streams topology like the following:
{code:java}
props[StreamsConfig.APPLICATION_ID_CONFIG] = ""poc-at-least-once""
props[StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name
props[StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG] = Serdes.String().javaClass.name
props[StreamsConfig.PROCESSING_GUARANTEE_CONFIG] = StreamsConfig.AT_LEAST_ONCE
// Large producer batch size to induce MESSAGE_TOO_LARGE
props[ProducerConfig.LINGER_MS_CONFIG] = ""300000""
props[ProducerConfig.BATCH_SIZE_CONFIG] = ""33554432""
/**         
* a custom ProductionExceptionHandler is registered to demonstrate that it is not triggered in this scenario. 
* in fact, neither the ProductionExceptionHandler nor the StreamsUncaughtExceptionHandler are invoked during this failure        
*/ props[StreamsConfig.PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG] = ""poc.MyProductionExceptionHandler""

val stream = streamsBuilder.stream<String, String>(""input.topic"")
stream.peek { key, value -> println(""$key:$value"") }
      .to(""output.topic"")*
 {code}
 

*What we observe:*
 * Records from {{input.topic}} are consumed and buffered at producer side

 * After some time (likely based on {{{}commit.interval.ms{}}}), the *consumer offset is committed*
 * Producer records *flush* is triggered

 * The sendind of records to kafka broker fails with {{{}MESSAGE_TOO_LARGE{}}}{*}{*}

 * As a result, the application {*}commits offsets without actually producing the records{*}, which leads to *silent message loss*

 

*Steps to Reproduce*
 # Generate ~50,000 records (sized similarly to the sample project) in {{input.topic to induce MESSAGE_TOO_LARGE}}
 # Start the topology with the configuration above
 # Wait for all messages to be consumed
 # Observe:

 * 
 ** Offsets are committed

 * 
 ** Output topic receives no messages

 * 
 ** Log shows repeated {{MESSAGE_TOO_LARGE}} error:

{code:java}
11:50:30.695 [kafka-producer-network-thread | kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] WARN  o.a.k.c.producer.internals.Sender - [Producer clientId=kstreams-poc-v1-37858c2e-7584-4489-8081-0111f710c431-StreamThread-1-producer] Got error produce response in correlation id 255 on topic-partition output.topic-0, splitting and retrying (2147483647 attempts left). Error: MESSAGE_TOO_LARGE {code}
 

*Reproduced* with :
 * kafka-client-3.7.0, kafka-streams-3.7.0
 * kafka-client-4.-.0, kafka-streams-4.0.0
 * kafka-client-7.9.2-ccs, kafka-streams-7.9.2-ccs
 * kafka-client-8.0.0-ccs, kafka-streams-8.0.0-ccs

 

*Expected Behavior*

In {{at_least_once}} mode, Kafka Streams should *not commit offsets* unless records are {*}successfully produced{*}. 

 

*Attached*
 * configs for stream, producer, consumer
 * sample project used to replicate the issue",Open,To Do,Bug,Critical,,Mihai Lucian,KAFKA,Kafka,,,streams,,
KAFKA-19478,13622678,2025-07-07T10:42:10.000+0000,2025-07-15T14:45:02.000+0000,,Optimize sticky assignor,"The current assignor used in KIP-1071 is verbatim the assignor used on the client-side. The assignor performance was not a big concern on the client-side, and it seems some additional performance overhead has crept in during the adaptation to the broker-side interfaces, so we expected this implementation to be too slow for groups of non-trivial size. We need to optimize the assignor to be able to efficiently run on the broker.",In Progress,In Progress,Sub-task,Major,Lucas Brutschy,Lucas Brutschy,KAFKA,Kafka,,,streams,,
KAFKA-19477,13622663,2025-07-07T08:38:04.000+0000,2025-07-09T20:59:47.000+0000,2025-07-09T11:58:55.000+0000,Sticky Assignor JMH Benchmark,"The current assignor used in KIP-1071 is verbatim the assignor used on the client-side. The assignor performance was not a big concern on the client-side, and it seems some additional performance overhead has crept in during the adaptation to the broker-side interfaces, so we expect it to be too slow for groups of non-trivial size.",Resolved,Done,Sub-task,Major,Lucas Brutschy,Lucas Brutschy,KAFKA,Kafka,,,"streams,unit tests",,
KAFKA-19476,13622644,2025-07-06T18:20:22.000+0000,2025-07-14T12:09:32.000+0000,,Improve state transition handling in SharePartition,,In Progress,In Progress,Sub-task,Major,Abhinav Dixit,Abhinav Dixit,KAFKA,Kafka,,,,,
KAFKA-19474,13622586,2025-07-04T16:38:02.000+0000,2025-07-09T01:55:14.000+0000,2025-07-09T01:55:14.000+0000,Wrong placement of WARN log for truncation below HWM,"{{ReplicaFetcherThread#truncate}} has a useful [WARN log|https://github.com/apache/kafka/blob/da4fbba2793528e283458e080a690ad141857b0b/core/src/main/scala/kafka/server/ReplicaFetcherThread.scala#L171] when a follower is truncating its log segment below its HWM.

This is of particular help when debugging data loss scenarios like those described in the motivation for [KIP-966|https://cwiki.apache.org/confluence/display/KAFKA/KIP-966%3A+Eligible+Leader+Replicas].

Unfortunately, a refactoring in [PR 5608|https://github.com/apache/kafka/pull/5608/files#diff-693a5eaaaa4fe61a18af23d5b06ccc024261dda01558cbaacb7020e1911c6d55R264] moved the check to _after_ the truncation, which may modify the HWM. As a result, the log may not be emitted even though the log is truncated below the HWM",Resolved,Done,Bug,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,core,,4.2.0
KAFKA-19472,13622584,2025-07-04T16:19:17.000+0000,2025-07-09T02:22:19.000+0000,,A BufferOverflowException is thrown by RemoteLogManager when FetchApiVersion <= 2,"RemoteLogReader throws a BufferOverflowException when FetchApi version <= 2 and firstBatchSize exceeds max.partition.fetch.bytes

Exception Message
{code:java}
[2025-07-04 16:11:42,910] ERROR Error occurred while reading the remote data for remote-storage-perf-5-0 (kafka.log.remote.RemoteLogReader)
java.nio.BufferOverflowException
        at java.base/java.nio.ByteBuffer.put(ByteBuffer.java:1007)
        at java.base/java.nio.HeapByteBuffer.put(HeapByteBuffer.java:243)
        at org.apache.kafka.common.record.DefaultRecordBatch.writeTo(DefaultRecordBatch.java:236)
        at kafka.log.remote.RemoteLogManager.read(RemoteLogManager.java:1647)
        at kafka.log.remote.RemoteLogReader.lambda$call$0(RemoteLogReader.java:66)
        at com.yammer.metrics.core.Timer.time(Timer.java:91)
        at kafka.log.remote.RemoteLogReader.call(RemoteLogReader.java:66)
        at kafka.log.remote.RemoteLogReader.call(RemoteLogReader.java:36)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:842)



{code}
firstBatch.writeTo(buffer) will throw BufferOverflowException when firstBatch size is larger than  the size of buffer passed to the method.


The fix below appears to resolve this issue, however, the potential side effects remain unknown.


 !screenshot-1.png! ",Patch Available,In Progress,Bug,Major,dyingjiecai,dyingjiecai,KAFKA,Kafka,,,log,,
KAFKA-19471,13622579,2025-07-04T15:19:57.000+0000,2025-07-15T02:12:02.000+0000,,Enable acknowledgement for a record which could not be deserialized,"If a record fetched by a share consumer fails to be deserialized, the KIP states that it is automatically released and that the application cannot override this behavior. Actually, experience with KafkaShareConsumer shows that it would be helpful to be able to override this to REJECT such records instead.

We can add an override `KafkaShareConsumer.acknowledge(String topic, int partition, long offset, AcknowledgeType type)` for this purpose where the user does not have a `ConsumerRecord` instance available, but they do know the topic, partition and offset from the DeserializationException. The validation of this information is exactly the same as for the same information when it is implied by the `ConsumerRecord`.",In Progress,In Progress,Sub-task,Major,Lan Ding,Andrew Schofield,KAFKA,Kafka,,,,,4.2.0
KAFKA-19470,13622553,2025-07-04T11:10:59.000+0000,2025-07-12T03:34:00.000+0000,,Avoid creating loggers repeatedly to affect the performance of request processing,"We see that the broker has a performance bottleneck when processing requests in kafka 2.8.2 version + jdk 11 environment. The CPU profiler is as follows:

!image-2025-07-04-19-05-51-900.png|width=1065,height=489!

After research, we found that this is a problem with jdk11 but without resolution [https://bugs.openjdk.org/browse/JDK-8266964]  StackWalker has a performance degradation for jdk11.

When creating every logger, log4j2 will invoke StackWalker, the following is the performance data observed with arthas. It takes about 60ms to create each logger.

!image-2025-07-04-19-25-57-774.png|width=725,height=367!

we should set the logger variable of each class to static to avoid wasting performance on creating loggers.

seen in KAFKA-15141, the corresponding classes IncrementalFetchContext/DelayedProduce/SessionlessFetchContext have been processed, but I think a better approach should be to optimize the trait Logging, to avoid similar performance issues later. We can introduce a static Map object to cache the duplicate loggers.

 ",Open,To Do,Improvement,Major,,terrytlu,KAFKA,Kafka,,,core,,
KAFKA-19469,13622523,2025-07-04T06:33:21.000+0000,2025-07-08T10:32:08.000+0000,,Dead-letter queues for share groups,This Jira tracks the development of KIP-1191: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1191%3A+Dead-letter+queues+for+share+groups,Open,To Do,New Feature,Major,,Lan Ding,KAFKA,Kafka,,,,queues-for-kafka,
KAFKA-19466,13622386,2025-07-02T19:53:36.000+0000,2025-07-09T17:01:47.000+0000,2025-07-09T17:01:47.000+0000,LogConcurrencyTest should close the log when the test completes,"In 
testUncommittedDataNotConsumedFrequentSegmentRolls() and testUncommittedDataNotConsumed(), we call createLog(), but never close the log when the tests complete.",Resolved,Done,Improvement,Major,Jhen-Yung Hsu,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19464,13622333,2025-07-02T12:47:50.000+0000,2025-07-15T10:54:10.000+0000,,remove findNextFetchOffset.set(true) from acknowledge and release acquired records functionality,https://github.com/apache/kafka/pull/20080#discussion_r2179796106,Open,To Do,Sub-task,Major,Apoorv Mittal,Abhinav Dixit,KAFKA,Kafka,,,,,
KAFKA-19463,13622236,2025-07-01T12:20:19.000+0000,2025-07-08T13:14:12.000+0000,2025-07-08T13:14:12.000+0000,nextFetchOffset does not take ongoing state transition into account,,Resolved,Done,Sub-task,Major,Abhinav Dixit,Abhinav Dixit,KAFKA,Kafka,,,,,4.2.0
KAFKA-19462,13622234,2025-07-01T12:14:42.000+0000,2025-07-08T17:55:09.000+0000,2025-07-03T02:46:49.000+0000,"""fetch.max.bytes"" config is not honored when remote + local fetch","Currently in local fetch case, we'll calculate the remaining bytes to be fetched for each partition via ""fetch.max.bytes"" and ""max.partition.fetch.bytes"" configs. For example:
 # Config:
max.partition.fetch.bytes = 1MB
fetch.max.bytes = 1.5MB
 # Topic foo has 2 partitions.
 # Consumer fetches data from topic foo
 # Fetches from foo-0 first, it got 1MB of data (max.partition.fetch.bytes), so remaining 0.5 MB of data available to be fetched
 # Fetches from foo-1 for max 0.5MB.
 # Total returned 1.5MB records

However, in remote + local fetch case, because we don't know how much data we can fetch before querying remote log metadata manager or other resource, we can't have a value to tell replicaManager beforehand. Currently, we treat it as 0 bytes read. And that's why the final returned data could exceed the ""fetch.max.bytes"" value.

For example:
 # Config:
max.partition.fetch.bytes = 1MB
fetch.max.bytes = 1.5MB
 # Topic foo has 2 partitions + topic boo has 1 partition with tiered storage enabled.
 # Consumer fetches data from topic foo and boo
 # Fetches from boo-0, because we don't know how much data we can get, return 0, and send to remote async read.
 # Fetches from foo-0, it got 1MB of data, so remaining 0.5 MB of data available to be fetched
 # Fetches from foo-1 for max 0.5MB.
 # remote async read for boo-0 returned 1MB data (max.partition.fetch.bytes).
 # Total returned 2.5MB records, which exceeds `fetch.max.bytes = 1.5MB`

 ",Resolved,Done,Bug,Major,Luke Chen,Luke Chen,KAFKA,Kafka,,,,,4.2.0
KAFKA-19461,13622220,2025-07-01T09:55:56.000+0000,2025-07-11T03:35:45.000+0000,,Add share group admin integration tests to PlaintextAdminIntegrationTest,"Admin.deleteShareGroupOffsets, Admin.alterShareGroupOffsets and other methods added in KIP-932 should have some integration tests.",In Progress,In Progress,Sub-task,Major,Lan Ding,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-19460,13622218,2025-07-01T09:35:17.000+0000,2025-07-14T08:09:09.000+0000,2025-07-14T04:57:50.000+0000,fetch result might have size < fetch.min.bytes even if data is available in replica ,"In the doc of ""[fetch.min.bytes|https://kafka.apache.org/documentation/#consumerconfigs_fetch.min.bytes]"", it said:

??The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.??

It makes users believe the records returned will always greater fetch.min.bytes if there is sufficient data in replica. But even if the data is sufficient is available in the replica, there is still possible the returned records size < fetch.min.bytes.

 

For example:
 # Config 
fetch.max.bytes=1500
max.partition.fetch.bytes=1000
fetch.min.bytes=1100
fetch.max.wait.ms=500
 # topic foo has 2 partitions, and each partition contains 1 record with size 1000 bytes.
 # When a consumer fetches data from these 2 partitions, it starts from foo-0, and fetch 1000 bytes of data, and 500 bytes left before reaching fetch.max.bytes.
 # When fetching foo-1, since we only have 500 bytes available to be fetched, and the first batch size in foo-1 is 1000 bytes, which is greater than 500, so we don't fetch it.
 # In the end, the total returned size is 1000 bytes, which is less than fetch.min.bytes, without waiting until `fetch.max.wait.ms` expired. It's because we checked the total size in replicas are more than ""fetch.min.bytes"", so no wait for ""fetch.max.wait.ms"".

 

I think the logic is correct. It's just we need to update the doc to make it clear to users. We might also need to check `replica.fetch.min.bytes` config.",Resolved,Done,Improvement,Major,Xuze Yang,Luke Chen,KAFKA,Kafka,,,,,4.2.0
KAFKA-19459,13622201,2025-07-01T05:17:33.000+0000,2025-07-15T03:53:46.000+0000,2025-07-15T03:53:46.000+0000,List internal topics for the user,"If internal topics cannot be deleted automatically due to a version mismatch (see [here|https://github.com/apache/kafka/blob/c8f83592b2acdee0e2aa6993b378ba9e429a1f1e/tools/src/main/java/org/apache/kafka/tools/streams/StreamsGroupCommand.java#L493] and [here|https://github.com/apache/kafka/blob/c8f83592b2acdee0e2aa6993b378ba9e429a1f1e/tools/src/main/java/org/apache/kafka/tools/streams/StreamsGroupCommand.java#L827]), they must be removed manually. To assist the user, retrieve the internal topics associated with this application and include them in the error message.",Resolved,Done,Sub-task,Trivial,Alieh Saeedi,Alieh Saeedi,KAFKA,Kafka,,,,,4.2.0
KAFKA-19458,13622186,2025-06-30T23:47:27.000+0000,2025-07-14T09:56:06.000+0000,,Successive AlterReplicaLogDirsRequest on a topic partition may leak log segments,"Successive {{AlterReplicaLogDirsRequest}} to change log directory of a given topic partition may cause log segment leak. Consider the following scenario:

1. A request tries to change the logdir for topic partition {{tp}} from {{d1}} to {{d2}}.
2. The handler invokes {{replicaManager#alterReplicaLogDirs}}
3. A future replica is created as a result of the above method invoking {{partition#maybeCreateFutureReplica}} and cleaning for {{tp}} is disabled as {{logManager#abortAndPauseCleaning}} is invoked.
4. Now, *before* the previous request is completed, let's assume another request to change the logdir from {{d2}} to {{d3}}
5. This time, {{replicaManager#alterReplicaLogDirs}}'s call to {{partition#futureReplicaDirChanged}} will return {{true}} and we remove the fetcher and unset the reference to {{futureLog}} in {{Partition}}.
6. We then re-create a future by invoking {{partition#maybeCreateFutureReplica}} with {{d3}} and pause log cleaning for {{tp}} *again*.
7. {{partition#maybeReplaceCurrentWithFutureReplica}} is invoked when the future has caught up and the callback in it swaps the future log for the local log and resumes cleaning by invoking {{LogManager#resumeCleaning}}.
8. The above decrements the count in {{LogCleaningState.logCleaningPaused}} from {{2}} to {{1}}. Cleanup for {{tp}} is therefore paused until a broker restart",Patch Available,In Progress,Bug,Major,Gaurav Narula,Gaurav Narula,KAFKA,Kafka,,,,,4.2.0
KAFKA-19457,13622174,2025-06-30T19:01:52.000+0000,2025-07-09T08:53:01.000+0000,2025-07-09T08:52:52.000+0000,Revisit the threshold to retry initialize share state RPC in group coordinator.,,Resolved,Done,Sub-task,Major,Sushant Mahajan,Sushant Mahajan,KAFKA,Kafka,,,,,4.2.0
KAFKA-19452,13622167,2025-06-30T18:33:02.000+0000,2025-07-14T16:48:18.000+0000,2025-07-14T16:48:18.000+0000,Fix flaky test: LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments,"Saw multiple occurrences of the following failure. [https://github.com/apache/kafka/actions/runs/15937460305/job/44960490730?pr=19964] and [https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961.|https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961]

 

 
{code:java}
2025-06-27T23:43:12.2936738Z kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments() failed, log available in /home/runner/work/kafka/kafka/core/build/reports/testOutput/kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments().test.stdout
2025-06-27T23:43:12.2938722Z 
2025-06-27T23:43:12.2939140Z Gradle Test Run :core:test > Gradle Test Executor 17 > LogRecoveryTest > testHWCheckpointWithFailuresMultipleLogSegments() FAILED
2025-06-27T23:43:12.2939940Z     org.opentest4j.AssertionFailedError: Did not observe leader change for partition new-topic-0 after 15000 ms
2025-06-27T23:43:12.2940561Z         at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38)
2025-06-27T23:43:12.2945169Z         at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138)
2025-06-27T23:43:12.2946584Z         at app//kafka.utils.TestUtils$.awaitLeaderChange(TestUtils.scala:719)
2025-06-27T23:43:12.2947923Z         at app//kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:218) {code}
 

 ",Resolved,Done,Improvement,Major,Rajani Karuturi,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19451,13622165,2025-06-30T17:59:26.000+0000,2025-07-14T19:02:19.000+0000,2025-07-14T19:02:19.000+0000,Fix flaky test: RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval(),"Saw the following flaky test in [https://github.com/apache/kafka/actions/runs/15937660290/job/44960938007?pr=19961].
{code:java}
2025-06-28T00:09:57.4324761Z Gradle Test Run :storage:test > Gradle Test Executor 47 > RemoteIndexCacheTest > testCacheEntryIsDeletedOnRemoval() STARTED 2025-06-28T00:09:57.4327688Z org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval() failed, log available in /home/runner/work/kafka/kafka/storage/build/reports/testOutput/org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval().test.stdout 2025-06-28T00:09:57.4329862Z  2025-06-28T00:09:57.4330480Z Gradle Test Run :storage:test > Gradle Test Executor 47 > RemoteIndexCacheTest > testCacheEntryIsDeletedOnRemoval() FAILED 2025-06-28T00:09:57.4332568Z     java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/kafka-RemoteIndexCacheTest6205787022310961862/remote-log-index-cache/2147584984_WMUkQgEpTJ6GksATEZn4iQ.txnindex.deleted 2025-06-28T00:09:57.4334299Z         at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:87) 2025-06-28T00:09:57.4336792Z         at java.base/java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:103) 2025-06-28T00:09:57.4337909Z         at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1855) 2025-06-28T00:09:57.4339038Z         at java.base/java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:129) 2025-06-28T00:09:57.4340181Z         at java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:527) 2025-06-28T00:09:57.4341199Z         at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513) 2025-06-28T00:09:57.4342209Z         at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) 2025-06-28T00:09:57.4343223Z         at java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150) 2025-06-28T00:09:57.4344159Z         at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 2025-06-28T00:09:57.4345155Z         at java.base/java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:652) 2025-06-28T00:09:57.4346710Z         at org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.getIndexFileFromRemoteCacheDir(RemoteIndexCacheTest.java:1290) 2025-06-28T00:09:57.4348431Z         at org.apache.kafka.storage.internals.log.RemoteIndexCacheTest.testCacheEntryIsDeletedOnRemoval(RemoteIndexCacheTest.java:339) 2025-06-28T00:09:57.4349345Z  2025-06-28T00:09:57.4349471Z         Caused by: 2025-06-28T00:09:57.4350789Z         java.nio.file.NoSuchFileException: /tmp/kafka-RemoteIndexCacheTest6205787022310961862/remote-log-index-cache/2147584984_WMUkQgEpTJ6GksATEZn4iQ.txnindex.deleted 2025-06-28T00:09:57.4352393Z             at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) 2025-06-28T00:09:57.4353392Z             at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106) 2025-06-28T00:09:57.4354360Z             at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) 2025-06-28T00:09:57.4355793Z             at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) 2025-06-28T00:09:57.4356539Z             at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148) 2025-06-28T00:09:57.4357193Z             at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99) 2025-06-28T00:09:57.4357721Z             at java.base/java.nio.file.Files.readAttributes(Files.java:1851) 2025-06-28T00:09:57.4358201Z             at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:220) 2025-06-28T00:09:57.4358697Z             at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:277) 2025-06-28T00:09:57.4359159Z             at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:374) 2025-06-28T00:09:57.4359670Z             at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:83) 2025-06-28T00:09:57.4360085Z             ... 11 more {code}
 ",Resolved,Done,Improvement,Major,Lan Ding,Jun Rao,KAFKA,Kafka,,,,,4.2.0
KAFKA-19450,13622141,2025-06-30T13:04:09.000+0000,2025-07-09T08:51:36.000+0000,2025-07-09T08:51:36.000+0000,ShareConsumerPerformance does not handle exceptions from consumeMessagesForSingleShareConsumer,"Within {{{}ShareConsumerPerformance.java,a{}}}ll the share consumers run with within an {{executorService}} object and when we perform {{executorService.submit()}}  in {{{}ShareConsumerPerformance{}}}, we do not store this future and exception would be recovered only when we do a {{future.get()}} in this case. I believe this is a shortcoming in {{ShareConsumerPerformance.java}} which needs to be improved.",Resolved,Done,Sub-task,Major,Abhinav Dixit,Abhinav Dixit,KAFKA,Kafka,,,,,4.2.0
KAFKA-19444,13622001,2025-06-27T14:09:16.000+0000,2025-07-10T13:49:22.000+0000,2025-07-09T09:24:58.000+0000,SASL GSSAPI not working with librdkafka and AK 4.x,"By running librdkafka with AK 4.0 we see that SASL GSSAPI isn't working.
The feature is missing because librdkafka is incorrectly checking for JoinGroup v0 only and not v0+.

When testing librdkafka versions with 4.0 we missed this case so JoinGroup v0 and v1 were removed.

A [fix|https://github.com/confluentinc/librdkafka/pull/5131] is already merged in librdkafka and will be released in v2.11.0.
Enabling back the deprecated JoinGroup RPC versions will help the users looking to continue using Kerberos authentication without upgrading.",Resolved,Done,Bug,Critical,Ismael Juma,Emanuele Sabellico,KAFKA,Kafka,,,,,"4.0.1,4.1.0"
KAFKA-19441,13621946,2025-06-27T05:39:45.000+0000,2025-07-15T03:35:39.000+0000,,Encapsulate MetadataImage in GroupCoordinator,"The MetadataImage has a ton of stuff in it and it gets passed around all over the place in the new GroupCoordinator. This makes it difficult to understand what metadata the group coordinator actually relies on and makes it too easy to use metadata in ways it wasn't meant to be used. 

 

If we encapsulate the MetadataImage in an interface that clearly indicates what metadata the group coordinator actually uses, it is much easier at a glance to see what dependencies it has on the metadata. Also, if the source of metadata needs to evolve for whatever reason, we can do so easily by wrapping MetadataImage in an interface.",Open,To Do,Improvement,Major,Liz Bennett,Liz Bennett,KAFKA,Kafka,,,group-coordinator,,
KAFKA-19440,13621898,2025-06-26T14:41:36.000+0000,2025-07-09T08:55:29.000+0000,2025-07-03T10:08:49.000+0000,Admin.alterShareGroupOffsets doesn't handle exceptions correctly,"When testing [https://github.com/apache/kafka/pull/19820], it became clear that the error handling in Kafka admin for alterShareGroupOffsets isn't quite right and this means error codes do not make it back to kafka-share-groups.sh.",Resolved,Done,Sub-task,Major,Andrew Schofield,Andrew Schofield,KAFKA,Kafka,,,,,4.2.0
KAFKA-19435,13621752,2025-06-25T10:01:30.000+0000,2025-07-14T14:13:12.000+0000,2025-07-14T14:13:12.000+0000,Optimize `kafka-consumer-groups.sh` to return the offset info of other partitions even when the leader of some partitions are missing,"When we use the `{{{}kafka-consumer-groups.sh`{}}} script to query the LAG of some consumer group, if the leader of the corresponding topic-partition is missing, it will directly throw an exception, which may cause the following problems:
 # There are 10 partitions in total, but if only 1 partition lacks a leader, users will be unable to view the LAG of the other 9 partitions.
 # Throwing an exception directly will subconsciously make users think that there is a major failure in the cluster.

 

Perhaps we can optimize this script to return as much information as possible instead of throwing an exception directly.

 

 ",Resolved,Done,Improvement,Minor,kangning.li,kangning.li,KAFKA,Kafka,,,admin,,4.2.0
KAFKA-19431,13621684,2025-06-24T14:24:14.000+0000,2025-07-11T10:04:39.000+0000,,Stronger assignment consistency with subscription for consumer groups ,"Currently, consumer group assignments are eventually consistent with subscriptions: when a member has unrevoked partitions, it is not allowed to reconcile with the latest target assignment. If a member with unrevoked partitions shrinks its subscription, it may observe assignments from the broker containing topics it is no longer subscribed to.

If we wanted to, we could tighten this up at the cost of extra CPU time. Note that it's not feasible to close the gap for regex subscriptions, since there will always be a window when the regex is not yet resolved and we cannot tell whether a topic is part of the subscription.

One way to do this would be to update {{CurrentAssignmentBuilder}} and
 * Add a {{MetadataImage}} and map of resolved regexes
 * Define the set of subscribed topic uuids as the union of the topic name subscription and resolved regex topic names, like how {{TargetAssignmentBuilder}} does it.
 ** When the regex is unresolved, we can’t know which topics are part of the subscription. We treat unresolved regexes as matching no topics, to be conservative. This way, the assignment is always consistent with the subscription.
 * Update the loop over topics in {{computeNextAssignment}} to treat the assigned partitions as an empty set when the topic is not part of the subscription.
 * Do not advance the member epoch past the target assignment epoch when exiting the {{UNREVOKED_PARTITIONS}} state.
 * Define an {{updateCurrentAssignment}} method that drops any unsubscribed topics from the member’s current assignment and transitions the member to {{UNREVOKED_PARTITIONS}} if any topics were dropped.
 * Use {{updateCurrentAssignment}} on the other {{UNREVOKED_PARTITIONS}} path.

Additionally, if we ever end up with asynchronous assignors (such as client-computed ones),
 * We add a new flag to {{maybeReconcile}} called {{{}hasSubscriptionChanged{}}}. When the flag is set, we run the {{CurrentAssignmentBuilder}} even when reconciled to the target assignment, since the target assignment can lag behind the group epoch.
 * Use {{updateCurrentAssignment}} on all {{CurrentAssignmentBuilder}} paths that do not use {{{}computeNextAssignment{}}}.",Open,To Do,Improvement,Major,Sean Quah,Sean Quah,KAFKA,Kafka,,,group-coordinator,,
KAFKA-19430,13621594,2025-06-23T19:58:19.000+0000,2025-07-09T20:57:15.000+0000,,Don't fail on RecordCorruptedException,"From [https://github.com/confluentinc/kafka-streams-examples/issues/524]

Currently, the existing `DeserializationExceptionHandler` is applied when de-serializing the record key/value byte[] inside Kafka Streams. This implies that a `RecordCorruptedException` is not handled.

We should explore to not let Kafka Streams crash, but maybe retry this error automatically (as `RecordCorruptedException extends RetriableException`), and find a way to pump the error into the existing exception handler.

If the error is transient, user can still use `REPLACE_THREAD` in the uncaught exception handler, but this is a rather heavy weight approach.",Open,To Do,Improvement,Major,Uladzislau Blok,Matthias J. Sax,KAFKA,Kafka,,,streams,,
KAFKA-19427,13621524,2025-06-23T07:35:07.000+0000,2025-07-15T12:54:39.000+0000,,"The __consumer_offsets topic applies the broker configuration message.max.bytes, which may cause the coordinator broker to allocate too much memory and cause OOM","h3. Kafka cluster configuration

1.Kafka version：4.0
2.The cluster specifications are: 3 brokers and 3 controllers
3.JVM startup parameters:
!image-2025-06-23-14-16-00-554.png!

4.JDK version：
!image-2025-06-23-14-17-34-767.png!
h3. Steps to reproduce the problem

1.In this new cluster, create a test topic: {*}test{*}，and this cluster will eventually have *only this one topic* tested by external users.

topic config : NewTopic newTopic = new NewTopic(""test"", 3, (short) 1);
2.Start the producer and send 1,000 messages
3.Start the consumer and use the earliest strategy for consumption. The groupIds are rivenTest1/rivenTest2/.../rivenTest8

4.During the process of starting the consumer, it was found that some consumer groups failed to start, and the coordinator brokers corresponding to these groups also had OOM exceptions

client error logs：
{code:java}
[main] INFO org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector - initializing Kafka metrics collector
[main] INFO org.apache.kafka.common.security.authenticator.AbstractLogin - Successfully logged in.
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.9.1
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: f745dfdcee2b9851
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750661985923
[main] INFO org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Subscribed to topic(s): test
[main] INFO org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Cluster ID: 3esGOWhETi-zo2uHq7NsFg
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Discovered group coordinator 18-97-25-88-k.mq.zoomdev.us:9889 (id: 2147483644 rack: null)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: need to re-join with the given member-id: consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] (Re-)joining group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Successfully joined group with generation Generation{generationId=17, memberId='consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4', protocol='roundrobin'}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Finished assignment for group at generation 17: {consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4=Assignment(partitions=[test-0, test-1, test-2])}
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: rebalance failed due to 'Unexpected error from SyncGroup: The server experienced an unexpected error when processing the request.' (KafkaException)
org.apache.kafka.common.KafkaException: Unexpected error from SyncGroup: The server experienced an unexpected error when processing the request.
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:893)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$SyncGroupResponseHandler.handle(AbstractCoordinator.java:812)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1311)
    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:1286)
    at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:206)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)
    at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:429)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:314)
    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:253)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.pollForFetches(ClassicKafkaConsumer.java:692)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.poll(ClassicKafkaConsumer.java:623)
    at org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer.poll(ClassicKafkaConsumer.java:596)
    at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:874)
    at us.zoom.mq.examples.ConsumerTest.startConsumer(ConsumerTest.java:233)
    at us.zoom.mq.examples.ConsumerTest.main(ConsumerTest.java:149)
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Member consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4 sending LeaveGroup request to coordinator 18-97-25-88-k.mq.zoomdev.us:9889 (id: 2147483644 rack: null) due to the consumer is being closed
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Resetting generation and member id due to: consumer pro-actively leaving the group
[main] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] Request joining group due to: consumer pro-actively leaving the group
[main] ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=consumer-rivenTest6-1, groupId=rivenTest6] LeaveGroup request with Generation{generationId=17, memberId='consumer-rivenTest6-1-38849218-32fa-430d-b14c-d3ce7ff402c4', protocol='roundrobin'} failed with error: The server experienced an unexpected error when processing the request.
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
[main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
[main] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-rivenTest6-1 unregistered {code}
coordinator broker error logs：
!image-2025-06-23-15-04-26-598.png!

 

 
h3. Analysis:

This is a brand new Kafka4.0 cluster with only one topic created;
The JDK version is 17;
Why does the broker encounter OOM so quickly when it is just sending and consuming data? Is there a memory leak somewhere?
1 First, use the arthas tool to analyze the memory usage

!image-2025-06-23-15-04-15-708.png!

We can see that most of the heap memory is {*}occupied by the old generation{*}, and it is likely that the program will directly experience OOM of the heap memory when it needs to {color:#ff0000}*apply for a large object. It should be noted that the maximum memory we allocate to the Kafka process is actually 3G, and there is also a lot of space left in the heap memory. Why does it directly trigger the Java heap space type OOM in this case?*{color}

{color:#172b4d}2.Dump memory snapshots and use tools to analyze what is currently occupying a large amount of memory in the program
!image-2025-06-23-15-33-06-851.png!

!image-2025-06-23-15-33-26-209.png!
After analyzing the memory usage, I found that it was basically all the *coordinators* objects in the *CoordinatorRuntime* class that occupied the memory and did not release it; coordinators is a ConcurrentHashMap structure, the key is the TopicPartition type, and the value is the CoordinatorContext type.
!image-2025-06-23-15-11-13-026.png!

Why does a broker machine simply start a consumer, the topic has only three partitions, and the consumer group uses no more than 10 partitions in total, and the *coordinators* object in the broker process occupies such a large amount of memory and does not release it?
Is there a problem with the broker configuration or the JDK17 version or the jvm startup parameters, or is there a memory leak in the kafka 4.0 version code?{color}{color:#172b4d}Please help analyze and answer, looking forward to your reply.
Thank you very much!{color}",Open,To Do,Bug,Critical,,RivenSun,KAFKA,Kafka,,,"consumer,group-coordinator",,
KAFKA-19404,13620780,2025-06-12T15:24:57.000+0000,2025-07-15T10:45:13.000+0000,,Connect's plugin.path documentation does not match behavior,"The doc currently states:
{quote}The list should consist of top level directories that include any combination of:
a) directories immediately containing jars with plugins and their dependencies
b) uber-jars with plugins and their dependencies
c) directories immediately containing the package directory structure of classes of plugins and their dependencies
{quote}
In practice plugins are found even if they are in nested directories. For example we can set {{plugin.path}} to {{/var/plugins}} and it will find all these plugins:
{noformat}
/var/plugins
└── nested1
    └── nested2
        ├── plugin1
        │   └── plugin1.jar
        ├── plugin2
        │   └── plugin2.jar
        ├── plugin3
            └── plugin3.jar
{noformat}

I think we should adjust the documentation to match the current behavior",Open,To Do,Task,Major,,Mickael Maison,KAFKA,Kafka,,,connect,,
KAFKA-19400,13620670,2025-06-11T14:00:42.000+0000,2025-07-14T14:38:11.000+0000,,Update AddRaftVoterRPC to support controller auto-joining,"When AddRaftVoterRPCs are sent as part of auto-joining, the active controller should send a response after the new voter set has been appended to only its own log. This allows the auto-joining replica to fetch the new voter set.",Open,To Do,Improvement,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19395,13620494,2025-06-10T04:54:06.000+0000,2025-07-11T08:22:37.000+0000,,The version and license information in the `NOTICE-binary` file for `JUnit` are inconsistent,"[1]: [https://github.com/apache/kafka/blob/3a0a1705a1a7caa9b4b14158b05325138c904451/NOTICE-binary#L163]

[2]:

https://github.com/apache/kafka/blob/3a0a1705a1a7caa9b4b14158b05325138c904451/NOTICE-binary#L251",Open,To Do,Improvement,Minor,Nick Guo,Nick Guo,KAFKA,Kafka,,,,,
KAFKA-19390,13620394,2025-06-09T05:47:05.000+0000,2025-07-15T17:07:04.000+0000,2025-07-08T16:16:09.000+0000,AbstractIndex#resize() does not release old mmap on Linux,"Our kafka broker crashed with the following error:
{code:java}
[2025-03-29 09:37:03,218] ERROR Error while appending records to <topic>-<partition> in dir /kafka-logs/data ...
java.io.IOException: Map failed
at java.base/sun.nio.ch.FileChannelImpl.mapInternal(FileChannelImpl.java:1127)
at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1032)
at org.apache.kafka.storage.internals.log.AbstractIndex.createMappedBuffer(AbstractIndex.java:467)
at org.apache.kafka.storage.internals.log.AbstractIndex.createAndAssignMmap(AbstractIndex.java:105)
at org.apache.kafka.storage.internals.log.AbstractIndex.<init>(AbstractIndex.java:83)
at org.apache.kafka.storage.internals.log.TimeIndex.<init>(TimeIndex.java:65)
at org.apache.kafka.storage.internals.log.LazyIndex.loadIndex(LazyIndex.java:242)
at org.apache.kafka.storage.internals.log.LazyIndex.get(LazyIndex.java:179)
at org.apache.kafka.storage.internals.log.LogSegment.timeIndex(LogSegment.java:146)
at org.apache.kafka.storage.internals.log.LogSegment.readMaxTimestampAndOffsetSoFar(LogSegment.java:201)
at org.apache.kafka.storage.internals.log.LogSegment.maxTimestampSoFar(LogSegment.java:211)
at org.apache.kafka.storage.internals.log.LogSegment.append(LogSegment.java:262)
at kafka.log.LocalLog.append(LocalLog.scala:417)
...
Caused by: java.lang.OutOfMemoryError: Map failed
at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
at java.base/sun.nio.ch.FileChannelImpl.mapInternal(FileChannelImpl.java:1124)
... 33 more{code}
We found that kafka process hit the vm.max_map_count limit (which was set to 262144) and most of the mapped entries correspond to deleted index files.
{code:java}
> sudo cat /proc/${KAFKA_PID}/maps | grep deleted
7d8c5cc00000-7d8c5d600000 rw-s 00000000 08:11 202854769 /kafka-logs/data/topic1-22/00000000332910579773.timeindex.deleted (deleted)
7d8c5d800000-7d8c5e200000 rw-s 00000000 08:11 202854768 /kafka-logs/data/topic1-22/00000000332910579773.index.deleted (deleted)
7d8c67400000-7d8c67e00000 rw-s 00000000 08:11 202562514 /kafka-logs/data/topic2-116/00000000165968090794.timeindex.deleted (deleted)
7d8c68000000-7d8c68a00000 rw-s 00000000 08:11 202562513 /kafka-logs/data/topic2-116/00000000165968090794.index.deleted (deleted)
7d8c6d400000-7d8c6de00000 rw-s 00000000 08:11 202596518 /kafka-logs/data/topic2-356/00000000168702579081.timeindex.deleted (deleted)
7d8c6e000000-7d8c6ea00000 rw-s 00000000 08:11 202596517 /kafka-logs/data/topic2-356/00000000168702579081.index.deleted (deleted)
7d8c71c00000-7d8c72600000 rw-s 00000000 08:11 202798981 /kafka-logs/data/topic3-433/00000000116740630582.timeindex.deleted (deleted)
7d8c72800000-7d8c73200000 rw-s 00000000 08:11 202798980 /kafka-logs/data/topic3-433/00000000116740630582.index.deleted (deleted)
7d8c77c00000-7d8c78600000 rw-s 00000000 08:11 202754947 /kafka-logs/data/topic3-74/00000000118067749684.timeindex.deleted (deleted)
7d8c78800000-7d8c79200000 rw-s 00000000 08:11 202754946 /kafka-logs/data/topic3-74/00000000118067749684.index.deleted (deleted)
7d8c79400000-7d8c79e00000 rw-s 00000000 08:11 202813710 /kafka-logs/data/topic2-82/00000000162756700035.timeindex.deleted (deleted)
7d8c7a000000-7d8c7aa00000 rw-s 00000000 08:11 202813709 /kafka-logs/data/topic2-82/00000000162756700035.index.deleted (deleted)
7d8c7ac00000-7d8c7b600000 rw-s 00000000 08:11 202596526 /kafka-logs/data/topic2-355/00000000169939763750.timeindex.deleted (deleted)
7d8c7b800000-7d8c7c200000 rw-s 00000000 08:11 202596525 /kafka-logs/data/topic2-355/00000000169939763750.index.deleted (deleted)
7d8c7c400000-7d8c7ce00000 rw-s 00000000 08:11 202562498 /kafka-logs/data/topic2-295/00000000168913981903.timeindex.deleted (deleted)
7d8c7d000000-7d8c7da00000 rw-s 00000000 08:11 202562497 /kafka-logs/data/topic2-295/00000000168913981903.index.deleted (deleted)
7d8c80c00000-7d8c81600000 rw-s 00000000 08:11 202754939 /kafka-logs/data/topic3-13/00000000115588098896.timeindex.deleted (deleted)
7d8c81800000-7d8c82200000 rw-s 00000000 08:11 202754938 /kafka-logs/data/topic3-13/00000000115588098896.index.deleted (deleted)
7d8c83c00000-7d8c84600000 rw-s 00000000 08:11 202798989 /kafka-logs/data/topic3-314/00000000118254254601.timeindex.deleted (deleted)
7d8c84800000-7d8c85200000 rw-s 00000000 08:11 202798988 /kafka-logs/data/topic3-314/00000000118254254601.index.deleted (deleted)
...{code}
In AbstractIndex.resize(), the old memory mapping is explicitly unmapped on windows or z/OS using safeForceUnmap(), but on Linux the unmapping step is skipped.
The same issue was originally reported in KAFKA-7442, but the corresponding pull request was never merged.
We propose that resize() should call safeForceUnmap() on all platforms to prevent stale mappings from lingering.

ref: [https://speakerdeck.com/lycorptech_jp/20250609b]
h2. Test

We tested our patch on the following system environment and found no measurable performance regression.

*Environment*
 * Kafka: 3.8.1
 * Java: OpenJDK 17
 * OS: Rocky Linux 9
 * Broker: 10 brokers for original, 10 brokers for patched
 ** CPU: Intel Xeon Silver 4310 (2 sockets)
 ** Memory: 250 GB
 ** Disk: HDD

*Workloads*
 * Peak throughput per broker:
 ** 6K req/s
 ** 85.2 MB/s

h3. Idle percentage of request handler and network processor threads

original:

!orig_request_handler.png|width=1000,height=222!

!orig_network_processor.png|width=999,height=163!

patched:

!patched_request_handler.png|width=997,height=228!

!patched_network_processor.png|width=1004,height=160!
h3. Resource load

original:

!orig_cpu.png|width=1000,height=227!

!orig_disk_io.png|width=997,height=168!

patched:

!patched_cpu.png|width=999,height=227!

!patched_disk_io.png|width=1002,height=160!",Resolved,Done,Bug,Major,Masahiro Mori,Masahiro Mori,KAFKA,Kafka,,,core,Linux,4.2.0
KAFKA-19377,13620178,2025-06-05T15:20:00.000+0000,2025-07-15T06:19:49.000+0000,,Update /streams/developer-guide/security.html for KIP-1071,"KIP-1071 needs DESCRIBE GROUP / READ GROUP permissions.

to describe required ACLs for streams groups",In Progress,In Progress,Sub-task,Major,Haozhong Ma,Lucas Brutschy,KAFKA,Kafka,,,"documentation,streams",,
KAFKA-19357,13619796,2025-06-01T07:50:22.000+0000,2025-07-15T04:17:09.000+0000,,AsyncConsumer#close hangs during closing because the commitAsync request never completes due to a missing coordinator,"KAFKA-16103 ensures the AsyncConsumer wait the requests sent by commitAsync during the close. However, the `CoordinatorRequestManager` does not send any request to find coordinator during close, and hence the requests sent by `CommitRequestManager are never completed during the close since there is no coordinator",Open,To Do,Bug,Major,Yu Chia Ma,Chia-Ping Tsai,KAFKA,Kafka,,,"clients,consumer",,4.2.0
KAFKA-19354,13619705,2025-05-30T17:59:08.000+0000,2025-07-09T15:16:00.000+0000,,KRaft observer unable to recover after re-bootstrapping to follower,"[Original dev mail thread|https://lists.apache.org/thread/ws3390khsxhdg2b8cnv2mzv8slz5xq7q]

If an observer's FETCH request to the quorum leader experiences a failure/timeout, it is possible that when it re-bootstraps, it will connect to a follower node (random selection). Subsequently, the observer node will continually send FETCH requests to that follower, and in receive a response with a ""partitionError"" errorCode=6 (NOT_LEADER_OR_FOLLOWER), which does not trigger a re-bootstrap.
Thus, the observer will be stuck sending FETCH requests to the follower instead of the leader, halting metadata replication and causing it to fall out of sync.

To recover from this state, re-bootstrapping would need to occur by restarting the affected observer or follower, until it connects to the correct leader.

*Steps to reproduce:*
1. Spin up Kafka cluster with 3 or 5 controllers. (ideally 5 to increase likelihood of bootstrapping to a follower instead of the leader)
2. Enable a network delay on a particular observer broker (e.g. `tc qdisc add dev eth0 root netem delay 2500ms`). I picked 2500ms since default timeout is 2s for `controller.quorum.fetch.timeout.ms`/`controller.quorum.request.timeout.ms`. After a few seconds, disable the network delay (e.g. `tc qdisc del dev eth0 root netem`).
3. The observer node will re-bootstrap, potentially to a follower instead of the leader. If so, the observer will continuously send fetch requests to the follower node, receive `NOT_LEADER_OR_FOLLOWER` in response, and will no longer replicate metadata.

*Debug logs demonstrating this scenario:*
- https://gist.github.com/justin-chen/1f3eee79d9a5066a467818a0b1bc006f
- kraftcontroller-3 (leader), kraftcontroller-4 (follower), kafka-0 (observer)
",Patch Available,In Progress,Bug,Major,Alyssa Huang,Justin Chen,KAFKA,Kafka,,,kraft,,
KAFKA-19332,13619209,2025-05-26T12:05:09.000+0000,2025-07-15T11:13:53.000+0000,,Fix flaky test : testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck and testAlterReadCommittedToReadUncommittedIsolationLevelWithRejectAck,The test has been flaky in AK builds - [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.test=testAlterReadCommittedToReadUncommittedIsolationLevelWithReleaseAck()%5B2%5D],Reopened,To Do,Sub-task,Major,Apoorv Mittal,Shivsundar R,KAFKA,Kafka,,,,,
KAFKA-19306,13618461,2025-05-18T17:54:52.000+0000,2025-07-11T18:00:44.000+0000,,Migrate LogCompactionTester to tool module,as title,Open,To Do,Improvement,Major,Yunchi Pang,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19305,13618460,2025-05-18T16:27:35.000+0000,2025-07-14T18:53:16.000+0000,,Ensure all image classes are immutable,"The collections in ClientQuotaImage, TopicImage, and ScramImage are not wrapped to ensure immutability. In contrast, the collections in other image classes are.",Open,To Do,Improvement,Minor,Chang Chi Hsu,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-19301,13618436,2025-05-18T03:03:13.000+0000,2025-07-15T14:54:44.000+0000,,"Move PartitionListener, AlterPartitionListener, AssignmentState, PartitionState, and PendingPartitionChange to storage module","# As title to move scala class / trait to storage module.
 # Rewrite them in Java.",Open,To Do,Sub-task,Major,Chih-Yuan Chien,PoAn Yang,KAFKA,Kafka,,,,,
KAFKA-19279,13618079,2025-05-14T09:03:13.000+0000,2025-07-14T05:55:03.000+0000,,DefaultMessageFormatter is inaccessible as it's package-private class,"As per [upgrade-notes|
https://github.com/apache/kafka/blob/ec70c4436210b7578777739091e365b03c69946f/docs/upgrade.html#L259],  it has been told as below
{code:java}
kafka.tools.DefaultMessageFormatter{code}
class was removed. 
 
Please use the 
{code:java}
org.apache.kafka.tools.consumer.DefaultMessageFormatter{code}
class instead.
 
 
Now 
{code:java}
org.apache.kafka.tools.consumer.DefaultMessageFormatter{code}
is package-private class which means, we can't directly use it as below
 
{code:java}
private val defaultFormatter: DefaultMessageFormatter = new DefaultMessageFormatter {code}
 
It throws error as 
{code:java}
Symbol DefaultMessageFormatter is inaccessible from this place {code}
Due ot this, we can't use other member variable `printHeaders`. 
 
This is happened when we upgraded kafka-tools from 3.4.1 to 3.9.0 version. 
 
Need help in this regarding. ",Patch Available,In Progress,Bug,Major,,Aniruddha Navare,KAFKA,Kafka,,,tools,,
KAFKA-19276,13617942,2025-05-13T09:19:10.000+0000,2025-07-09T09:43:58.000+0000,,Trigger rebalance when assignment related configurations are updated,"In KIP-1071, there are configurations that affect the assignment, and that can be configured dynamically. Those are:
 * streams.num.standby.replicas
 * streams.assignor.name
 * streams.num.warmup.replicas
 * streams.acceptable.recovery.lag

However, only the first one is currently implemented.

We should implement a generic mechanism to trigger a rebalance on the next heartbeat into a streams group, when any of the above group-level configurations have been changed.

 

Triggering a rebalance would mean setting the `bumpGroupEpoch` flag here: [https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java#L1933]

 

Detecting a group config change would probably happen here:

[https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupConfigManager.java#L58]

 ",In Progress,In Progress,Sub-task,Major,Uladzislau Blok,Lucas Brutschy,KAFKA,Kafka,,,streams,,
KAFKA-19259,13617707,2025-05-09T20:25:34.000+0000,2025-07-10T00:10:14.000+0000,,Async consumer fetch intermittent delays on console consumer,"We noticed that fetching with the kafka-console-consumer.sh tool using the new consumer shows some intermittent delays, that are not seen when running the same with the classic consumer. Note that I disabled auto-commit to isolate the delay, and from a first look seems to come from the fetchBuffer.awaitNonEmpty logic, that alternatively takes almost the full poll timeout (runs ""fast"", then ""slow"", and continues to alternate)

[https://github.com/apache/kafka/blob/0b81d6c7802c1be55dc823ce51729f2c6a6071a7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1808]  

The difference in behaviour between the 2 consumers can be seen with this setup:
 * topic with 6 partitions (I tried with 1 partition first and didn't see the delay, then with 3 and 6 I could see it) 
 * data populated in topic with producer sending generated uuids to the topic in while loop 
 * run console consumer (asycn) no commit:

bin/kafka-console-consumer.sh --topic t1 --bootstrap-server localhost:9092 --consumer-property group.protocol=consumer --group cg1 --consumer-property enable.auto.commit=false
Here we can notice the pattern that looks like batches, and custom logs on the awaitNonEmpty show it take the full poll timeout on alternate poll iterations.

 * run same but for classic consumer (consumer-property group.protocol=classic) -> not such pattern of intermittent delays

Produce continuously (I used this) 
while sleep 1; do echo $(uuidgen); done | bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic t1

This needs more investigation to fully understand if it's indeed something in the fetch path or something else) ",Open,To Do,Bug,Major,Arpit Goyal,Lianet Magrans,KAFKA,Kafka,,,"clients,consumer",consumer-threading-refactor,4.2.0
KAFKA-19254,13617477,2025-05-07T20:15:16.000+0000,2025-07-14T20:28:30.000+0000,2025-07-14T20:28:30.000+0000,Add generic feature level metric,KIP link: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1180%3A+Add+a+generic+feature+level+metric,Resolved,Done,New Feature,Major,Kevin Wu,Kevin Wu,KAFKA,Kafka,,,,,4.2.0
KAFKA-19248,13617319,2025-05-06T17:59:22.000+0000,2025-07-09T12:49:07.000+0000,2025-07-09T12:48:55.000+0000,Plugins Test for Multiversioning in Kafka Connect,Add tests for plugin level isolation for Kafka connect (KIP-891).,Resolved,Done,Improvement,Major,,Snehashis Pal,KAFKA,Kafka,,,connect,,4.1.0
KAFKA-19244,13617187,2025-05-05T23:18:17.000+0000,2025-07-12T16:54:51.000+0000,2025-07-01T05:11:24.000+0000,"Add support for kafka-streams-groups.sh options beyond list/describe group (delete group, offset-related APIs)","Implement the followings:
 * modify offsets: supports (-{-}execute, -{-}dry-run, {{{}-export{}}})

 * 
 ** {{--reset-offsets --input-topics <String: topics> --shift-by Long}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> --to-offset Long}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> -to-latest}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> -to-earliest}}

 * 
 ** {{--reset-offsets --all-input-topics --shift-by Long}}

 * 
 ** {{--reset-offsets --all-input-topics --to-offset Long}}

 * 
 ** {{--reset-offsets --all-input-topics -to-latest}}

 * 
 ** {{--reset-offsets --all-input-topics -to-earliest}}

 * 
 ** {{--reset-offsets --all-input-topics -to-earliest}}

 * 
 ** {{--reset-offsets --from-file String:fileName}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> --by-duration String: duration}}

 * 
 ** {{--reset-offsets --all-input-topics --by-duration String: duration}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> --to-datetime String: datetime}}

 * 
 ** {{--reset-offsets --all-input-topics --to-datetime String: datetime}}

 * 
 ** {{--reset-offsets --input-topics <String: topics> -to-current}}

 * 
 ** {{--reset-offsets --all-input-topics -to-current}}

 * delete offsets:

 * 
 ** {{--delete-offsets --all-input-topics}}

 * 
 ** {{--delete-offsets --input-topics <String: topics>}}

 * 
 ** {{--delete-offsets --from-file String:fileName}}

 * delete internal topics:

 * 
 ** {{--delete --internal-topics}}

 * delete groups:

 * 
 ** {{--delete --group}}",Resolved,Done,Sub-task,Major,Alieh Saeedi,Alieh Saeedi,KAFKA,Kafka,,,,,
KAFKA-19213,13616707,2025-04-29T15:03:08.000+0000,2025-07-15T00:19:52.000+0000,,Kafka java client ignores default properties,"Using Kafka Java client, when creating a kafka consumer/producer using a properties object with defaults the defaults are ignored.

Not sure if this is an intentional design choice or not but it was a surprise to me.

My expectation was if a KafkaConsumer or KafkaProducer accepts a Properties object it should not ignore the defaults in the Properties object.

 

The culprit seems to be in the propsToMap in the Utils class.

It doesn't use the propertyNames method to get all the keys known to the Properties object.
{code:java}
public static Map<String, Object> propsToMap(Properties properties) {
    Map<String, Object> map = new HashMap<>(properties.size());
    for (Map.Entry<Object, Object> entry : properties.entrySet()) {
        if (entry.getKey() instanceof String) {
            String k = (String) entry.getKey();
            map.put(k, properties.get(k));
        } else {
            throw new ConfigException(entry.getKey().toString(), entry.getValue(), ""Key must be a string."");
        }
    }
    return map;
}
{code}
I was using version 7.6.0-ccs of the kafka client, on the trunk branch the implementation seems different but is subject to the same issue:
{code:java}
   /**
     * Convert a properties to map. All keys in properties must be string type. Otherwise, a ConfigException is thrown.
     * @param properties to be converted
     * @return a map including all elements in properties
     */
    public static Map<String, Object> propsToMap(Properties properties) {
        return castToStringObjectMap(properties);
    }   

 /**
     * Cast a map with arbitrary type keys to be keyed on String.
     * @param inputMap A map with unknown type keys
     * @return A map with the same contents as the input map, but with String keys
     * @throws ConfigException if any key is not a String
     */
    public static Map<String, Object> castToStringObjectMap(Map<?, ?> inputMap) {
        Map<String, Object> map = new HashMap<>(inputMap.size());
        for (Map.Entry<?, ?> entry : inputMap.entrySet()) {
            if (entry.getKey() instanceof String) {
                String k = (String) entry.getKey();
                map.put(k, entry.getValue());
            } else {
                throw new ConfigException(String.valueOf(entry.getKey()), entry.getValue(), ""Key must be a string."");
            }
        }
        return map;
    }{code}
 

A possible suggestion for how to fix the behavior:

 
{code:java}
public static Map<String, Object> propsToMap(Properties properties) {
    final Enumeration<?> enumeration = properties.propertyNames();

    Map<String, Object> props = new HashMap<>();
    while (enumeration.hasMoreElements()) {
        Object key = enumeration.nextElement();
        if (key instanceof String) {
            final String keyString = (String) key;
            props.put(keyString,properties.getProperty(keyString));
        } else {
            throw new ConfigException(String.valueOf(key),properties.get(key), ""Key must be a string."");
        }
    }
    return props;
}
 {code}
Provided a basic test case to demonstrate the issue. See file attachment.

 

 

 ",Open,To Do,Bug,Minor,Evanston Zhou,Pas Filip,KAFKA,Kafka,,,clients,,
KAFKA-19191,13616177,2025-04-23T19:59:30.000+0000,2025-07-09T12:45:18.000+0000,2025-04-24T15:02:19.000+0000,Don't load bootstrap.checkpoint files if the cluster is already bootstrapped,"Clusters bootstrapped with a metadata version that is already deprecated throw the following exception:
{code:java}
java.lang.IllegalArgumentException: No MetadataVersion with metadata version 
        ...
	at org.apache.kafka.metadata.bootstrap.BootstrapMetadata.recordToMetadataVersion(BootstrapMetadata.java:109)
	at org.apache.kafka.metadata.bootstrap.BootstrapMetadata.fromRecords(BootstrapMetadata.java:90)
	at org.apache.kafka.metadata.bootstrap.BootstrapDirectory.readFromBinaryFile(BootstrapDirectory.java:91)
	at org.apache.kafka.metadata.bootstrap.BootstrapDirectory.read(BootstrapDirectory.java:72)
	at kafka.server.KafkaRaftServer$.initializeLogDirs(KafkaRaftServer.scala:206)
	at kafka.server.KafkaRaftServer.<init>(KafkaRaftServer.scala:57)
	at kafka.Kafka$.buildServer(Kafka.scala:68)
	at kafka.Kafka$.main(Kafka.scala:78)
	at kafka.Kafka.main(Kafka.scala) {code}",Resolved,Done,Bug,Blocker,Colin McCabe,José Armando García Sancio,KAFKA,Kafka,,,controller,,4.0.1
KAFKA-19184,13615924,2025-04-21T16:27:50.000+0000,2025-07-09T09:23:44.000+0000,2025-07-09T09:22:50.000+0000,Document kraft version upgrade,"Add specific documentation for how to upgrade from kraft version 0 to 1. Needs to include:
 # Configuration changes before the upgrade
 # Upgrade command
 # Configuration changes after the upgrade",Resolved,Done,Sub-task,Major,José Armando García Sancio,José Armando García Sancio,KAFKA,Kafka,,,"docs,kraft",,4.1.0
KAFKA-19132,13615054,2025-04-12T07:32:57.000+0000,2025-07-11T21:42:36.000+0000,,Move FetchSession and related classes to server module,https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/FetchSession.scala,Patch Available,In Progress,Sub-task,Minor,Dmitry Werner,Dmitry Werner,KAFKA,Kafka,,,,,
KAFKA-19106,13614546,2025-04-08T10:10:05.000+0000,2025-07-15T03:51:58.000+0000,,Improve Connect SourceTask commit(),,Open,To Do,Improvement,Minor,Sudesh Wasnik,Sudesh Wasnik,KAFKA,Kafka,,,connect,,
KAFKA-19083,13614146,2025-04-04T03:38:41.000+0000,2025-07-14T08:35:05.000+0000,,Fix flaky MetricsDuringTopicCreationDeletionTest. testMetricsDuringTopicCreateDelete(String).quorum=kraft,"This test flaky on CI
[https://github.com/apache/kafka/actions/runs/14254634419/job/39956582918]

 ",Open,To Do,Test,Minor,,黃竣陽,KAFKA,Kafka,,,,,
KAFKA-19081,13614089,2025-04-03T13:09:43.000+0000,2025-07-09T12:45:51.000+0000,2025-04-24T21:27:32.000+0000,Share session capacity and eviction,"The share session cache currently operates in a similar manner to the fetch session cache. However, the eviction behaviour is not appropriate for share sessions.",Resolved,Done,Sub-task,Major,Andrew Schofield,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-19070,13613834,2025-04-01T14:41:38.000+0000,2025-07-15T03:36:10.000+0000,,Adding task number to the user provided client id (via consumer.override.client.id) to ensure each consumer has a unique client ID to avoid metric registration conflicts.,"If user provides client id via ""{*}consumer.override.client.id{*}"", we simply take this value and override the default client id (which is :: {*}""connector-consumer-"" + taskId{*}). This create all the consumers with the same client id which does not lead to failures but cause issues in metric registration and ultimately emitting wrong metrics values.",Open,To Do,Improvement,Minor,,Pritam Kumar,KAFKA,Kafka,,,connect,improvement,4.2.0
KAFKA-19039,13613040,2025-03-25T10:02:00.000+0000,2025-07-15T03:51:59.000+0000,,The refresh_collaborators.py script messes .asf.yaml,"When running refresh_collaborators.py it's messing the ""protected_branches"" field in .asf.yaml:


{code:java}
@@ -48,4 +47,4 @@ github:
   # Disable legacy branch protections. We have manual rulesets which protect trunk
   # and our release branches. See INFRA-26603
-  protected_branches: ~
+  protected_branches:
{code}",Patch Available,In Progress,Bug,Major,Siyang He,Mickael Maison,KAFKA,Kafka,,,,,
KAFKA-19014,13612455,2025-03-19T23:24:00.000+0000,2025-07-09T12:46:23.000+0000,2025-04-21T17:28:18.000+0000,Potential race condition in remote-log-reader and remote-log-index-cleaner thread,"A race condition between threads below results in MappedByteBuffer to reference to a deleted file and attempts to read the file are potentially resulting in JVM to crash.

 

Chain of events:

*Thread - 1 remote-log-reader*

1/ Fetches the offsetIndex from the indexCache which internally maps the physical offset index file as MappedByteBuffer.

OffsetIndex offsetIndex = indexCache.getIndexEntry(segmentMetadata).offsetIndex(); ([here|https://github.com/apache/kafka/blob/cf7029c0264fd7f7b15c2e98acc874ec8c3403f2/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L1772])

*Thread - 2 index cache thread*

Entry is marked for cleanup i.e physical offset index file is renamed.

*Thread - 3 remote-log-index-cleaner*

Physical offset index file is deleted.

*Thread - 1 remote-log-reader*

Attempts run binary search on the MappedByteBuffer that is mapped to a non-existent file.

long upperBoundOffset = offsetIndex.fetchUpperBoundOffset(startOffsetPosition, fetchSize).map(position -> position.offset).orElse(segmentMetadata.endOffset() + 1); ([here|https://github.com/apache/kafka/blob/3.8/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L1619])

 

Results in JVM fatal error (SIGSEV) with stack trace:

 
{code:java}
Stack: [0x000072ee9112d000,0x000072ee9122d000],  sp=0x000072ee9122b360,  free space=1016k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 6483 c2 java.nio.DirectByteBuffer.getInt(I)I java.base@17.0.14 (28 bytes) @ 0x000072f23d2f12f1 [0x000072f23d2f12a0+0x0000000000000051]
j  org.apache.kafka.storage.internals.log.OffsetIndex.relativeOffset(Ljava/nio/ByteBuffer;I)I+5
j  org.apache.kafka.storage.internals.log.OffsetIndex.parseEntry(Ljava/nio/ByteBuffer;I)Lorg/apache/kafka/storage/internals/log/OffsetPosition;+11
j  org.apache.kafka.storage.internals.log.OffsetIndex.parseEntry(Ljava/nio/ByteBuffer;I)Lorg/apache/kafka/storage/internals/log/IndexEntry;+3
j  org.apache.kafka.storage.internals.log.AbstractIndex.binarySearch(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;Lorg/apache/kafka/storage/internals/log/AbstractIndex$SearchResultType;II)I+30
j  org.apache.kafka.storage.internals.log.AbstractIndex.indexSlotRangeFor(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;Lorg/apache/kafka/storage/internals/log/AbstractIndex$SearchResultType;)I+126
j  org.apache.kafka.storage.internals.log.AbstractIndex.smallestUpperBoundSlotFor(Ljava/nio/ByteBuffer;JLorg/apache/kafka/storage/internals/log/IndexSearchType;)I+8
 {code}
 

 

As per MappedByteBuffer documentation ([here|https://devdocs.io/openjdk~17/java.base/java/nio/mappedbytebuffer]):

All or part of a mapped byte buffer may become inaccessible at any time, for example if the mapped file is truncated. An attempt to access an inaccessible region of a mapped byte buffer will not change the buffer's content and will cause an unspecified exception to be thrown either at the time of the access or at some later time. It is therefore strongly recommended that appropriate precautions be taken to avoid the manipulation of a mapped file by this program, or by a concurrently running program, except to read or write the file's content.",Resolved,Done,Bug,Major,Kamal Chandraprakash,Hasil Sharma,KAFKA,Kafka,,,,tiered-storage,
KAFKA-19012,13612360,2025-03-19T12:53:13.000+0000,2025-07-10T19:47:01.000+0000,,Messages ending up on the wrong topic,"We're experiencing messages very occasionally ending up on a different topic than what they were published to. That is, we publish a message to topicA and consumers of topicB see it and fail to parse it because the message contents are meant for topicA. This has happened for various topics. 

We've begun adding a header with the intended topic (which we get just by reading the topic from the record that we're about to pass to the OSS client) right before we call producer.send, this header shows the correct topic (which also matches up with the message contents itself). Similarly we're able to use this header and compare it to the actual topic to prevent consuming these misrouted messages, but this is still concerning.

Some details:
 - This happens rarely: it happened approximately once per 10 trillion messages for a few months, though there was a period of a week or so where it happened more frequently (once per 1 trillion messages or so)
 - It often happens in a small burst, eg 2 or 3 messages very close in time (but from different hosts) will be misrouted
 - It often but not always coincides with some sort of event in the cluster (a broker restarting or being replaced, network issues causing errors, etc). Also these cluster events happen quite often with no misrouted messages
 - We run many clusters, it has happened for several of them
 - There is no pattern between intended and actual topic, other than the intended topic tends to be higher volume ones (but I'd attribute that to there being more messages published -> more occurrences affecting it rather than it being more likely per-message)
 - It only occurs with clients that are using a non-zero linger
 - Once it happened with two sequential messages, both were intended for topicA but both ended up on topicB, published by the same host (presumably within the same linger batch)
 - Most of our clients are 3.2.3 and it has only affected those, most of our brokers are 3.2.3 but it has also happened with a cluster that's running 3.8.1 (but I suspect a client rather than broker problem because of it never happening with clients that use 0 linger)",In Progress,In Progress,Bug,Blocker,Kirk True,Donny Nadolny,KAFKA,Kafka,,,"clients,producer ",,
KAFKA-18913,13610455,2025-03-03T16:53:31.000+0000,2025-07-15T08:57:08.000+0000,,Consider removing state-updater feature flag,"We did enable the new StateUpdated thread with 3.8 release.

We should consider removing the internal feature flag {{__state.updater.enabled__}}, and drop the old code that restores state stores in the poll loop by the stream thread.

The entry point of the old restoration code is the following: https://github.com/apache/kafka/blob/138c2a211ad90e3966b01b99c4570df50ec48ea2/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L1100 
",In Progress,In Progress,Task,Blocker,Janindu Pathirana,Matthias J. Sax,KAFKA,Kafka,,,streams,,4.2.0
KAFKA-18884,13609990,2025-02-27T10:08:37.000+0000,2025-07-08T14:31:44.000+0000,,Move TransactionMetadata to transaction-coordinator module,as title,In Progress,In Progress,Sub-task,Minor,PoAn Yang,PoAn Yang,KAFKA,Kafka,,,,,
KAFKA-18879,13609963,2025-02-27T06:38:47.000+0000,2025-07-09T12:46:44.000+0000,2025-04-24T10:39:46.000+0000,Formatter for share group specific records in __consumer_offsets,,Resolved,Done,Sub-task,Major,Sushant Mahajan,Sushant Mahajan,KAFKA,Kafka,,,,,
KAFKA-18874,13609910,2025-02-26T16:29:11.000+0000,2025-07-15T03:51:55.000+0000,,KRaft controller does not retry registration if the first attempt times out,"There is a [retry mechanism|https://github.com/apache/kafka/blob/3.9.0/core/src/main/scala/kafka/server/ControllerRegistrationManager.scala#L274] with exponential backoff built-in in KRaft controller registration. The timeout of the first attempt is 5 s for KRaft controllers ([code|https://github.com/apache/kafka/blob/3.9.0/core/src/main/scala/kafka/server/ControllerServer.scala#L448]) which is not configurable.

If for some reason the controller's first registration request times out, the attempt should be retried but in practice this does not happen and the controller is not able to join the quorum. We see the following in the faulty controller's log:
{noformat}
2025-02-21 13:31:46,606 INFO [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=3, incarnationId=mEzjHheAQ_eDWejAFquGiw, zkMigrationReady=true, listeners=[Listener(name='CONTROLPLANE-9090', host='kraft-rollback-kafka-controller-pool-3.kraft-rollback-kafka-kafka-brokers.csm-op-test-kraft-rollback-631e64ac.svc', port=9090, securityProtocol=1)], features=[Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=21)]) (kafka.server.ControllerRegistrationManager) [controller-3-registration-manager-event-handler]
...
2025-02-21 13:31:51,627 ERROR [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] RegistrationResponseHandler: channel manager timed out before sending the request. (kafka.server.ControllerRegistrationManager) [controller-3-to-controller-registration-channel-manager]
2025-02-21 13:31:51,726 INFO [ControllerRegistrationManager id=3 incarnation=mEzjHheAQ_eDWejAFquGiw] maybeSendControllerRegistration: waiting for the previous RPC to complete. (kafka.server.ControllerRegistrationManager) [controller-3-registration-manager-event-handler]
{noformat}
After this we can not see any controller retry in the log.",Open,To Do,Bug,Minor,,Daniel Fonai,KAFKA,Kafka,,,controller,,
KAFKA-18681,13606830,2025-01-30T21:19:08.000+0000,2025-07-11T17:29:06.000+0000,2025-07-11T17:29:06.000+0000,GetReplicaLogInfo request,Create the [GetReplicaLogInfo|https://cwiki.apache.org/confluence/display/KAFKA/KIP-966%3A+Eligible+Leader+Replicas#KIP966:EligibleLeaderReplicas-GetReplicaLogInfoRequest(ComingwithUncleanRecovery)] which is required to implement any unclean-election strategy.,Resolved,Done,Sub-task,Major,Jonah Hooper,Jonah Hooper,KAFKA,Kafka,604800.0,,,Phase-2,
KAFKA-18524,13604923,2025-01-14T20:51:20.000+0000,2025-07-09T12:46:58.000+0000,2025-02-18T16:29:21.000+0000,Fix flaky RemoteIndexCacheTest#testCorrectnessForCacheAndIndexFilesWhenResizeCache,org.opentest4j.AssertionFailedError: Failed to mark evicted cache entry for cleanup after resizing cache. at app//org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:38) at app//org.junit.jupiter.api.Assertions.fail(Assertions.java:138) at app//kafka.log.remote.RemoteIndexCacheTest.verifyEntryIsEvicted$1(RemoteIndexCacheTest.scala:560) at app//kafka.log.remote.RemoteIndexCacheTest.testCorrectnessForCacheAndIndexFilesWhenResizeCache(RemoteIndexCacheTest.scala:625) at java.base@17.0.13/java.lang.reflect.Method.invoke(Method.java:569) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511) at java.base@17.0.13/java.util.ArrayList.forEach(ArrayList.java:1511),Resolved,Done,Bug,Major,Chia-Ping Tsai,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-18404,13603877,2025-01-06T06:50:41.000+0000,2025-07-10T11:36:32.000+0000,2025-01-13T13:57:54.000+0000,Remove partitionMaxBytes from DelayedShareFetch ,,Resolved,Done,Sub-task,Major,Abhinav Dixit,Abhinav Dixit,KAFKA,Kafka,,,,,4.1.0
KAFKA-18379,13603450,2024-12-30T19:20:53.000+0000,2025-07-10T03:33:02.000+0000,,Enforce resigned cannot transition to any other state in same epoch,,Open,To Do,Improvement,Major,TengYao Chi,Alyssa Huang,KAFKA,Kafka,,,,,
KAFKA-18336,13602894,2024-12-20T20:18:02.000+0000,2025-07-15T08:18:45.000+0000,,Improve jmh tests on ACL in AuthorizerBenchmark and StandardAuthorizerUpdateBenchmark,"Now AuthorizerBenchmark benchmarks don't return result and result doesn't wrap in Blackhole.

Compiler can eliminate returned value and benchmark would be incorrect

 

StandardAuthorizerUpdateBenchmark has a big error
For example
Benchmark                                                                            (aclCount)  Mode  Cnt  Score   Error  Units
StandardAuthorizerUpdateBenchmark.testAddAcl       25000  avgt   10  5.425 ± 1.723  ms/op",Patch Available,In Progress,Test,Minor,,Evgeny Kuvardin,KAFKA,Kafka,,,system tests,,
KAFKA-18297,13602519,2024-12-18T06:46:45.000+0000,2025-07-09T12:47:09.000+0000,2025-02-13T23:34:10.000+0000,Fix flaky PlaintextAdminIntegrationTest.testConsumerGroups,,Resolved,Done,Bug,Major,Yu-Lin Chen,Chia-Ping Tsai,KAFKA,Kafka,,,"clients,consumer","flaky-test,integration-test,kip-848-client-support",
KAFKA-18288,13602478,2024-12-17T18:12:32.000+0000,2025-07-08T13:29:00.000+0000,2025-04-30T06:30:49.000+0000,Add support kafka-streams-groups.sh --describe,"Implement --describe and its options: (--state, --offset, --members and the combination of them with --verbose)

 

This is implemented in the kip1071 feature branch already and needs to be ported to trunk.",Resolved,Done,Sub-task,Major,Alieh Saeedi,Lucas Brutschy,KAFKA,Kafka,,,streams,,
KAFKA-18220,13601836,2024-12-12T13:19:01.000+0000,2025-07-15T10:58:06.000+0000,,Refactor AsyncConsumerMetrics so they are appropriate for share consumers,This is a follow-on from [https://github.com/apache/kafka/pull/17199|https://github.com/apache/kafka/pull/17199].,Open,To Do,Sub-task,Major,Shivsundar R,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-18201,13601607,2024-12-10T20:40:14.000+0000,2025-07-14T03:54:13.000+0000,,testGroupMetadataMessageFormatter fails for new consumer protocol,"This fails when the new protocol/consumer are used, with java.lang.NullPointerException: Cannot invoke ""com.fasterxml.jackson.databind.JsonNode.get(String)"" because ""keyNode"" is null

Not sure if it's a bug that needs fixing or if the test needs to be updated to successfully run with group.protocol=CONSUMER",In Progress,In Progress,Bug,Major,PoAn Yang,Lianet Magrans,KAFKA,Kafka,,,,,
KAFKA-18157,13600890,2024-12-04T14:50:27.000+0000,2025-07-08T07:13:27.000+0000,,Consider UnsupportedVersionException child class to represent the case of unsupported fields,"[https://github.com/apache/kafka/pull/17989#discussion_r1869333289] 

 

Adding more pointers, I expect that when the Unsupported exceptions is due to fields validation it comes from building the request on [https://github.com/apache/kafka/blob/f60382bf21601b1c6708d094fadb13de59a77278/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java#L582] , vs when it happens because the API is not supported, it should be detected when checking the usable version on [https://github.com/apache/kafka/blob/81447c7c9543d56e2fce76b861d3918d4cac6a2a/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java#L155] ",Open,To Do,Improvement,Minor,TengYao Chi,Lianet Magrans,KAFKA,Kafka,,,clients,,
KAFKA-18120,13600375,2024-11-29T09:20:28.000+0000,2025-07-09T12:49:39.000+0000,2025-07-09T12:49:27.000+0000,KIP-891: Support for multiple versions of connect plugins.,"Jira to track implementation of KIP-891 [KIP-891: Running multiple versions of Connector plugins - Apache Kafka - Apache Software Foundation|https://cwiki.apache.org/confluence/display/KAFKA/KIP-891%3A+Running+multiple+versions+of+Connector+plugins]

 ",Resolved,Done,New Feature,Major,Snehashis Pal,Snehashis Pal,KAFKA,Kafka,,,connect,,4.1.0
KAFKA-18105,13600222,2024-11-27T19:02:24.000+0000,2025-07-13T09:34:01.000+0000,,Fix flaky PlaintextAdminIntegrationTest#testElectPreferredLeaders,org.opentest4j.AssertionFailedError: Timed out waiting for leader to become Some(0). Last metadata lookup returned leader = Some(1) ==> expected: <true> but was: <false>,Open,To Do,Bug,Major,Chang Chi Hsu,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-17853,13596351,2024-10-22T16:04:59.000+0000,2025-07-15T13:10:56.000+0000,,Console share consumer is not terminated immediately,"Even when share groups is not enabled , if pressed ctrl+c, then client doesn't immediately shutsdown.

 
{code:java}
./bin/kafka-console-share-consumer.sh --bootstrap-server localhost:9092 --topic T1 --group SG1 --consumer-property client.id=share-consumer-2
[2024-10-22 17:01:07,958] ERROR [ShareConsumer clientId=share-consumer-2, groupId=SG1] ShareGroupHeartbeatRequest failed due to fatal error: The node does not support SHARE_GROUP_HEARTBEAT (org.apache.kafka.clients.consumer.internals.ShareHeartbeatRequestManager)
[2024-10-22 17:01:07,958] ERROR [ShareConsumer clientId=share-consumer-2, groupId=SG1] Member <no ID> with epoch 0 transitioned to fatal state (org.apache.kafka.clients.consumer.internals.ShareMembershipManager)
[2024-10-22 17:01:12,763] ERROR Error processing message, terminating consumer process:  (org.apache.kafka.tools.consumer.ConsoleShareConsumer)
org.apache.kafka.common.errors.UnsupportedVersionException: The node does not support SHARE_GROUP_HEARTBEAT
^C^C

Processed a total of 0 messages
  {code}",Reopened,To Do,Sub-task,Major,Shivsundar R,Apoorv Mittal,KAFKA,Kafka,,,,,
KAFKA-17715,13594561,2024-10-07T23:13:14.000+0000,2025-07-09T09:08:01.000+0000,,Remove `force_use_zk_connection` from e2e,"Since no E2E tests are using the argument, we should remove it.",Open,To Do,Bug,Major,Ming-Yen Chung,Chia-Ping Tsai,KAFKA,Kafka,,,,,
KAFKA-17601,13593099,2024-09-24T14:05:30.000+0000,2025-07-10T00:34:32.000+0000,,Inter-broker connections do not expose their clientSoftwareName and clientSoftwareVersion tags,"[KIP-511|[https://cwiki.apache.org/confluence/display/KAFKA/KIP-511%3A+Collect+and+Expose+Client%27s+Name+and+Version+in+the+Brokers]] made it possible to see what library versions are used by the Kafka clients.

When Kafka brokers are connecting to other brokers this information is not properly populated, we see the ""unknown"" value instead for both `ClientSoftwareName` and `ClientSoftwareVersion`.",In Progress,In Progress,Bug,Minor,PoAn Yang,Tamas Kornai,KAFKA,Kafka,,,,,
KAFKA-17444,13590512,2024-08-30T02:33:44.000+0000,2025-07-13T03:52:12.000+0000,,High memory allocation rate for FetchSession.update ,"!image-2024-08-30-10-34-33-011.png|width=757,height=241!

when single node 7w fetch qps, 1000+ consumer fetch connection the touch compare logic will consume a lot memory allocate 8%.",Open,To Do,Improvement,Major,,jinlong wang,KAFKA,Kafka,,,core,,
KAFKA-17019,13583476,2024-06-21T18:54:17.000+0000,2025-07-15T13:14:29.000+0000,,Producer TimeoutException should include root cause,"With KAFKA-16965 we added a ""root cause"" to some `TimeoutException` thrown by the producer. However, it's only a partial solution to address a specific issue.

We should consider to add the ""root cause"" for _all_ `TimeoutException` cases and unify/cleanup the code to get an holistic solution to the problem.",Open,To Do,Improvement,Major,sanghyeok An,Matthias J. Sax,KAFKA,Kafka,,,"clients,producer ",,
KAFKA-17014,13583405,2024-06-21T05:52:52.000+0000,2025-07-13T03:52:19.000+0000,,ScramFormatter should not use String for password.,"Since String is immutable, there are no easy ways to erase a String password after use.  It is a security concern so we should not use String for passwords.  See also  https://stackoverflow.com/questions/8881291/why-is-char-preferred-over-string-for-passwords",Open,To Do,Improvement,Major,Mingdao Yang,Tsz-wo Sze,KAFKA,Kafka,,,security,,
KAFKA-16869,13581229,2024-05-31T23:50:10.000+0000,2025-07-13T14:45:13.000+0000,,Rewrite HighAvailabilityTaskAssignor to implement the new TaskAssignor interface,"We need to add a new HighAvailabilityTaskAssignor that implements the new TaskAssignor interface. Once we have that, we need to remember to also make these related changes:
 # Change the StreamsConfig.TASK_ASSIGNOR_CLASS_CONFIG default from null to the new HAAssignor
 # Check for this new HAAssignor type when evaluating the OptionalInt rack-aware assignment configs in the public AssignmentConfigs class. If these configs are Optional.empty()  and the new HAAssignor is used, they should be overridden to the HAAssignor-specific default values. This code already exists but should be updated to check for the new HAAssignor class name instead of  ""null"" 
 # Until the old HAAssignor and old internal task assignor config can be removed completely, make sure the new HAAssignor is used by default when a TaskAssignor is selected in StreamsPartitionAssignor",Open,To Do,Sub-task,Major,,A. Sophie Blee-Goldman,KAFKA,Kafka,,,streams,,
KAFKA-16768,13579313,2024-05-15T01:50:53.000+0000,2025-07-15T14:06:09.000+0000,,SocketServer leaks accepted SocketChannel instances due to race condition,"The SocketServer has threads for Acceptors and Processors. These threads communicate via Processor#accept/Processor#configureNewConnections and the `newConnections` queue.

During shutdown, the Acceptor and Processors are each stopped by setting shouldRun to false, and then shutdown proceeds asynchronously in all instances together. This leads to a race condition where an Acceptor accepts a SocketChannel and queues it to a Processor, but that Processor instance has already started shutting down and has already drained the newConnections queue.

KAFKA-16765 is an analogous bug in NioEchoServer, which uses a completely different implementation but has the same flaw.

An example execution order that includes this leak:
1. Acceptor#accept() is called, and a new SocketChannel is accepted.
2. Acceptor#assignNewConnection() begins
3. Acceptor#close() is called, which sets shouldRun to false in the Acceptor and attached Processor instances
4. Processor#run() checks the shouldRun variable, and exits the loop
5. Processor#closeAll() executes, and drains the `newConnections` variable
6. Processor#run() returns and the Processor thread terminates
7. Acceptor#assignNewConnection() calls Processor#accept(), which adds the SocketChannel to `newConnections`
8. Acceptor#assignNewConnection() returns
9. Acceptor#run() checks the shouldRun variable and exits the loop, and the Acceptor thread terminates.
10. Acceptor#close() joins all of the terminated threads, and returns

At the end of this sequence, there are still open SocketChannel instances in newConnections, which are then considered leaked.",Open,To Do,Bug,Major,Chang-Yu Huang,Greg Harris,KAFKA,Kafka,,,core,newbie,
KAFKA-16717,13579066,2024-05-13T12:17:08.000+0000,2025-07-15T17:23:09.000+0000,,Add AdminClient.alterShareGroupOffsets,,In Progress,In Progress,Sub-task,Major,Jimmy Wang,Andrew Schofield,KAFKA,Kafka,,,,,
KAFKA-16143,13564776,2024-01-15T21:32:05.000+0000,2025-07-10T00:11:55.000+0000,2024-12-13T12:21:29.000+0000,New JMX metrics for AsyncKafkaConsumer,This task is to consider what _new_ metrics we need from the KIP-848 protocol that aren't already exposed by the current set of metrics. This will require a KIP to introduce the new metrics.,Resolved,Done,Improvement,Major,PoAn Yang,Kirk True,KAFKA,Kafka,,,"clients,consumer,metrics","consumer-threading-refactor,kip-848-client-support,metrics,needs-kip",4.0.0
KAFKA-16061,13563131,2023-12-29T01:57:33.000+0000,2025-07-08T18:00:45.000+0000,2025-07-08T17:59:01.000+0000,KRaft JBOD follow-ups and improvements,,Resolved,Done,Improvement,Major,Igor Soarez,Colin McCabe,KAFKA,Kafka,,,,,3.7.0
KAFKA-15955,13560241,2023-12-01T15:47:55.000+0000,2025-07-08T17:56:37.000+0000,2025-07-08T17:56:37.000+0000,Migrating ZK brokers send dir assignments,"Broker in ZooKeeper mode, while in migration mode, should start sending directory assignments to the KRaft Controller using AssignmentsManager.",Resolved,Done,Sub-task,Minor,Proven Provenzano,Igor Soarez,KAFKA,Kafka,,,,,
KAFKA-15910,13559637,2023-11-28T02:43:30.000+0000,2025-07-15T14:30:33.000+0000,2023-12-06T16:38:34.000+0000,New group coordinator needs to generate snapshots while loading,"After the new coordinator loads a __consumer_offsets partition, it logs the following exception when making a read operation (fetch/list groups, etc):

 
{{{}java.lang.RuntimeException: No in-memory snapshot for epoch 740745. Snapshot epochs are:{}}}{{{}at org.apache.kafka.timeline.SnapshotRegistry.getSnapshot(SnapshotRegistry.java:178){}}}{{{}at org.apache.kafka.timeline.SnapshottableHashTable.snapshottableIterator(SnapshottableHashTable.java:407){}}}{{{}at org.apache.kafka.timeline.TimelineHashMap$ValueIterator.<init>(TimelineHashMap.java:283){}}}{{{}at org.apache.kafka.timeline.TimelineHashMap$Values.iterator(TimelineHashMap.java:271){}}}
{{...}}
 
This happens because we don't have a snapshot at the last updated high watermark after loading. We cannot generate a snapshot at the high watermark after loading all batches because it may contain records that have not yet been committed. We also don't know where the high watermark will advance up to so we need to generate a snapshot for each offset the loader observes to be greater than the current high watermark. Then once we add the high watermark listener and update the high watermark we can delete all of the snapshots prior. ",Resolved,Done,Sub-task,Major,Jeff Kim,Jeff Kim,KAFKA,Kafka,,,,,3.7.0
KAFKA-15853,13558524,2023-11-18T22:36:01.000+0000,2025-07-14T03:54:22.000+0000,,Move KafkaConfig to server module,"The server module is a Java-only module, so this also requires converting from Scala to Java.",In Progress,In Progress,Sub-task,Major,PoAn Yang,Ismael Juma,KAFKA,Kafka,,,,,
KAFKA-15709,13555956,2023-10-29T22:37:09.000+0000,2025-07-14T08:12:12.000+0000,,KRaft support in ServerStartupTest,"The following tests in ServerStartupTest in core/src/test/scala/unit/kafka/server/ServerStartupTest.scala need to be updated to support KRaft

38 : def testBrokerCreatesZKChroot(): Unit = {

51 : def testConflictBrokerStartupWithSamePort(): Unit = {

65 : def testConflictBrokerRegistration(): Unit = {

82 : def testBrokerSelfAware(): Unit = {

93 : def testBrokerStateRunningAfterZK(): Unit = {

Scanned 107 lines. Found 0 KRaft tests out of 5 tests",Open,To Do,Task,Minor,,Sameer Tejani,KAFKA,Kafka,,,core,"kraft,kraft-test,newbie",
KAFKA-15615,13554316,2023-10-16T22:29:34.000+0000,2025-07-10T03:48:38.000+0000,,Improve handling of fetching during metadata updates,"[During a review of the new fetcher|https://github.com/apache/kafka/pull/14406#discussion_r1333393941], [~junrao] found what appears to be an opportunity for optimization.

When a fetch response receives an error about partition leadership, fencing, etc. a metadata refresh is triggered. However, it takes time for that refresh to occur, and in the interim, it appears that the consumer will blindly attempt to fetch data for the partition again, in kind of a ""definition of insanity"" type of way. Ideally, the consumer would have a way to temporarily ignore those partitions, in a way somewhat like the ""pausing"" approach so that they are skipped until the metadata refresh response is fully processed.

This affects both the existing KafkaConsumer and the new PrototypeAsyncConsumer.",Patch Available,In Progress,Improvement,Major,appchemist,Kirk True,KAFKA,Kafka,,,"clients,consumer","consumer-threading-refactor,fetcher",
KAFKA-15367,13547591,2023-08-16T15:12:57.000+0000,2025-07-08T17:58:34.000+0000,,Write a test verifying a non-JBOD to JBOD transition in a cluster,A cluster running in KRaft without JBOD should be able to transition into JBOD mode without issues,Open,To Do,Test,Major,,Igor Soarez,KAFKA,Kafka,,,,,
KAFKA-15331,13546849,2023-08-10T15:47:41.000+0000,2025-07-14T16:04:05.000+0000,,Handle remote log enabled topic deletion when leader is not available,"When a topic gets deleted, then there can be a case where all the replicas can be out of ISR. This case is not handled, See: [https://github.com/apache/kafka/pull/13947#discussion_r1289331347] for more details.",In Progress,In Progress,Bug,Major,ally heev,Kamal Chandraprakash,KAFKA,Kafka,,,,,
KAFKA-14915,13532858,2023-04-17T10:05:19.000+0000,2025-07-15T06:07:21.000+0000,2025-07-14T14:14:38.000+0000,Option to consume multiple partitions that have their data in remote storage for the target offsets.,Context: https://github.com/apache/kafka/pull/13535#discussion_r1171250580,Resolved,Done,Improvement,Major,Luke Chen,Satish Duggana,KAFKA,Kafka,,,,tiered-storage,4.2.0
KAFKA-14830,13529501,2023-03-21T19:34:44.000+0000,2025-07-09T14:17:12.000+0000,,Illegal state error in transactional producer,"We have seen the following illegal state error in the producer:
{code:java}
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for topic-0:120027 ms has passed since batch creation
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for topic-1:120026 ms has passed since batch creation
[Producer clientId=client-id2, transactionalId=transactional-id] Aborting incomplete transaction
[Producer clientId=client-id2, transactionalId=transactional-id] Invoking InitProducerId with current producer ID and epoch ProducerIdAndEpoch(producerId=191799, epoch=0) in order to bump the epoch
[Producer clientId=client-id2, transactionalId=transactional-id] ProducerId set to 191799 with epoch 1
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.NetworkException: Disconnected from node 4
[Producer clientId=client-id2, transactionalId=transactional-id] Transiting to abortable error state due to org.apache.kafka.common.errors.TimeoutException: The request timed out.
[Producer clientId=client-id2, transactionalId=transactional-id] Uncaught error in request completion:
java.lang.IllegalStateException: TransactionalId transactional-id: Invalid transition attempted from state READY to state ABORTABLE_ERROR
        at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:1089)
        at org.apache.kafka.clients.producer.internals.TransactionManager.transitionToAbortableError(TransactionManager.java:508)
        at org.apache.kafka.clients.producer.internals.TransactionManager.maybeTransitionToErrorState(TransactionManager.java:734)
        at org.apache.kafka.clients.producer.internals.TransactionManager.handleFailedBatch(TransactionManager.java:739)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:753)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:743)
        at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:695)
        at org.apache.kafka.clients.producer.internals.Sender.completeBatch(Sender.java:634)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$null$1(Sender.java:575)
        at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$handleProduceResponse$2(Sender.java:562)
        at java.base/java.lang.Iterable.forEach(Iterable.java:75)
        at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse(Sender.java:562)
        at org.apache.kafka.clients.producer.internals.Sender.lambda$sendProduceRequest$5(Sender.java:836)
        at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)
        at org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:583)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:575)
        at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:328)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:243)
        at java.base/java.lang.Thread.run(Thread.java:829)
 {code}
The producer hits timeouts which cause it to abort an active transaction. After aborting, the producer bumps its epoch, which transitions it back to the `READY` state. Following this, there are two errors for inflight requests, which cause an illegal state transition to `ABORTABLE_ERROR`. But how could the transaction ABORT complete if there were still inflight requests? ",In Progress,In Progress,Bug,Critical,Kirk True,Jason Gustafson,KAFKA,Kafka,,,"clients,producer ",transactions,4.2.0
KAFKA-14719,13524779,2023-02-15T07:03:02.000+0000,2025-07-08T08:04:50.000+0000,,Fix a return code when broker bootstrapped just now.,"I'm not sure whether it is a problem.

 

I write record to kafka when I use cppkafka lib( [https://github.com/mfontanini/cppkafka.git] )

which is wrapped by librdkafka( [https://github.com/confluentinc/librdkafka.git] ).

 

I start a fuzzy test, which keep restart kafka cluster and keeping continuously 'write requests'.

 

I find a return code may be not good, but I'm not sure about that.

I think providing some codes may help, someone can help me review it.   

 https://github.com/apache/kafka/pull/13254",Open,To Do,Task,Major,,Yuqi Du,KAFKA,Kafka,,,,,
KAFKA-14560,13516342,2023-01-03T15:32:49.000+0000,2025-07-10T13:49:22.000+0000,2025-02-24T04:29:59.000+0000,Remove old client protocol API versions in Kafka 4.0 (KIP-896),"Please see KIP for details:

https://cwiki.apache.org/confluence/display/KAFKA/KIP-896%3A+Remove+old+client+protocol+API+versions+in+Kafka+4.0",Resolved,Done,Improvement,Blocker,Ismael Juma,Ismael Juma,KAFKA,Kafka,,,,,4.0.0
KAFKA-14410,13504776,2022-11-21T08:45:02.000+0000,2025-07-09T12:47:20.000+0000,2024-12-11T10:01:19.000+0000,Allow connect runtime to run multiple versions of a connector. ,Connect Runtime should support running multiple versions of the same connector. Please refer to [KIP-891|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=235834793] for more information on the problem and the proposed solution. ,Resolved,Done,Improvement,Major,Snehashis Pal,Snehashis Pal,KAFKA,Kafka,,,connect,,
KAFKA-14405,13503713,2022-11-19T01:24:27.000+0000,2025-07-14T01:27:44.000+0000,,Log a warning when users attempt to set a config controlled by Streams,"Related to https://issues.apache.org/jira/browse/KAFKA-14404

It's too easy for users to try overriding one of the client configs that Streams hardcodes, and since we just silently ignore it there's no good way for them to tell their config is not being used. Sometimes this may be harmless but in cases like the Producer's partitioner, there could be important application logic that's never being invoked.

When processing user configs in StreamsConfig, we should check for all these configs and log a warning when any of them have been set",Open,To Do,Bug,Major,,A. Sophie Blee-Goldman,KAFKA,Kafka,,,streams,newbie,
KAFKA-13965,13448889,2022-06-08T01:17:21.000+0000,2025-07-10T03:32:42.000+0000,,Document broker-side socket-server-metrics,"There are a bunch of broker JMX metrics in the ""socket-server-metrics"" space that are not documented on kafka.apache.org/documentation

 
 * {_}MBean{_}: kafka.server:{{{}type=socket-server-metrics,listener=<listenerName>,networkProcessor=<processorIndex>{}}}
 ** From KIP-188: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-188+-+Add+new+metrics+to+support+health+checks]
 *  kafka.server:type=socket-server-metrics,name=connection-accept-rate,listener=\{listenerName}
 ** From KIP-612: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-612%3A+Ability+to+Limit+Connection+Creation+Rate+on+Brokers]

It would be helpful to get all the socket-server-metrics documented

 ",Open,To Do,Improvement,Major,Ksolves India Limited,James Cheng,KAFKA,Kafka,,,documentation,"newbie,newbie++",
KAFKA-13555,13418323,2021-12-18T00:33:45.000+0000,2025-07-13T14:44:45.000+0000,,Consider number if input topic partitions for task assignment,"StreamsAssignor tries to distribute tasks evenly across all instances/threads of a Kafka Streams application. It knows about instances/thread (to give more capacity to instances with more thread), and it distinguishes between stateless and stateful tasks. We also try to not move state around but to use a sticky assignment if possible. However, the assignment does not take the number of input topic partitions into account.

For example, an upstream tasks could compute two joins, and thus has 3 input partitions, while a downstream task compute a follow up aggregation with a single input partitions (from the repartition topic). It could happen that one thread gets the 3 input partition tasks assigned, while the other thread get the single input partition tasks assigned resulting to an uneven partition assignment across both threads.",Open,To Do,Improvement,Major,,Matthias J. Sax,KAFKA,Kafka,,,streams,,
KAFKA-12512,13366377,2021-03-19T14:46:00.000+0000,2025-07-11T07:55:44.000+0000,,Pass client listener and security protocol to KafkaClusterTestKit,"When using our JUnit ClusterTestExtensions in KRaft mode, we do not pass the client security protocol or listener down to the underlying KRaft cluster. We should fix this",Open,To Do,Task,Minor,,David Arthur,KAFKA,Kafka,,,,,
KAFKA-12380,13361284,2021-02-26T22:41:58.000+0000,2025-07-15T05:43:32.000+0000,2022-04-29T05:37:28.000+0000,Executor in Connect's Worker is not shut down when the worker is,"The `Worker` class has an [`executor` field|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L100] that the public constructor initializes with a new cached thread pool ([https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127|https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127].]).

When the worker is stopped, it does not shutdown this executor. This is normally okay in the Connect runtime and MirrorMaker 2 runtimes, because the worker is stopped only when the JVM is stopped (via the shutdown hook in the herders).

However, we instantiate and stop the herder many times in our integration tests, and this means we're not necessarily shutting down the herder's executor. Normally this won't hurt, as long as all of the runnables that the executor threads run actually do terminate. But it's possible those threads *might* not terminate in all tests. TBH, I don't know that such cases actually exist.

 ",Resolved,Done,Bug,Minor,Rajani Karuturi,Randall Hauch,KAFKA,Kafka,,,connect,newbie,3.3.0
KAFKA-12281,13356487,2021-02-03T12:01:19.000+0000,2025-07-15T05:54:35.000+0000,,Deprecate org.apache.kafka.streams.errors.BrokerNotFoundException,"It's been 3 years since 234ec8a gets rid of usage of BrokerNotFoundException. Hence, it is time to deprecate BrokerNotFoundException.
",In Progress,In Progress,Improvement,Minor,Rajani Karuturi,Chia-Ping Tsai,KAFKA,Kafka,,,streams,"beginner,needs-kip,newbie",
KAFKA-12243,13354758,2021-01-26T20:30:55.000+0000,2025-07-11T07:48:22.000+0000,,Add toString methods to some of the classes introduced by this Epic,,Open,To Do,Sub-task,Major,,José Armando García Sancio,KAFKA,Kafka,,,,,
KAFKA-10844,13345496,2020-12-11T11:20:21.000+0000,2025-07-09T15:36:12.000+0000,,groupBy without shuffling,"The idea is to give a way to keep the current partitioning while doing a groupBy.

Our use-case is the following:
We process device data (stream is partitioned by device-id), each device produces several metrics. We want to aggregate by metric, so currently we do a
{code:java}
 selectKey( ... => (device, metric)).groupByKey.windowedBy(...).aggregate(...)  {code}
This shuffles the data around, but it's not necessary, each (device, metric) group could stay in the original partition.

This is not only an optimization question. We are experiencing invalid aggregations when reprocessing history. In these reprocessing, we frequently see some tasks moving faster on some partitions. This causes problems with event-time: Lets' say data for device d1 is in partition p1 and stream-time t1, and device d2 / partition p2 / time t2.
Now, if I re-key by (device, metric), records from both devices could have the same hash-key and land in the same partition. And if t2 is far ahead of t1, then all time-windows for t1 get expired at once.

Maybe I miss some way of doing this with the existing API, please let me know. Currently, I manually repartition and specify a custom partitioner, but it's tedious.

If I were to rewrite the aggregations manually with Transformer API, I would use (device, key) for my state store key, without changing the record key.

 

_(poke_ [~vvcephei] _following our discussion on users ml)_

KIP-759: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-759%3A+Unneeded+repartition+canceling] ",Patch Available,In Progress,Improvement,Major,João Pedro Fonseca,Mathieu DESPRIEE,KAFKA,Kafka,,,streams,kip,
KAFKA-10825,13344901,2020-12-08T19:13:13.000+0000,2025-07-15T09:21:26.000+0000,,Consolidate code between ZK and AlterISR for ISR updates,"It would be nice to consolidate the two code paths that are used for ISR updates in Partition.scala. If we can totally (or mostly) abstract away the ZK code, it will simplify some of the KIP-500 work.",In Progress,In Progress,Task,Minor,David Arthur,David Arthur,KAFKA,Kafka,,,,,
KAFKA-10623,13336542,2020-10-21T18:39:33.000+0000,2025-07-10T09:36:06.000+0000,,[Easy] Refactor code to avoid discovery conflicts for classes:{Supported|Finalized}VersionRange,"This Jira suggests changing few existing class names to avoid class discovery conflicts. Particularly the following classes:
{code:java}
org.apache.kafka.clients.admin.{Supported|Finalized}VersionRange{code}
conflict with

 

 
{code:java}
org.apache.kafka.common.feature.{Supported|Finalized}VersionRange{code}
The former is internal facing, while the latter is external facing (since it is used in the Admin#describeFeatures API). So, the internal facing classes can be renamed suitably. Possible alternative naming suggestions:

 

 
{code:java}
org.apache.kafka.clients.admin.{Supported|Finalized}Versions
{code}
{code:java}
org.apache.kafka.clients.admin.Broker{Supported|Finalized}Versions
{code}
{code:java}
org.apache.kafka.clients.admin.Broker{Supported|Finalized}VersionRange{code}
 ",Open,To Do,Sub-task,Minor,Ksolves India Limited,Kowshik Prakasam,KAFKA,Kafka,,,,,
KAFKA-10409,13323050,2020-08-17T12:42:20.000+0000,2025-07-11T16:18:24.000+0000,,Refactor Kafka Streams RocksDb iterators ,"From [https://github.com/apache/kafka/pull/9137#discussion_r470345513] :

[~ableegoldman] : 

> Kind of unrelated, but WDYT about renaming {{RocksDBDualCFIterator}} to {{RocksDBDualCFAllIterator}} or something on the side? I feel like these iterators could be cleaned up a bit in general to be more understandable – for example, it's weird that we do the {{iterator#seek}}-ing in the actual {{all()}} method but for range queries we do the seeking inside the iterator constructor.

and [https://github.com/apache/kafka/pull/9137#discussion_r470361726] :

> Personally I found the {{RocksDBDualCFIterator}} logic a bit difficult to follow even before the reverse iteration, so it would be nice to have some tests specifically covering reverse iterators over multi-column-family timestamped stores",Patch Available,In Progress,Improvement,Minor,João Pedro Fonseca,Jorge Esteban Quilcate Otoya,KAFKA,Kafka,,,streams,newbie,
KAFKA-9965,13303232,2020-05-06T23:44:49.000+0000,2025-07-15T13:47:04.000+0000,2024-12-27T14:28:12.000+0000,Uneven distribution with RoundRobinPartitioner in AK 2.4+,"{{RoundRobinPartitioner}} states that it will provide equal distribution of records across partitions. However with the enhancements made in KIP-480, it may not. In some cases, when a new batch is started, the partitioner may be called a second time for the same record:

[https://github.com/apache/kafka/blob/2.4/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L909]

[https://github.com/apache/kafka/blob/2.4/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L934]

Each time the partitioner is called, it increments a counter in {{RoundRobinPartitioner}}, so this can result in unequal distribution.

Easiest fix might be to decrement the counter in {{RoundRobinPartitioner#onNewBatch}}.

 ",Resolved,Done,Bug,Major,,Michael Bingham,KAFKA,Kafka,,,producer ,,
KAFKA-7442,13187518,2018-09-26T07:06:29.000+0000,2025-07-08T16:18:59.000+0000,2025-07-08T16:17:33.000+0000,forceUnmap mmap on linux when index resize,"when resize OffsetIndex or TimeIndex,We should force unmap mmap for linux platform. Rather than waiting mixedgc or  fullgc to unmap MappedByteBuffer objects


##before full gc
{code}
{""request"":{""mbean"":""java.nio:name=mapped,type=BufferPool"",""type"":""read""},""value"":{""TotalCapacity"":2434496968,""MemoryUsed"":2434496968,""Count"":5392,""Name"":""mapped"",""ObjectName"":{""objectName"":""java.nio:name=mapped,type=BufferPool""}},""timestamp"":1537945759,""status"":200}


S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
0.00 100.00  28.88   4.93  97.64  94.72     24    0.176     0    0.000    0.176
0.00 100.00  31.37   4.93  97.64  94.72     24    0.176     0    0.000    0.176
{code}

{code}
jmap -histo:live kafka_pid
{code}
 
###after full gc
{code}
S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT   
0.00   0.00  23.22   5.03  97.92  94.93     24    0.176     1    0.617    0.793
0.00   0.00  25.70   5.03  97.92  94.93     24    0.176     1    0.617    0.793
0.00   0.00  27.86   5.03  97.92  94.93     24    0.176     1    0.617    0.793

{""request"":{""mbean"":""java.nio:name=mapped,type=BufferPool"",""type"":""read""},""value"":{""TotalCapacity"":1868266036,""MemoryUsed"":1868266036,""Count"":5338,""Name"":""mapped"",""ObjectName"":{""objectName"":""java.nio:name=mapped,type=BufferPool""}},""timestamp"":1537945860,""status"":200}
{code}



{code}
def resize(newSize: Int) {
    inLock(lock) {
      val raf = new RandomAccessFile(_file, ""rw"")
      val roundedNewSize = roundDownToExactMultiple(newSize, entrySize)
      val position = mmap.position

      /* Windows won't let us modify the file length while the file is mmapped :-( */
      if(Os.isWindows)
        forceUnmap(mmap)
      try {
        raf.setLength(roundedNewSize)
        mmap = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, roundedNewSize)
        _maxEntries = mmap.limit / entrySize
        mmap.position(position)
      } finally {
        CoreUtils.swallow(raf.close())
      }
    }
  }
{code}



{code}
[2018-09-21 13:12:24,078] INFO Rolled new log segment for 'topic-265' in 2 ms. (kafka.log.Log)
[2018-09-21 13:13:16,436] FATAL [ReplicaFetcherThread-12-15], Disk error while replicating data for topic-264 (kafka.server.ReplicaFetcherThread)
kafka.common.KafkaStorageException: I/O exception in append to log 'topic-264'
        at kafka.log.Log.append(Log.scala:349)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:130)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:153)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:141)
        at scala.Option.foreach(Option.scala:257)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:141)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:138)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply$mcV$sp(AbstractFetcherThread.scala:138)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:138)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:138)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:234)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:136)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:103)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:63)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:940)
        at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:61)
        at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:52)
        at kafka.log.LogSegment.<init>(LogSegment.scala:67)
        at kafka.log.Log.roll(Log.scala:778)
        at kafka.log.Log.maybeRoll(Log.scala:744)
        at kafka.log.Log.append(Log.scala:405)
        ... 16 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:937)
        ... 22 more

{code}",Resolved,Done,Bug,Major,huxihx,scott.zhai,KAFKA,Kafka,,,log,,4.2.0
